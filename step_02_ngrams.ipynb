{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(tokens, n):\n",
    "    \"\"\"Takes an iterable of tokens and pads with sentence boundary symbols.\n",
    "    \n",
    "    Always adds sentence end symbols. \n",
    "    For unigram sequences, does not add sentence starts.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    tokens : list, tuple, iterable\n",
    "        sentence to be padded\n",
    "    n : int\n",
    "        the ngram order\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        Input padded with sentence boundary symbols\n",
    "    \"\"\"\n",
    "    start = \"<s>\"\n",
    "    end = \"</s>\"    \n",
    "    tokens = list(tokens)\n",
    "    tokens = [start]*(n-1) + tokens + [end]\n",
    "    # Always return tuples, we don't want to modify the input in-place.\n",
    "    tokens = tuple(tokens)  \n",
    "    # YOUR CODE HERE\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_n_grams(tokens, n):\n",
    "    \"\"\"Takes in a tuple of tokens and forms n-grams\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    tokens : tuple\n",
    "        Tokens to make ngrams from\n",
    "    n : int\n",
    "        The order of ngrams to make\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of tuples: all ngrams of the specified order.\n",
    "    \"\"\"\n",
    "    if len(tokens) < n:\n",
    "        print(\"N-grams order is too big\")\n",
    "        return []\n",
    "    \n",
    "    ngrams = []\n",
    "    for i in range (len(tokens)-n+1):\n",
    "        ngrams.append(tokens[i:i+n])\n",
    "        \n",
    "    return ngrams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allgrams_pipeline(data, max_n):\n",
    "    \"\"\"Produces ngrams of all orders up to max_n from data, with padding\n",
    "    \n",
    "    This uses the user defined pad() and make_n_grams() functions.\n",
    "    It acts as an additional test for those.\n",
    "    \n",
    "    However, you must not change this. If there is some error, change \n",
    "    pad or make_n_grams instead.\"\"\"\n",
    "    for sentence in data:\n",
    "        for n in range(1, max_n+1):\n",
    "            padded = pad(sentence, n)\n",
    "            yield from make_n_grams(padded, n)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "def get_counts(ngrams, max_n):\n",
    "    \"\"\"Counts ngrams in a dataset.\n",
    "    \n",
    "    Takes an iterable of ngrams, of variable order. Simply counts how many times each\n",
    "    ngram is seen. The main idea is how the counts are organized.\n",
    "    \n",
    "    The input is an iterable, which might produce a stream such as:\n",
    "    \n",
    "    ('this',),\n",
    "    ('is',), \n",
    "    ('the',), \n",
    "    ('first',), \n",
    "    ('sentence',),\n",
    "    ('</s>',),    \n",
    "    ('<s>', 'this'), \n",
    "    ('this', 'is'), \n",
    "    ('is', 'the'), \n",
    "    ('the', 'first'), \n",
    "    ('first', 'sentence'), \n",
    "    ('sentence', '</s>'),\n",
    "\n",
    "    Note how the stream has a mix of unigrams and bigrams.\n",
    "\n",
    "    The output is a triply nested dict.\n",
    "    The first level is indexed by ngram order,\n",
    "    the second level is indexed by the history,\n",
    "    and the third level is indexed by the last token (the predicted token).\n",
    "    Additionally, we recommend making the third level an extended type of dict: a Counter\n",
    "    See https://docs.python.org/3/library/collections.html#collections.Counter\n",
    "    Example of the output structure:\n",
    "    {\n",
    "        1: {\n",
    "            (,): \n",
    "                Counter({\n",
    "                    '<s>': 21,\n",
    "                    'this': 43,\n",
    "                    'most': 31,\n",
    "                    'is': 50,\n",
    "                })\n",
    "        2: {\n",
    "            ('<s>', ): \n",
    "                Counter({\n",
    "                    'this': 21,\n",
    "                }),\n",
    "            ('the',):\n",
    "                Counter({\n",
    "                    'most': 31,\n",
    "                    'least': 14,\n",
    "                }),\n",
    "        3: {\n",
    "            ('<s>', 'this'): \n",
    "                Counter({\n",
    "                    'is': 12,\n",
    "                    'has': 8,\n",
    "                    '</s>': 1,\n",
    "                }),\n",
    "            ('the', 'most'):\n",
    "                Counter({\n",
    "                    'beautiful': 8,\n",
    "                    'intelligent': 10,\n",
    "                    'funny': 3,\n",
    "                }),\n",
    "    }\n",
    "    This structure is useful, since each history will also get its own \n",
    "    conditional probability distribution.\n",
    "    Note that when n==1, the ngram history simply becomes \n",
    "    the empty tuple, (,). This is fine.\n",
    "  \n",
    "    Arguments\n",
    "    ---------\n",
    "    sentences : iterable (such as list)\n",
    "        An iterable over ngrams.\n",
    "    max_n : int\n",
    "        The maximum ngram order.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Triply nested dict, from ngram order to n_gram history parts, \n",
    "        to a dictionary of all continuations and their counts, e.g.\n",
    "        {2: {('a',): {'b': 3 'c': 4}}}\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    n_gram_dict = {order: defaultdict(Counter) for order in range(1,max_n+1)}\n",
    "    # The line above creates the triply nested dict.\n",
    "    # The second and third layers are special: defaultdict and Counter\n",
    "    # See their documentation:\n",
    "    # https://docs.python.org/3/library/collections.html#collections.defaultdict\n",
    "    # https://docs.python.org/3/library/collections.html#collections.Counter    \n",
    "    for ngram in iter(ngrams):\n",
    "        order = len(ngram)\n",
    "        if ngram[-1] not in n_gram_dict[order][ngram[:-1]].keys():\n",
    "            n_gram_dict[order][ngram[:-1]][ngram[-1]] = 1\n",
    "        else:\n",
    "            n_gram_dict[order][ngram[:-1]][ngram[-1]] += 1\n",
    "    \n",
    "    # Lastly, make the defaultdicts into normal dicts, \n",
    "    # so that defaultdict doesn't bite us later (it can hide some bugs)        \n",
    "    return {n: dict(counts) for n, counts in n_gram_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell has a utility function, which you need to use down the line.\n",
    "The function is already provided here because it is also needed for the\n",
    "sanity checks in the visible tests for the next task.\n",
    "\"\"\"\n",
    "\n",
    "def logsumexp2(*logs):\n",
    "    \"\"\"Linear-scale addition in log-scale\n",
    "    \n",
    "    https://en.wikipedia.org/wiki/LogSumExp#log-sum-exp_trick_for_log-domain_calculations\"\"\"\n",
    "    x_star = max(logs)\n",
    "    return x_star + log2(sum(pow(2, x-x_star) for x in logs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log2\n",
    "NEGINF = -float('inf')\n",
    "\n",
    "\n",
    "# Look at logprob_abs_discount first, to understand the full picture.\n",
    "# Then, start by implementing logprob_discounted\n",
    "# It has its own tests below; see that you can pass them first.\n",
    "# Next, implement log_interp_weight.\n",
    "# It also has its own tests.\n",
    "# Finally, fill in the missing parts in logprob_abs_discount\n",
    "\n",
    "def logprob_discounted(counts, context, token, delta):\n",
    "    \"\"\"The discounted log probability\n",
    "        \n",
    "    Remember to discount to 0 at most, max(count-delta, 0).\n",
    "    If discounted count becomes 0, the discounted log prob becomes -inf.\n",
    "    And the same concerns as with logprob_mle apply.\n",
    "    \n",
    "    This is the left side of the sum in the probability equations\n",
    "    (the log version of it).\n",
    "    \"\"\"\n",
    "    n = len(context) + 1  # N-gram order\n",
    "    token_count = counts[n][context][token]\n",
    "    \n",
    "    context_count = sum(counts[n][context].values())\n",
    "    if (max(token_count-delta,0) == 0) or (context_count==0):\n",
    "        return NEGINF\n",
    "    else:        \n",
    "        score = log2(max(token_count-delta,0)) - log2(context_count)\n",
    "        return score\n",
    "    \n",
    "def log_interp_weight(counts, context, delta):\n",
    "    \"\"\"The interpolation weight, as determined by the discount.\n",
    "    \n",
    "    You will need to figure out the total sum of discount applied\n",
    "    for this context.\n",
    "    \n",
    "    This is the lambda in the equations (log version of it).\n",
    "    \"\"\"\n",
    "    n = len(context) + 1  # N-gram order\n",
    "    discount_sum = 0\n",
    "    \n",
    "    for token in counts[n][context]:\n",
    "        token_count = counts[n][context][token]\n",
    "        discount_sum += token_count - max(token_count-delta,0)    \n",
    "    \n",
    "    context_count = sum(counts[n][context].values())\n",
    "    if (discount_sum == 0) or (context_count==0):\n",
    "        return NEGINF\n",
    "    else:        \n",
    "        lamda = log2(discount_sum) - log2(context_count)\n",
    "        return lamda    \n",
    "    \n",
    "\n",
    "def logprob_abs_discount(counts, context, token, delta=0.2):\n",
    "    \"\"\"Produces smoothed estimate of log(P(token | context))\n",
    "    \n",
    "    Now we will use absolute discounting and interpolation to lower\n",
    "    orders.\n",
    "    \n",
    "    There are four main challenges to compute here:\n",
    "    1. The discounted count for the token\n",
    "        - Remember to discount to 0 at most, \n",
    "          max(count-delta, 0)\n",
    "        - If discounted count becomes 0, the discounted log prob becomes -inf.\n",
    "          And the same concerns as with logprob_mle apply.\n",
    "    2. The interpolation weight, as determined by the discount.\n",
    "        - You will need to figure out the total sum of discount applied\n",
    "          for this context.\n",
    "    3. The log probability to interpolate with.\n",
    "        - This is easy: use recursion. So just call:\n",
    "          logprob_abs_discount(counts, context[1:], token, delta)\n",
    "        - Unigrams are the special case: they interpolate with the uniform\n",
    "          distribution P(x) = 1 / vocab size.\n",
    "    4. Interpolation in the log domain.\n",
    "        - So you can get log(P_delta(token|context)) and \n",
    "          log(P_interp(token|context[1:])) without problems. But then you need\n",
    "          the logarithmic equivalent of a sum.\n",
    "        - For that, use the logsumexp2 function defined above. \n",
    "    \n",
    "    The ngram counts are as produced by get_counts, same format as \n",
    "    with logprob_mle.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    counts : dict\n",
    "        Triply nested dict as shown above.\n",
    "    context : tuple\n",
    "        The context to predict on as tuple, e.g. ('<s>',)\n",
    "    token : str\n",
    "        The token to predict.\n",
    "    delta : float\n",
    "        The value to discount by.\n",
    "    \"\"\"\n",
    "    n = len(context) + 1  # N-gram order\n",
    "    vocab = set(counts[1][tuple()])\n",
    "    V = len(vocab)  # Vocabulary size\n",
    "    \n",
    "    # Check that word is in the intended vocabulary,\n",
    "    # i.e. at least seen once in the data (as unigram).\n",
    "    # If the word is never seen in the data, we cannot expect it.\n",
    "    if token not in vocab:\n",
    "        return NEGINF\n",
    "\n",
    "    # Find an order where context has been seen:     \n",
    "    if n not in counts or context not in counts[n]:\n",
    "        if n == 1:\n",
    "            raise ValueError(\"Invalid counts-dict, needs to have all lower order counts.\")\n",
    "        return logprob_abs_discount(counts, context[1:], token)\n",
    "    \n",
    "    # 1. Discounted prob (computed by separate function above)\n",
    "    lp_discounted = logprob_discounted(counts, context, token, delta)\n",
    "    \n",
    "    # 2. Log interpolation weight (computed by separate function above):\n",
    "    log_lambda = log_interp_weight(counts, context, delta)\n",
    "    \n",
    "    # 3. Log lower order probability:\n",
    "    if n == 1:\n",
    "        # Stopping recursion at the unigram level, by interpolating with\n",
    "        # unigram distribution:\n",
    "        lp_lower = - log2(V)\n",
    "    else:  # Recursion\n",
    "        lp_lower = logprob_abs_discount(counts, context[1:], token)\n",
    "    \n",
    "    # 4. Putting it all together:\n",
    "    log_sum = [lp_discounted, log_lambda + lp_lower]\n",
    "    result = logsumexp2(*log_sum)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(test_data, model_counts, logprob_func, **lp_kwargs):\n",
    "    \"\"\"\n",
    "    Computes perplexity on the given test data with the given language model\n",
    "    (as specified by the counts and the logprob function).\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    test_data : list\n",
    "        List of lists of tokenized sentences.\n",
    "    model_counts : dict\n",
    "        Triply nested dict of ngram counts, as returned by get_counts()\n",
    "    logprob_func : function\n",
    "        Function with signature (counts, context, token), which returns the\n",
    "        log-probability of the token given the context.\n",
    "    **lp_kwargs : kwargs\n",
    "        Log prob key word arguments, passed to logprob_func\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The perplexity of the model on the test data.\n",
    "    \"\"\"\n",
    "    max_n = max(model_counts.keys())\n",
    "    total_log_prob = 0.\n",
    "    num_tokens = 0\n",
    "    for sentence in test_data:\n",
    "        padded = pad(sentence, max_n)\n",
    "        ngrams = make_n_grams(padded, max_n)\n",
    "        for *context, token in ngrams:\n",
    "            total_log_prob += logprob_func(model_counts, tuple(context), token, **lp_kwargs)\n",
    "            num_tokens += 1\n",
    "    ppl = pow(2, -total_log_prob / num_tokens)\n",
    "    return ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_text(model_counts, logprob_func, seed_text=None, **lp_kwargs):\n",
    "    \"\"\"Generates text from an N-gram model.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    model_counts : dict\n",
    "        N-gram counts as returned by get_counts()\n",
    "    logprob_func : callable\n",
    "        Function with signature (counts, context, token), which returns the\n",
    "        log-probability of the token given the context.\n",
    "    seed_text : list, optional\n",
    "        Text to start generating from. If None, will start from the\n",
    "        appropriate amount of sentence-start symbols (N-1).\n",
    "    **lp_kwargs : kwargs\n",
    "        Log prob key word arguments, passed to logprob_func\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        Sentence generated by model as a list of tokens. If\n",
    "        seed_text was given, will include it. Padding is stripped.\n",
    "    \"\"\"\n",
    "    max_n = max(model_counts.keys())\n",
    "    vocab = list(model_counts[1][tuple()])\n",
    "    if seed_text is None:\n",
    "        seed_text = ('<s>',) * (max_n-1)\n",
    "    end = '</s>'\n",
    "    output = list(seed_text)\n",
    "    while output[-1] != end and len(output) < 200:  # Also guard against infinite loops\n",
    "        context = output[-max_n+1:] if max_n > 1 else []\n",
    "        token_logprobs = [2**logprob_func(model_counts, tuple(context), token, **lp_kwargs) \n",
    "                          for token in vocab]\n",
    "        next_part = random.choices(vocab, token_logprobs)\n",
    "        output.extend(next_part)  # next_part is a list: [token]\n",
    "    return tuple(token for token in output if token not in ['<s>', '</s>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detokenize(seq, full_sentence=True):\n",
    "    \"\"\"A simple rule-based detokenizer for this assignment\"\"\"\n",
    "    last_seen_start = full_sentence\n",
    "    formatted_tokens = []\n",
    "    for token in seq:\n",
    "        if token == \"<s>\":\n",
    "            last_seen_start = True\n",
    "            continue\n",
    "        if token == \"</s>\":\n",
    "            continue\n",
    "        if last_seen_start:\n",
    "            token = token.capitalize()\n",
    "            last_seen_start = False\n",
    "        if token == \"i\":\n",
    "            token = \"I\"\n",
    "        if token in \".!?,;\" or token == \"'s\":\n",
    "            formatted_tokens.append(token)\n",
    "        elif formatted_tokens:\n",
    "            formatted_tokens.append(\" \" + token)\n",
    "        else:\n",
    "            formatted_tokens.append(token)\n",
    "    return \"\".join(formatted_tokens)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import our data here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We know you love Chewy.', \"We know you're here.\", \"We know you know the Chewy-RyanCohen-GameStop connection, but it wasn't real enough for you yet.\", \"Well, I don't have to tell you, because you're not stupid, but I will anyway:  it's gotten really real enough for you now.\", 'The Chewy executive triumvirate joining the GameStop board of directors is your signal, friend.', 'You may start pumping GME to your boomer audience.', 'Now.', \"I don't believe reddit has been too kind to you in the past, but worry not, follow through with this and you'll have lots of friends here and we'll have your back forever.\", 'Well, definitely not forever, but at least for a while.', 'What better time to start than today?', 'With love, brother.', 'P.S.', \"- don't be afraid to use the rocket ðŸš€, it feels good.\"]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# Load result:\n",
    "with open(\"data.txt\", \"rb\") as fp:   # Unpickling\n",
    "    sentences = pickle.load(fp)\n",
    "\n",
    "print(sentences[22:35])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 967968/967968 [02:12<00:00, 7332.20it/s]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "\n",
    "janeausten_tokenized = []\n",
    "for sentence in tqdm(sentences):\n",
    "    \n",
    "    # We split the sentences into a list to do the next step\n",
    "    sentence_list = word_tokenize(sentence)    \n",
    "    if sentence_list:        \n",
    "        janeausten_tokenized.append(sentence_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training set has 871171 sentences, and the test set 96797 sentences\n"
     ]
    }
   ],
   "source": [
    "test_split_index = round(0.9 * len(janeausten_tokenized))\n",
    "janeausten_train = janeausten_tokenized[:test_split_index]\n",
    "janeausten_test = janeausten_tokenized[test_split_index:]\n",
    "\n",
    "print(\"The training set has\", len(janeausten_train), \"sentences, and the test set\", len(janeausten_test), \"sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10 most common tokens:\n",
      ".\n",
      "the\n",
      ",\n",
      "to\n",
      "I\n",
      "a\n",
      "and\n",
      "is\n",
      "of\n",
      "you\n",
      "\n",
      "The 2490-2500 most common tokens:\n",
      "Bill\n",
      "offering\n",
      "exp\n",
      "manager\n",
      "28\n",
      "bond\n",
      "GREEN\n",
      "prior\n",
      "pumps\n",
      "club\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import itertools\n",
    "janeausten_unigram_counts = Counter(itertools.chain.from_iterable(janeausten_train)).most_common()\n",
    "\n",
    "print(\"The 10 most common tokens:\")\n",
    "print(\"\\n\".join(word for word, freq in janeausten_unigram_counts[:10]))\n",
    "print()\n",
    "print(\"The 2490-2500 most common tokens:\")\n",
    "print(\"\\n\".join(word for word, freq in janeausten_unigram_counts[2490:2500]))\n",
    "\n",
    "\n",
    "#######################################################################################\n",
    "#######################################################################################\n",
    "#######################################################################################\n",
    "janeausten_vocab_filt = set(word for word, freq in janeausten_unigram_counts[:2500]) | {\"</s>\", \"<unk>\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_oovs(vocab, data, unk=\"<unk>\"):\n",
    "    \"\"\"Replace OOV words with unknown-token\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    vocab : set\n",
    "        The set of tokens that are in-vocabulary.\n",
    "        token not in vocab => token is out-of-vocabulary.\n",
    "    data : list of iterables\n",
    "        List of sentences, which are lists (or other iterables) of tokens.\n",
    "    unk : str\n",
    "        Token to replace tokens which are not in the vocabulary\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        list of lists, (list of sentences in data, sentences are lists of tokens)\n",
    "        The data with out-of-vocabulary tokens replaced with the unknown token.\n",
    "        Does NOT modify in-place.\n",
    "    \n",
    "    \"\"\"\n",
    "    # NOTE: Do not modify input in-place.\n",
    "    data_oovs_replaced = [list(sentence) for sentence in data]\n",
    "    for i, sentence in enumerate(data):\n",
    "        for j, token in enumerate(sentence):\n",
    "            if token not in vocab:\n",
    "                data_oovs_replaced[i][j] = unk\n",
    "        \n",
    "    return data_oovs_replaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's see an example:\n",
      "This is a company that is being <unk> very well given the <unk>.\n"
     ]
    }
   ],
   "source": [
    "janeausten_train_filt = replace_oovs(janeausten_vocab_filt, janeausten_train)\n",
    "janeausten_test_filt = replace_oovs(janeausten_vocab_filt, janeausten_test)\n",
    "\n",
    "print(\"Let's see an example:\")\n",
    "print(detokenize(janeausten_train_filt[2000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce counts from the real world data:\n",
    "janeausten_counts = get_counts(allgrams_pipeline(janeausten_train_filt, 5), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LIWCache:\n",
    "    \"\"\"Very simple cache for log_interp_weight for speeding up querys\n",
    "    \n",
    "    The log_interp_weight function gets called many times with the same\n",
    "    arguments. The normal Python LRU Cache decorator however cannot handle\n",
    "    the counts argument, as it is unhashable. \n",
    "    \"\"\"\n",
    "    def __init__(self, func):\n",
    "        self.func = func\n",
    "        self.cache = {}\n",
    "        self._caching = False\n",
    "        self.hits = 0\n",
    "        self.misses = 0\n",
    "        \n",
    "    def __call__(self, counts, context, delta):\n",
    "        if not self._caching:\n",
    "            return self.func(counts, context, delta)\n",
    "        key = (context, delta)\n",
    "        if key not in self.cache:\n",
    "            self.cache[key] = self.func(counts, context, delta)\n",
    "            self.misses += 1\n",
    "        else:\n",
    "            self.hits += 1\n",
    "        return self.cache[key]\n",
    "        \n",
    "    @property\n",
    "    def caching(self):\n",
    "        return self._caching\n",
    "\n",
    "    @caching.setter\n",
    "    def caching(self, value):\n",
    "        self._caching = value\n",
    "        if not value:\n",
    "            self.cache = {}  # Empty\n",
    "            self.hits = 0\n",
    "            self.misses = 0\n",
    "\n",
    "if not isinstance(log_interp_weight, LIWCache):\n",
    "    log_interp_weight = LIWCache(log_interp_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_text = ['it', 'is', 'a', 'truth', 'universally', 'acknowledged', ',', 'that',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences from a smoothed model:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                               | 1/4 [00:15<00:46, 15.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is a truth universally acknowledged, that kind of money on this like my own brother.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                          | 2/4 [00:28<00:27, 13.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is a truth universally acknowledged, that would cause me to crash the stock.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                     | 3/4 [01:19<00:30, 30.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is a truth universally acknowledged, that's right .... * to the fucking retards on this earth that would make sense from a bunch of <unk> <unk> <unk>, <unk> <unk> ) and August <unk> on Monday, much love.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [01:26<00:00, 21.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is a truth universally acknowledged, that means drop is coming.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentences from a smoothed model:\")\n",
    "log_interp_weight.caching = True\n",
    "text_results = []\n",
    "for i in tqdm(range(4)):\n",
    "    text_result = detokenize(generate_text(janeausten_counts, logprob_abs_discount, seed_text, delta=5.2))\n",
    "    text_results.append(text_result)\n",
    "    print(text_results[i])\n",
    "    \n",
    "log_interp_weight.caching = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', 'is', 'a', 'truth', 'universally', 'acknowledged,', 'that', 'kind', 'of', 'money', 'on', 'this', 'like', 'my', 'own', 'brother']\n",
      "---------------------------------------------------\n",
      "Score inf\n",
      "===================================================\n",
      "['It', 'is', 'a', 'truth', 'universally', 'acknowledged,', 'that', 'would', 'cause', 'me', 'to', 'crash', 'the', 'stock']\n",
      "---------------------------------------------------\n",
      "Score inf\n",
      "===================================================\n",
      "['It', 'is', 'a', 'truth', 'universally', 'acknowledged,', \"that's\", 'right', '....', '*', 'to', 'the', 'fucking', 'retards', 'on', 'this', 'earth', 'that', 'would', 'make', 'sense', 'from', 'a', 'bunch', 'of', '<unk>', '<unk>', '<unk>,', '<unk>', '<unk>', ')', 'and', 'August', '<unk>', 'on', 'Monday,', 'much', 'love']\n",
      "---------------------------------------------------\n",
      "Score inf\n",
      "===================================================\n",
      "['It', 'is', 'a', 'truth', 'universally', 'acknowledged,', 'that', 'means', 'drop', 'is', 'coming']\n",
      "---------------------------------------------------\n",
      "Score inf\n",
      "===================================================\n"
     ]
    }
   ],
   "source": [
    "for sentence in text_results:\n",
    "    text = sentence.split()\n",
    "    text[-1] = text[-1][:-1]\n",
    "    print(text)\n",
    "    print('---------------------------------------------------')    \n",
    "    print('Score {}'.format(perplexity([text], janeausten_counts, logprob_abs_discount)))\n",
    "    print('===================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences from a smoothed model:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                               | 1/4 [00:04<00:14,  4.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elon Musk and <unk>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                          | 2/4 [00:18<00:20, 10.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elon Musk <unk> to <unk> an <unk> on this gambling subreddit.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                     | 3/4 [00:42<00:16, 16.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elon Musk's tweet ] ( https : <unk>? <unk> & amp; <unk> & amp; <unk>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [01:46<00:00, 26.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elon Musk is <unk> <unk> ( <unk> to <unk> bought it that long and are already worried about it <unk> really got â€œ I â€™ d have a <unk> my friend to options so I can short <unk> plus <unk> are totally not a thing <unk> <unk>!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "seed_text = ['Elon', 'Musk',]\n",
    "print(\"Sentences from a smoothed model:\")\n",
    "log_interp_weight.caching = True\n",
    "text_results = []\n",
    "for i in tqdm(range(4)):\n",
    "    text_result = detokenize(generate_text(janeausten_counts, logprob_abs_discount, seed_text, delta=5.2))\n",
    "    text_results.append(text_result)\n",
    "    print(text_results[i])\n",
    "    \n",
    "log_interp_weight.caching = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Elon', 'Musk', 'and', '<unk>']\n",
      "---------------------------------------------------\n",
      "Score 299.87244152083383\n",
      "===================================================\n",
      "['Elon', 'Musk', '<unk>', 'to', '<unk>', 'an', '<unk>', 'on', 'this', 'gambling', 'subreddit']\n",
      "---------------------------------------------------\n",
      "Score 69.26147146592136\n",
      "===================================================\n",
      "['Elon', \"Musk's\", 'tweet', ']', '(', 'https', ':', '<unk>?', '<unk>', '&', 'amp;', '<unk>', '&', 'amp;', '<unk']\n",
      "---------------------------------------------------\n",
      "Score inf\n",
      "===================================================\n",
      "['Elon', 'Musk', 'is', '<unk>', '<unk>', '(', '<unk>', 'to', '<unk>', 'bought', 'it', 'that', 'long', 'and', 'are', 'already', 'worried', 'about', 'it', '<unk>', 'really', 'got', 'â€œ', 'I', 'â€™', 'd', 'have', 'a', '<unk>', 'my', 'friend', 'to', 'options', 'so', 'I', 'can', 'short', '<unk>', 'plus', '<unk>', 'are', 'totally', 'not', 'a', 'thing', '<unk>', '<unk>']\n",
      "---------------------------------------------------\n",
      "Score 19.78849991375468\n",
      "===================================================\n"
     ]
    }
   ],
   "source": [
    "for sentence in text_results:\n",
    "    text = sentence.split()\n",
    "    text[-1] = text[-1][:-1]\n",
    "    print(text)\n",
    "    print('---------------------------------------------------')    \n",
    "    print('Score {}'.format(perplexity([text], janeausten_counts, logprob_abs_discount)))\n",
    "    print('===================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences from a smoothed model:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                               | 1/4 [00:36<01:49, 36.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gme to do that but you 're still bullish, SPY to <unk> $ <unk> % <unk> <unk> <unk> FOR A <unk> <unk> TO <unk> AT ALL.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                          | 2/4 [00:41<00:35, 17.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gme to the moon!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                     | 3/4 [00:49<00:13, 13.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gme to <unk> miss the next <unk> <unk>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [01:04<00:00, 16.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gme to $ <unk> since the <unk> <unk> is working well.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "seed_text = ['GME', 'to',]\n",
    "print(\"Sentences from a smoothed model:\")\n",
    "log_interp_weight.caching = True\n",
    "text_results = []\n",
    "for i in tqdm(range(4)):\n",
    "    text_result = detokenize(generate_text(janeausten_counts, logprob_abs_discount, seed_text, delta=5.2))\n",
    "    text_results.append(text_result)\n",
    "    print(text_results[i])\n",
    "    \n",
    "log_interp_weight.caching = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Gme', 'to', 'do', 'that', 'but', 'you', \"'re\", 'still', 'bullish,', 'SPY', 'to', '<unk>', '$', '<unk>', '%', '<unk>', '<unk>', '<unk>', 'FOR', 'A', '<unk>', '<unk>', 'TO', '<unk>', 'AT', 'ALL']\n",
      "---------------------------------------------------\n",
      "Score inf\n",
      "===================================================\n",
      "['Gme', 'to', 'the', 'moon']\n",
      "---------------------------------------------------\n",
      "Score inf\n",
      "===================================================\n",
      "['Gme', 'to', '<unk>', 'miss', 'the', 'next', '<unk>', '<unk']\n",
      "---------------------------------------------------\n",
      "Score inf\n",
      "===================================================\n",
      "['Gme', 'to', '$', '<unk>', 'since', 'the', '<unk>', '<unk>', 'is', 'working', 'well']\n",
      "---------------------------------------------------\n",
      "Score inf\n",
      "===================================================\n"
     ]
    }
   ],
   "source": [
    "for sentence in text_results:\n",
    "    text = sentence.split()\n",
    "    text[-1] = text[-1][:-1]\n",
    "    print(text)\n",
    "    print('---------------------------------------------------')    \n",
    "    print('Score {}'.format(perplexity([text], janeausten_counts, logprob_abs_discount)))\n",
    "    print('===================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences from a smoothed model:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                               | 1/4 [00:06<00:19,  6.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daddy You fuck around too much\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                          | 2/4 [00:07<00:06,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daddy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                     | 3/4 [00:11<00:03,  3.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daddy my upvote.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:12<00:00,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daddy.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "seed_text = ['Daddy',]\n",
    "print(\"Sentences from a smoothed model:\")\n",
    "log_interp_weight.caching = True\n",
    "text_results = []\n",
    "for i in tqdm(range(4)):\n",
    "    text_result = detokenize(generate_text(janeausten_counts, logprob_abs_discount, seed_text, delta=5.2))\n",
    "    text_results.append(text_result)\n",
    "    print(text_results[i])\n",
    "    \n",
    "log_interp_weight.caching = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Daddy', 'You', 'fuck', 'around', 'too', 'muc']\n",
      "---------------------------------------------------\n",
      "Score inf\n",
      "===================================================\n",
      "['Dadd']\n",
      "---------------------------------------------------\n",
      "Score inf\n",
      "===================================================\n",
      "['Daddy', 'my', 'upvote']\n",
      "---------------------------------------------------\n",
      "Score inf\n",
      "===================================================\n",
      "['Daddy']\n",
      "---------------------------------------------------\n",
      "Score inf\n",
      "===================================================\n"
     ]
    }
   ],
   "source": [
    "for sentence in text_results:\n",
    "    text = sentence.split()\n",
    "    text[-1] = text[-1][:-1]\n",
    "    print(text)\n",
    "    print('---------------------------------------------------')    \n",
    "    print('Score {}'.format(perplexity([text], janeausten_counts, logprob_abs_discount)))\n",
    "    print('===================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences from a smoothed model:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                               | 1/4 [00:21<01:03, 21.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My wife â€™ s boyfriend is the <unk> <unk> of the United States has to do.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                          | 2/4 [00:50<00:51, 25.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My wife â€™ s boyfriend said if SPY did n't even see any fucking positions or strike prices in here OP BAN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                     | 3/4 [00:54<00:16, 16.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My wife â€™ s boyfriend\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [01:03<00:00, 15.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My wife â€™ s boyfriend â€™ comments.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "seed_text = ['My', 'wife',]\n",
    "print(\"Sentences from a smoothed model:\")\n",
    "log_interp_weight.caching = True\n",
    "text_results = []\n",
    "for i in tqdm(range(4)):\n",
    "    text_result = detokenize(generate_text(janeausten_counts, logprob_abs_discount, seed_text, delta=5.2))\n",
    "    text_results.append(text_result)\n",
    "    print(text_results[i])\n",
    "    \n",
    "log_interp_weight.caching = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'wife', 'â€™', 's', 'boyfriend', 'is', 'the', '<unk>', '<unk>', 'of', 'the', 'United', 'States', 'has', 'to', 'do']\n",
      "---------------------------------------------------\n",
      "Score 11.595135423904363\n",
      "===================================================\n",
      "['My', 'wife', 'â€™', 's', 'boyfriend', 'said', 'if', 'SPY', 'did', \"n't\", 'even', 'see', 'any', 'fucking', 'positions', 'or', 'strike', 'prices', 'in', 'here', 'OP', 'BA']\n",
      "---------------------------------------------------\n",
      "Score 15.468115285824581\n",
      "===================================================\n",
      "['My', 'wife', 'â€™', 's', 'boyfrien']\n",
      "---------------------------------------------------\n",
      "Score inf\n",
      "===================================================\n",
      "['My', 'wife', 'â€™', 's', 'boyfriend', 'â€™', 'comments']\n",
      "---------------------------------------------------\n",
      "Score 25.27509886143185\n",
      "===================================================\n"
     ]
    }
   ],
   "source": [
    "for sentence in text_results:\n",
    "    text = sentence.split()\n",
    "    text[-1] = text[-1][:-1]\n",
    "    print(text)\n",
    "    print('---------------------------------------------------')    \n",
    "    print('Score {}'.format(perplexity([text], janeausten_counts, logprob_abs_discount)))\n",
    "    print('===================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_text = ['More', 'rocket', 'please',]\n",
    "print(\"Sentences from a smoothed model:\")\n",
    "log_interp_weight.caching = True\n",
    "text_results = []\n",
    "for i in tqdm(range(4)):\n",
    "    text_result = detokenize(generate_text(janeausten_counts, logprob_abs_discount, seed_text, delta=5.2))\n",
    "    text_results.append(text_result)\n",
    "    print(text_results[i])\n",
    "    \n",
    "log_interp_weight.caching = False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
