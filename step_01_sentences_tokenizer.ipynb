{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk.data\n",
    "import re\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: 507871 \n",
      "['Solid buy signal', 'I know this person has to be in this subreddit', 'TOMORROW ðŸš€ðŸš€ðŸš€ CONFIRMED', '#LONG LIVE THE MEME ðŸš€ðŸš€ðŸš€', 'THE GOD HAS SPOKEN! ðŸš€ðŸŒŒðŸš€ðŸŒŒ']\n",
      "['You know when you were little and you broke a lamp and your mom put you in timeout? The govt does a similar thing to people who are irresponsible with money except instead of timeout you get to spend some time in club fed. ', 'rEcEsSiOn', \"If you believe the official Chinese GDP numbers you're a bigger moron than he is.\", 'Howâ€™s club fed compare to club penguin', 'Weak hands bro', 'Why do you need a book when your already an expert?', 'If it goes bankrupt?', 'Which of you fucking retards bought gold for the Auto Moderator for todayâ€™s Daily Discussion? Just why?', 'Dead cat bounce: Day 14', \"I don't fucking understand how a dovish FED is a bullish signal... it's literally a sign of a weakening economy...\"]\n"
     ]
    }
   ],
   "source": [
    "# LOAD DATA (WILL TAKE FEW SEC)\n",
    "def read_csv(file_name):\n",
    "    file_path = './' + file_name + '/' + file_name + '.csv'\n",
    "    df = pd.read_csv(file_path, usecols = ['text'], encoding='utf-8-sig')\n",
    "    test_list = list(df.text)\n",
    "    return test_list\n",
    "\n",
    "data_list = read_csv('submissions')\n",
    "data_list = data_list + read_csv('comments')\n",
    "print('Data size: %d ' % len(data_list))\n",
    "print(data_list[:5])\n",
    "print(data_list[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of a long sentences:\n",
      "We know you love Chewy.  We know you're here.  We know you know the Chewy-RyanCohen-GameStop connection, but it wasn't real enough for you yet.  Well, I don't have to tell you, because you're not stupid, but I will anyway:  it's gotten really real enough for you now.  The Chewy executive triumvirate joining the GameStop board of directors is your signal, friend.  You may start pumping GME to your boomer audience.  Now.  I don't believe reddit has been too kind to you in the past, but worry not, follow through with this and you'll have lots of friends here and we'll have your back forever.  Well, definitely not forever, but at least for a while.  What better time to start than today?  With love, brother.\n",
      "\n",
      "P.S. - don't be afraid to use the rocket ðŸš€, it feels good.\n"
     ]
    }
   ],
   "source": [
    "print('Example of a long sentences:')\n",
    "print(data_list[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 507871/507871 [1:25:59<00:00, 98.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: 967968 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# WARNING! THIS CELL TAKE A LONG TIME TO RUN\n",
    "# The data we have is a list of sentences. Sometime it have only 1 sentence, sometime it have more than one sentences.\n",
    "# Idealy, I used sentence tokenizer to split it into 1 sentence\n",
    "# The problem is, sentence tokenizer only take a string, so we can only do it for each item in our list:\n",
    "# It will take sometime to run (AT LEAST 15 MINS)\n",
    "sentences = []\n",
    "\n",
    "for item in tqdm(data_list):\n",
    "    temp = sent_detector.tokenize(item)\n",
    "    sentences = sentences + temp\n",
    "\n",
    "print('Data size: %d ' % len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We know you love Chewy.', \"We know you're here.\", \"We know you know the Chewy-RyanCohen-GameStop connection, but it wasn't real enough for you yet.\", \"Well, I don't have to tell you, because you're not stupid, but I will anyway:  it's gotten really real enough for you now.\", 'The Chewy executive triumvirate joining the GameStop board of directors is your signal, friend.', 'You may start pumping GME to your boomer audience.', 'Now.', \"I don't believe reddit has been too kind to you in the past, but worry not, follow through with this and you'll have lots of friends here and we'll have your back forever.\", 'Well, definitely not forever, but at least for a while.', 'What better time to start than today?', 'With love, brother.', 'P.S.', \"- don't be afraid to use the rocket ðŸš€, it feels good.\"]\n"
     ]
    }
   ],
   "source": [
    "print(sentences[22:35])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save result:\n",
    "with open(\"data.txt\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(sentences, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load result:\n",
    "with open(\"data.txt\", \"rb\") as fp:   # Unpickling\n",
    "    sentences = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We know you love Chewy.', \"We know you're here.\", \"We know you know the Chewy-RyanCohen-GameStop connection, but it wasn't real enough for you yet.\", \"Well, I don't have to tell you, because you're not stupid, but I will anyway:  it's gotten really real enough for you now.\", 'The Chewy executive triumvirate joining the GameStop board of directors is your signal, friend.', 'You may start pumping GME to your boomer audience.', 'Now.', \"I don't believe reddit has been too kind to you in the past, but worry not, follow through with this and you'll have lots of friends here and we'll have your back forever.\", 'Well, definitely not forever, but at least for a while.', 'What better time to start than today?', 'With love, brother.', 'P.S.', \"- don't be afraid to use the rocket ðŸš€, it feels good.\"]\n"
     ]
    }
   ],
   "source": [
    "print(sentences[22:35])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
