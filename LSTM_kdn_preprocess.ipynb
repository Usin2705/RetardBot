{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "LSTM_kdn_preprocess.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwMoK7If86mX"
      },
      "source": [
        "# The LSTM model shown in the KDnuggets article\n",
        "\n",
        "https://www.kdnuggets.com/2020/07/pytorch-lstm-text-generation-tutorial.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aldZUwUz86mZ"
      },
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from torch.utils.data import DataLoader\n",
        "import re, string\n",
        "from nltk.tokenize import WordPunctTokenizer"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNi2QLnC-Kuf",
        "outputId": "359358ad-32fa-4c95-f7e3-fdc4845a3d42"
      },
      "source": [
        "# this is to connect this notebook to the contents of your Google Drive\n",
        "# files uploaded to Google Drive will not be deleted by inactivity,\n",
        "# but it does require an authorization code every time you use it\n",
        "from google.colab import drive\n",
        "drive.mount(r'/content/drive')"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LnKyN44_25w"
      },
      "source": [
        "# this is Ryoko's Google Drive filepath\n",
        "# please specify your own, or we can (probably) share a folder for it\n",
        "filepath = r'/content/drive/My Drive/RetardBot/'"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTRefLnj86mb"
      },
      "source": [
        "is_training = True\n",
        "#parameters needed to run the model\n",
        "#these originally needed to be specified from the terminal\n",
        "sequence_length = 4 #Default = 4 \n",
        "batch_size = 128 #Default = 256 Reduce if PC don't have enough RAM\n",
        "max_epochs = 10 #Default = 10\n",
        "#device = torch.device('cuda:0')\n",
        "device = torch.device('cpu')"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWp9TaJV86mb"
      },
      "source": [
        "### Notes on the model architecture\n",
        "\n",
        "Based on model from https://www.kdnuggets.com/2020/07/pytorch-lstm-text-generation-tutorial.html\n",
        "\n",
        "The model has three components:\n",
        "1. **Embedding layer:** converts input of size (batch_size, sequence_length) to embedding of size (batch_size, sequence_length, embedding_dim)\n",
        "2. **Stacked LSTM of 3 layers:** accepts embedding and a tuple (previous hidden state, previous cell state) and gives an output of size (batch_size, sequence_length, embedding_dim) and the tuple (current hidden state, current cell state). The hidden state and cell state both have size (num_layers, sequence_length, embedding_dim).\n",
        "3. **Linear layer:** Maps the output of LSTM to logits for each word in vocab. Not a probability yet. Output size is  (batch_size, sequence_length, vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipp87kCB86mc"
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, dataset):\n",
        "        super(Model, self).__init__()\n",
        "        self.lstm_size = 128\n",
        "        self.embedding_dim = 128\n",
        "        self.num_layers = 3 #stack 3 LSTM layers for abstract representation\n",
        "\n",
        "        n_vocab = len(dataset.uniq_words)\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=n_vocab,\n",
        "            embedding_dim=self.embedding_dim,\n",
        "        )\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=self.lstm_size,\n",
        "            hidden_size=self.lstm_size,\n",
        "            num_layers=self.num_layers,\n",
        "            dropout=0.1,\n",
        "        )\n",
        "        self.fc = nn.Linear(self.lstm_size, n_vocab)\n",
        "\n",
        "    def forward(self, x, prev_state):\n",
        "        embed = self.embedding(x)\n",
        "        output, state = self.lstm(embed, prev_state)\n",
        "        logits = self.fc(output)\n",
        "        return logits, state\n",
        "\n",
        "    def init_state(self, sequence_length):\n",
        "        return (torch.zeros(self.num_layers, sequence_length, self.lstm_size).to(device),\n",
        "                torch.zeros(self.num_layers, sequence_length, self.lstm_size).to(device))"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jorh-klW86md"
      },
      "source": [
        "### Notes on the custom dataset\n",
        "\n",
        "According to the Pytorch documentation, a custom dataset needs at least the functions \\_\\_len\\_\\_ and \\_\\_getitem\\_\\_. \\_\\_len\\_\\_\\_ allows len(dataset) to return the size of the dataset and  \\_\\_getitem\\_\\_ allows the ith element of the dataset to be fetched with dataset\\[i\\].\n",
        "\n",
        "In this custom dataset, \\_\\_len\\_\\_ and \\_\\_getitem\\_\\_ are designed like this. Let's say the only sentence we have in the dataset is:\n",
        "\n",
        "__*We are using LSTM to create the Retard-bot language model.*__\n",
        "\n",
        "__\\_\\_len\\_\\_:__<br>\n",
        "For this custom dataset it's defined as \"the size of the dataset - sequence length\". This is probably because this model is created to make predictions based the first 4 words (default sequence length) given as prompt, but I can't say for certain. So in the example sentence above, it will return  the length of \"**to create the Retard-bot language model.**\"\n",
        "\n",
        "__\\_\\_getitem\\_\\_:__<br>\n",
        "It seems that this returns a tuple of n-grams with the n defined by sequence length. So if we say dataset\\[0\\] in the simple example, we would get (**We are using LSTM**, **are using LSTM to**). Not sure why it does this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjLxQzLJLc-p"
      },
      "source": [
        "def preprocess(filename):\n",
        "  f = open(filename, \"r\")\n",
        "  text = f.read()\n",
        "\n",
        "  text = text.split(' ')\n",
        "  text = [word for word in text if not '\\\\' in r\"%r\" %word] #remove the words containing backslashes (formerly emojis or sth?)\n",
        "  text = [word.lower() for word in text] \n",
        "  text =  [WordPunctTokenizer().tokenize(word) for word in text] #using nltk to separate punctuation from words\n",
        "  text = [item for sublist in text for item in sublist]\n",
        "  \n",
        "  return text\n"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYCe9N3N86me"
      },
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        sequence_length\n",
        "    ):\n",
        "        \"\"\"\n",
        "        words:                 words in entire dataset split by whitespace\n",
        "        uniq_words:       the unique words sorted by frequency (most frequent first)\n",
        "        index_to_word: index to word dict {index0: word0, index1:word1...}, most frequent have smaller index\n",
        "        word_to_index: word to index dict {word0: index0, word1:index1...}, most frequent have smaller index\n",
        "        words_indexes:  the words converted to their indices using word_to_index\n",
        "        \"\"\"\n",
        "        self.sequence_length = sequence_length\n",
        "        self.words = self.load_words()\n",
        "        self.uniq_words = self.get_uniq_words()\n",
        "        self.index_to_word = {index: word for index, word in enumerate(self.uniq_words)}\n",
        "        self.word_to_index = {word: index for index, word in enumerate(self.uniq_words)}\n",
        "        self.words_indexes = [self.word_to_index[w] for w in self.words]\n",
        "        \n",
        "    def load_words(self):\n",
        "        #train_df = pd.read_csv('reddit-cleanjokes.csv') #original reddit-jokes dataset\n",
        "        #text = train_df['Joke'].str.cat(sep=' ')\n",
        "        return preprocess(filepath+\"data.txt\")\n",
        "    \n",
        "    def get_uniq_words(self):\n",
        "        word_counts = Counter(self.words)\n",
        "        return sorted(word_counts, key=word_counts.get, reverse=True) \n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.words_indexes) - self.sequence_length\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        return (\n",
        "            torch.tensor(self.words_indexes[index:index+self.sequence_length]),\n",
        "            torch.tensor(self.words_indexes[index+1:index+self.sequence_length+1]),\n",
        "        )"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anb85jBK86mf"
      },
      "source": [
        "dataset = Dataset(sequence_length)\n",
        "model = Model(dataset)\n",
        "model = model.to(device)\n",
        "\n",
        "def train(dataset, model):\n",
        "    model.train()\n",
        "    \n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle = True) # NEED TO SHUFFLE AND RERUN\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    \n",
        "    for epoch in range(max_epochs):\n",
        "        state_h, state_c = model.init_state(sequence_length)\n",
        "        epoch_loss = 0.0\n",
        "        \n",
        "        for i, batch in enumerate(dataloader):\n",
        "            x, y = batch\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
        "            loss = criterion(y_pred.transpose(1, 2), y)\n",
        "            \n",
        "            state_h = state_h.detach()\n",
        "            state_c = state_c.detach()\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            if (i +1)% 100 ==0:\n",
        "              print({ 'epoch': epoch+1, \"batch\": i+1, 'loss': epoch_loss/(i+1) })\n",
        "            if (i+1)% 3000 ==0:\n",
        "              # this is needed for work in Colab because once the time limit is up,\n",
        "              # it will automatically delete all files that are not saved in Google Drive\n",
        "              print(\"saving model\")\n",
        "              file_name = 'kdnuggets' + \"_preprocess1\"\n",
        "              torch.save(model.state_dict(), filepath+file_name)\n",
        "\n",
        "    print({ 'epoch': epoch+1, 'loss': epoch_loss/(i+1) })"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiIa1Hw786mg"
      },
      "source": [
        "def predict(dataset, model, text, next_words=100):\n",
        "    model.eval()\n",
        "    \n",
        "    words = text.split(' ')\n",
        "    state_h, state_c = model.init_state(len(words))\n",
        "    \n",
        "    for i in range(0, next_words):\n",
        "        x = torch.tensor([[dataset.word_to_index[w] for w in words[i:]]], device=device)\n",
        "        y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
        "        \n",
        "        last_word_logits = y_pred[0][-1]\n",
        "        p = torch.nn.functional.softmax(last_word_logits, dim=0).detach().cpu().numpy()\n",
        "        p /= p.sum() #this is to avoid an error numpy gives about probability not summing to 1\n",
        "        \n",
        "        word_index = np.random.choice(len(last_word_logits), p=p)\n",
        "        words.append(dataset.index_to_word[word_index])\n",
        "        \n",
        "    return words"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxKHjChc86mg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c313b71-5aa8-451d-ca34-4ec091723f0b"
      },
      "source": [
        "if is_training:\n",
        "    train(dataset, model)\n",
        "    file_name = 'kdnuggets' + str(max_epochs)\n",
        "    torch.save(model.state_dict(), filepath+file_name)\n",
        "else:\n",
        "    #model.load_state_dict(torch.load(file_name, map_location=lambda storage, loc: storage))\n",
        "    model.load_state_dict(torch.load(filepath + 'kdnuggets_preprocess1',map_location=torch.device('cpu')))\n",
        "    model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'epoch': 1, 'batch': 100, 'loss': 7.853996558189392}\n",
            "{'epoch': 1, 'batch': 200, 'loss': 7.445404195785523}\n",
            "{'epoch': 1, 'batch': 300, 'loss': 7.297458457946777}\n",
            "{'epoch': 1, 'batch': 400, 'loss': 7.217736127376557}\n",
            "{'epoch': 1, 'batch': 500, 'loss': 7.16439741230011}\n",
            "{'epoch': 1, 'batch': 600, 'loss': 7.12491729815801}\n",
            "{'epoch': 1, 'batch': 700, 'loss': 7.0963709354400635}\n",
            "{'epoch': 1, 'batch': 800, 'loss': 7.071686939001084}\n",
            "{'epoch': 1, 'batch': 900, 'loss': 7.048713848325941}\n",
            "{'epoch': 1, 'batch': 1000, 'loss': 7.026466125488281}\n",
            "{'epoch': 1, 'batch': 1100, 'loss': 7.00355760530992}\n",
            "{'epoch': 1, 'batch': 1200, 'loss': 6.98232726931572}\n",
            "{'epoch': 1, 'batch': 1300, 'loss': 6.959851621481088}\n",
            "{'epoch': 1, 'batch': 1400, 'loss': 6.937849579538618}\n",
            "{'epoch': 1, 'batch': 1500, 'loss': 6.915553883552551}\n",
            "{'epoch': 1, 'batch': 1600, 'loss': 6.894214717149734}\n",
            "{'epoch': 1, 'batch': 1700, 'loss': 6.872571383925045}\n",
            "{'epoch': 1, 'batch': 1800, 'loss': 6.851409888267517}\n",
            "{'epoch': 1, 'batch': 1900, 'loss': 6.831573239125704}\n",
            "{'epoch': 1, 'batch': 2000, 'loss': 6.8123837215900425}\n",
            "{'epoch': 1, 'batch': 2100, 'loss': 6.7947426046643935}\n",
            "{'epoch': 1, 'batch': 2200, 'loss': 6.77675473538312}\n",
            "{'epoch': 1, 'batch': 2300, 'loss': 6.760553240154101}\n",
            "{'epoch': 1, 'batch': 2400, 'loss': 6.744435278574626}\n",
            "{'epoch': 1, 'batch': 2500, 'loss': 6.729141010284424}\n",
            "{'epoch': 1, 'batch': 2600, 'loss': 6.713498516266163}\n",
            "{'epoch': 1, 'batch': 2700, 'loss': 6.6999831524601685}\n",
            "{'epoch': 1, 'batch': 2800, 'loss': 6.685956221989223}\n",
            "{'epoch': 1, 'batch': 2900, 'loss': 6.67255041730815}\n",
            "{'epoch': 1, 'batch': 3000, 'loss': 6.659417018731435}\n",
            "saving model\n",
            "{'epoch': 1, 'batch': 3100, 'loss': 6.646058694931769}\n",
            "{'epoch': 1, 'batch': 3200, 'loss': 6.633871586471796}\n",
            "{'epoch': 1, 'batch': 3300, 'loss': 6.621497884085684}\n",
            "{'epoch': 1, 'batch': 3400, 'loss': 6.610234248497907}\n",
            "{'epoch': 1, 'batch': 3500, 'loss': 6.599318485668728}\n",
            "{'epoch': 1, 'batch': 3600, 'loss': 6.589335964785682}\n",
            "{'epoch': 1, 'batch': 3800, 'loss': 6.568716389756454}\n",
            "{'epoch': 1, 'batch': 3900, 'loss': 6.559449975184905}\n",
            "{'epoch': 1, 'batch': 4000, 'loss': 6.549976138472557}\n",
            "{'epoch': 1, 'batch': 4100, 'loss': 6.5409492474067505}\n",
            "{'epoch': 1, 'batch': 4200, 'loss': 6.531749998274304}\n",
            "{'epoch': 1, 'batch': 4300, 'loss': 6.523299597030462}\n",
            "{'epoch': 1, 'batch': 4400, 'loss': 6.515045757510445}\n",
            "{'epoch': 1, 'batch': 4500, 'loss': 6.506472871780396}\n",
            "{'epoch': 1, 'batch': 4600, 'loss': 6.498354338977648}\n",
            "{'epoch': 1, 'batch': 4700, 'loss': 6.489798292910799}\n",
            "{'epoch': 1, 'batch': 4800, 'loss': 6.482579457362493}\n",
            "{'epoch': 1, 'batch': 4900, 'loss': 6.47490550284483}\n",
            "{'epoch': 1, 'batch': 5000, 'loss': 6.467240496444703}\n",
            "{'epoch': 1, 'batch': 5100, 'loss': 6.460187469837712}\n",
            "{'epoch': 1, 'batch': 5200, 'loss': 6.453374771246543}\n",
            "{'epoch': 1, 'batch': 5300, 'loss': 6.446184008976199}\n",
            "{'epoch': 1, 'batch': 5400, 'loss': 6.439320030389009}\n",
            "{'epoch': 1, 'batch': 5500, 'loss': 6.432764270695773}\n",
            "{'epoch': 1, 'batch': 5600, 'loss': 6.426348718319621}\n",
            "{'epoch': 1, 'batch': 5700, 'loss': 6.420140229526319}\n",
            "{'epoch': 1, 'batch': 5800, 'loss': 6.414312804156336}\n",
            "{'epoch': 1, 'batch': 5900, 'loss': 6.408276801109314}\n",
            "{'epoch': 1, 'batch': 6000, 'loss': 6.402805308500926}\n",
            "saving model\n",
            "{'epoch': 1, 'batch': 6100, 'loss': 6.397143767622651}\n",
            "{'epoch': 1, 'batch': 6200, 'loss': 6.391583807237686}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdQFxRldqvBC"
      },
      "source": [
        "model.load_state_dict(torch.load(filepath + 'kdnuggets_preprocess1',map_location=torch.device('cpu')))\n",
        "    model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9HnXR8l86mh"
      },
      "source": [
        "print(predict(dataset, model, text='What'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_CadDaE86mi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58d5bfb6-50cb-4547-f3cc-bf57e28d16bc"
      },
      "source": [
        "print(predict(dataset, model, text='invest in'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['invest', 'in', 'too.r)Õ\\x03\\x00X-\\x00\\x00\\x00tendies', 'added', 'the', 'country', \"it's\", 'now', 'using', 'his', 'options', 'are', 'down', 'here', 'when', 'you', 'supply', 'pound', 'unpartisan.r~á\\x00\\x00XM\\x00\\x00\\x00Its', 'Samsung', 'Sales', 'Flags,', 'Check', 'charges', 'then', 'close,', 'huge', 'time', 'and', 'proceeds', 'to', \"it's\", 'the', 'best', 'week', 'on', 'the', 'club', 'meant', 'to', 'hang', 'into', 'today,', '\\x00\\x00\\x00&amp;#x200B;\\n\\n**This', 'do', 'over', 'welfare', 'for', 'data', 'for', 'a', 'requirement', 'index', 'trap', 'and', 'as', 'leaked', 'when', 'it', 'on', 'us', 'when', 'RKT', 'can', 'brag', 'if', 'she', \"doesn't\", 'be', 'what', 'print', 'more', 'kinds', 'of', 'my', 'least', 'itâ\\x80\\x99s', 'not', 'a', 'couple', 'lotta', 'Earnings', '\\\\[2/3', 'Volatility\\\\]\\n*', '7:00', '9/18', '$NKLA', '$NKLA', '$NKLA', '250', '9/18', '$NKLA', '250', '9/18', '$NKLA', '250', 'blah', 'done', 'or', 'â\\x80\\x9chow', 'is', 'a']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzY6rWiN86mi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c371518e-c3ce-406a-8eea-2790dd601b48"
      },
      "source": [
        "print(predict(dataset, model, text='deal'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['deal', 'margarine', 'baron', 'sheriff', 'in', 'another', 'year', 'or', 'that,', 'he', 'says', 'five', 'bj', 'on', '3', 'months', 'honestly', 'loss,', 'just', 'would', 'do', 'it', 'at', 'truly', 'signed', 'securities', 'but', 'he', 'squished', 'issued', 'include', 'hooked', 'with', 'bullets', 'in', 'buys', 'time', 'for', '1/2', 'of', 'all', 'of', 'members', 'â\\x9c\\x85', 'TA,', 'am', 'bUy', 'MOVERS:\\n\\n######(**source:', 'fold', 'my', 'head', 'from', 'the', 'airport', 'intraday', 'the', 'S&amp;P:**\\n######', '***Monday', 'annually', 'and', 'buy', 'calls', 'to', 'shouting', 'are', 'underestimate', 'infinite', 'huge', 'hands', 'that', 'you', 'think', 'Elon', 'Iâ\\x80\\x99m', 'pink', 'thing', 'to', 'live', '10', 'opinion', 'is', '16.4%', 'by', 'Washington,', 'my', 'desk', 'or', 'major', 'pride', 'if', \"it's\", 'a', 'natural', \"homes.r\\x8c*\\n\\x00XL\\x00\\x00\\x00That's\", '2', '/u/ControlTheNarrative', 'I', 'would', 'want', 'to', 'have']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7DR_AjAf1jR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}