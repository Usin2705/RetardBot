# RetardBot
Reddit's WallStreetBet commment generation project<br>
Team members: Nhan Phan, Ryoko Noda.

## About the project
This project was done for Aalto University's Statistical Natural Language Processing course. 
The project report can be found [here](https://github.com/Usin2705/RetardBot/blob/master/Project_Report.pdf).

The repository contains the codes and data we used to create four language models that replicates the posts from Reddit's [WallStreetBets](https://www.reddit.com/r/wallstreetbets/).
Once put together, the models should be able to generate somewhat WallStreetBets-like sentences like the ones below. 
These are real sentences generated by our Bi-LSTM model, which we posted on WallStreetBets and then removed immediately (user name is hidden for privacy).

<img src="https://github.com/Usin2705/RetardBot/blob/master/illustration/sample_post_for_readme.png" width="600">
Files here are mainly of four categories:

1. The datasets
1. Codes to scrape the dataset
1. Notebook for data cleaning
1. Notebooks for language models

The repository contains codes we used for experimental purposes before finalizing the project. The codes we used in the final version are:

* WSBpmaw.py
* step_01_data_preprocessing.ipynb
* step_02_ngrams.ipynb
* step_03_RNN.ipynb

## The datasets
The datasets scraped from WallStreetBets can be found in the [data](https://github.com/Usin2705/RetardBot/tree/master/data) folder.
This folder contains datasets of various lengths (+ one dataset that we used in an experiment that is not from WallStreetBets).

__data_sample_4x.txt:__ The full dataset, containing 80,000 sentences.<br>
__data_sample_2x.txt:__ The half-size dataset, 40,000 sentences.<br>
__data_sample.txt:__ The quarter-size dataset, 20,000 sentences.<br>
__data_sample_test.txt:__ A very small dataset of 1,000 sentences. Can be used for tests.<br>
__reddit-cleanjokes.csv__: A dataset used to run the sample LSTM models. NOT FROM WALLSTREETBETS.

## Web scraping codes
There are two web scraping codes, one of which we abandoned after we found Pushshift.

__WSBpmaw.py:__ The code used in the final version, which uses the Pushshift wrapper [PMAW](https://github.com/mattpodolak/pmaw).<br>
__WSBPraw.py:__ The code that uses the more popular [PRAW](https://praw.readthedocs.io/en/latest/).

## Data preprocessing
There is only one file that we used when preprocessing the data.

__step_01_data_preprocessing.ipynb:__ Preprocesses the WallStreetBets datasets.

## Language models
We tried four models in this project: n-grams, GRU, LSTM, and Bi-LSTM. 
The n-grams model has a Jupyter notebook to itself, and the GRU, LSTM, and Bi-LSTM models can be found in a generic RNN notebook in which you can choose what model to use.

__step_02_ngrams.ipynb:__ The n-grams code for the final version. It contains 5-grams with absolute smoothing.<br>
__step_03_RNN.ipynb:__ The generic RNN model used in the final version. You can choose GRU, LSTM, or Bi-LSTM within the notebook.<br>
__test_LSTM_kdnuggets.ipynb:__ [A LSTM model from KDnuggets](https://www.kdnuggets.com/2020/07/pytorch-lstm-text-generation-tutorial.html) that we used to learn what LSTM is like.<br>
__test_LSTM_kdn_preprocess.ipynb:__ An experimental model where we added some data preprocessing to test_LSTM_kdnuggets.ipynb.
