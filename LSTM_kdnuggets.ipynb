{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The LSTM model shown in the KDnuggets article\n",
    "\n",
    "https://www.kdnuggets.com/2020/07/pytorch-lstm-text-generation-tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters needed to run the model\n",
    "#these originally needed to be specified from the terminal\n",
    "sequence_length = 4 #Default = 4 \n",
    "batch_size = 256 #Default = 256 Reduce if PC don't have enough RAM\n",
    "max_epochs = 400 #Default = 10\n",
    "device = torch.device('cuda:0')\n",
    "#device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on the model architecture\n",
    "\n",
    "Based on model from https://www.kdnuggets.com/2020/07/pytorch-lstm-text-generation-tutorial.html\n",
    "\n",
    "The model has three components:\n",
    "1. **Embedding layer:** converts input of size (batch_size, sequence_length) to embedding of size (batch_size, sequence_length, embedding_dim)\n",
    "2. **Stacked LSTM of 3 layers:** accepts embedding and a tuple (previous hidden state, previous cell state) and gives an output of size (batch_size, sequence_length, embedding_dim) and the tuple (current hidden state, current cell state). The hidden state and cell state both have size (num_layers, sequence_length, embedding_dim).\n",
    "3. **Linear layer:** Maps the output of LSTM to logits for each word in vocab. Not a probability yet. Output size is  (batch_size, sequence_length, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, dataset):\n",
    "        super(Model, self).__init__()\n",
    "        self.lstm_size = 128\n",
    "        self.embedding_dim = 128\n",
    "        self.num_layers = 3 #stack 3 LSTM layers for abstract representation\n",
    "\n",
    "        n_vocab = len(dataset.uniq_words)\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=n_vocab,\n",
    "            embedding_dim=self.embedding_dim,\n",
    "        )\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.lstm_size,\n",
    "            hidden_size=self.lstm_size,\n",
    "            num_layers=self.num_layers,\n",
    "            dropout=0.1,\n",
    "        )\n",
    "        self.fc = nn.Linear(self.lstm_size, n_vocab)\n",
    "\n",
    "    def forward(self, x, prev_state):\n",
    "        embed = self.embedding(x)\n",
    "        output, state = self.lstm(embed, prev_state)\n",
    "        logits = self.fc(output)\n",
    "        return logits, state\n",
    "\n",
    "    def init_state(self, sequence_length):\n",
    "        return (torch.zeros(self.num_layers, sequence_length, self.lstm_size).to(device),\n",
    "                torch.zeros(self.num_layers, sequence_length, self.lstm_size).to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on the custom dataset\n",
    "\n",
    "According to the Pytorch documentation, a custom dataset needs at least the functions \\_\\_len\\_\\_ and \\_\\_getitem\\_\\_. \\_\\_len\\_\\_\\_ allows len(dataset) to return the size of the dataset and  \\_\\_getitem\\_\\_ allows the ith element of the dataset to be fetched with dataset\\[i\\].\n",
    "\n",
    "In this custom dataset, \\_\\_len\\_\\_ and \\_\\_getitem\\_\\_ are designed like this. Let's say the only sentence we have in the dataset is:\n",
    "\n",
    "__*We are using LSTM to create the Retard-bot language model.*__\n",
    "\n",
    "__\\_\\_len\\_\\_:__<br>\n",
    "For this custom dataset it's defined as \"the size of the dataset - sequence length\". This is probably because this model is created to make predictions based the first 4 words (default sequence length) given as prompt, but I can't say for certain. So in the example sentence above, it will return  the length of \"**to create the Retard-bot language model.**\"\n",
    "\n",
    "__\\_\\_getitem\\_\\_:__<br>\n",
    "It seems that this returns a tuple of n-grams with the n defined by sequence length. So if we say dataset\\[0\\] in the simple example, we would get (**We are using LSTM**, **are using LSTM to**). Not sure why it does this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        sequence_length\n",
    "    ):\n",
    "        \"\"\"\n",
    "        words:                 words in entire dataset split by whitespace\n",
    "        uniq_words:       the unique words sorted by frequency (most frequent first)\n",
    "        index_to_word: index to word dict {index0: word0, index1:word1...}, most frequent have smaller index\n",
    "        word_to_index: word to index dict {word0: index0, word1:index1...}, most frequent have smaller index\n",
    "        words_indexes:  the words converted to their indices using word_to_index\n",
    "        \"\"\"\n",
    "        self.sequence_length = sequence_length\n",
    "        self.words = self.load_words()\n",
    "        self.uniq_words = self.get_uniq_words()\n",
    "        self.index_to_word = {index: word for index, word in enumerate(self.uniq_words)}\n",
    "        self.word_to_index = {word: index for index, word in enumerate(self.uniq_words)}\n",
    "        self.words_indexes = [self.word_to_index[w] for w in self.words]\n",
    "        \n",
    "    def load_words(self):\n",
    "        train_df = pd.read_csv('reddit-cleanjokes.csv')\n",
    "        text = train_df['Joke'].str.cat(sep=' ')\n",
    "        return text.split(' ')\n",
    "    \n",
    "    def get_uniq_words(self):\n",
    "        word_counts = Counter(self.words)\n",
    "        return sorted(word_counts, key=word_counts.get, reverse=True) \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.words_indexes) - self.sequence_length\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return (\n",
    "            torch.tensor(self.words_indexes[index:index+self.sequence_length]),\n",
    "            torch.tensor(self.words_indexes[index+1:index+self.sequence_length+1]),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(sequence_length)\n",
    "model = Model(dataset)\n",
    "model = model.to(device)\n",
    "\n",
    "def train(dataset, model):\n",
    "    model.train()\n",
    "    \n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle = True) # NEED TO SHUFFLE AND RERUN\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        state_h, state_c = model.init_state(sequence_length)\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for i, batch in enumerate(dataloader):\n",
    "            x, y = batch\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "            loss = criterion(y_pred.transpose(1, 2), y)\n",
    "            \n",
    "            state_h = state_h.detach()\n",
    "            state_c = state_c.detach()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        if ((epoch+1)%10) == 0:\n",
    "            print({ 'epoch': epoch+1, 'loss': epoch_loss/(i+1) })\n",
    "    print({ 'epoch': epoch+1, 'loss': epoch_loss/(i+1) })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dataset, model, text, next_words=100):\n",
    "    model.eval()\n",
    "    \n",
    "    words = text.split(' ')\n",
    "    state_h, state_c = model.init_state(len(words))\n",
    "    \n",
    "    for i in range(0, next_words):\n",
    "        x = torch.tensor([[dataset.word_to_index[w] for w in words[i:]]], device=device)\n",
    "        y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "        \n",
    "        last_word_logits = y_pred[0][-1]\n",
    "        p = torch.nn.functional.softmax(last_word_logits, dim=0).detach().cpu().numpy()\n",
    "        \n",
    "        word_index = np.random.choice(len(last_word_logits), p=p)\n",
    "        words.append(dataset.index_to_word[word_index])\n",
    "        \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of model:\n",
    "#kdnuggests400_pure.pt (no dropout)\n",
    "#kdnuggests400_dropout.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 10, 'loss': 6.050929145610079}\n",
      "{'epoch': 20, 'loss': 5.413591313869395}\n",
      "{'epoch': 30, 'loss': 4.985555618367297}\n",
      "{'epoch': 40, 'loss': 4.708034363198788}\n",
      "{'epoch': 50, 'loss': 4.503136847881561}\n",
      "{'epoch': 60, 'loss': 4.337051244492226}\n",
      "{'epoch': 70, 'loss': 4.191773130538616}\n",
      "{'epoch': 80, 'loss': 4.056973746482362}\n",
      "{'epoch': 90, 'loss': 3.9318155532187604}\n",
      "{'epoch': 100, 'loss': 3.798764003084061}\n",
      "{'epoch': 110, 'loss': 3.6571868100064866}\n",
      "{'epoch': 120, 'loss': 3.5183957079623607}\n",
      "{'epoch': 130, 'loss': 3.399082346165434}\n",
      "{'epoch': 140, 'loss': 3.2919927353554583}\n",
      "{'epoch': 150, 'loss': 3.184942935375457}\n",
      "{'epoch': 160, 'loss': 3.0972471668365156}\n",
      "{'epoch': 170, 'loss': 3.0171943142059003}\n",
      "{'epoch': 180, 'loss': 2.944134554964431}\n",
      "{'epoch': 190, 'loss': 2.8773856619571117}\n",
      "{'epoch': 200, 'loss': 2.8266189656359084}\n",
      "{'epoch': 210, 'loss': 2.7783442253762103}\n",
      "{'epoch': 220, 'loss': 2.7365453826620225}\n",
      "{'epoch': 230, 'loss': 2.7062484634683486}\n",
      "{'epoch': 240, 'loss': 2.6730356089612273}\n",
      "{'epoch': 250, 'loss': 2.645575274812414}\n",
      "{'epoch': 260, 'loss': 2.621062035256244}\n",
      "{'epoch': 270, 'loss': 2.601691385532947}\n",
      "{'epoch': 280, 'loss': 2.577843638176614}\n",
      "{'epoch': 290, 'loss': 2.5626026290528316}\n",
      "{'epoch': 300, 'loss': 2.548346326706257}\n",
      "{'epoch': 310, 'loss': 2.5350197350725217}\n",
      "{'epoch': 320, 'loss': 2.518898182726921}\n",
      "{'epoch': 330, 'loss': 2.507770008229195}\n",
      "{'epoch': 340, 'loss': 2.4959299538997892}\n",
      "{'epoch': 350, 'loss': 2.4888842537048017}\n",
      "{'epoch': 360, 'loss': 2.4769689580227467}\n",
      "{'epoch': 370, 'loss': 2.469414804844146}\n",
      "{'epoch': 380, 'loss': 2.4641928520608456}\n",
      "{'epoch': 390, 'loss': 2.453463318500113}\n",
      "{'epoch': 400, 'loss': 2.447349652330926}\n",
      "{'epoch': 400, 'loss': 2.447349652330926}\n"
     ]
    }
   ],
   "source": [
    "if is_training:\n",
    "    train(dataset, model)\n",
    "    file_name = 'kdnuggests' + str(max_epochs) + '.pt'\n",
    "    torch.save(model.state_dict(), file_name)\n",
    "else:\n",
    "    #file_name = \n",
    "    model.load_state_dict(torch.load(file_name, map_location=lambda storage, loc: storage))\n",
    "    model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Knock', 'knock.', 'Whos', 'there?', 'and', 'arms..', 'because', 'I', 'threw', 'my', 'sleep,', 'like', 'to', 'dry?', '\"I\\'m', 'afraid', 'of', 'Windsor', 'let', 'himself', 'go?', 'Flabio', 'What', 'did', 'the', 'most', 'accidents', 'happen', 'within', 'a', 'girl', 'Living', 'in', 'lairs.', 'What', 'type', 'of', 'the', 'end', 'of', 'razor', 'like', 'the', 'heck', 'he', 'brought', 'a', 'beginning.', '-ahem-', 'Just', 'a', 'poorly', 'dressed', 'as', \"'Jallikatu\", \"Bulls'.\", 'Did', 'you', 'know', 'wot', 'to', 'the', 'line', '-', 'Impatient', 'co-', '-', 'Impatient', 'cow.', 'Interrup........', 'MOOOOOOOOOOOOOOOO!!!!', '[Works', 'little', 'sister', 'told', 'joke:', 'What', 'do', 'a', 'tuna!', 'Person', 'working', 'with', 'ewe', 'people!?', 'What', 'to', 'the', 'comedy', 'routine?', 'Deadpan.', 'Two', 'antennas', 'met', 'Phil', \"Spector's\", 'brother', 'Crispin', 'the', 'sand', 'shortages', 'will', 'ask', 'questions!!']\n"
     ]
    }
   ],
   "source": [
    "print(predict(dataset, model, text='Knock knock. Whos there?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What', 'did', 'the', 'music', 'do', 'you', 'call', 'an', 'X', 'chromosomes.', 'Typical', 'woman.', 'If', 'you', 'might', 'say.', '**Jimmy', 'Carr**', 'What', 'do', 'you', 'run', 'around', 'in', 'salad', 'dressing.', 'My', 'English', 'teacher', 'tells', 'do', 'noodles', 'get', 'when', 'he', 'said...', '\"make', 'it', 'turned', 'out', 'of', 'soap-', '-so', 'of?', 'Autumn', 'Leaves.', \"What's\", 'the', 'roots', 'of', 'cheese?', 'There', 'once', 'thought', 'I', 'hate', 'in', 'the', 'Italian', 'for', 'New', 'Year', 'resolution?', 'Well,', \"it's\", 'past', 'tents.', 'What', 'do', 'you', 'call', 'a', 'bar...', '...and', 'pulled', 'shaving', 'a', 'mile', 'between', 'you', 'mix', 'Michael', 'Jordan', 'with', 'my', 'neck-', '-and', 'they', 'got', 'clucky.', 'What', 'did', 'the', 'silly', 'thing', 'to', 'the', 'dinosaur', 'FBI', 'agent?', 'A', 'Cairopractor!', 'Why']\n"
     ]
    }
   ],
   "source": [
    "print(predict(dataset, model, text='What did the'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Why', 'did', 'the', 'chicken', 'cross', 'the', 'road', 'for', 'his', 'bike', 'away.', 'Two', 'fish', 'say', 'to', 'drive', 'this', 'boat\".', 'The', 'computer', 'CPU', 'say', \"'Control\", 'Freak', 'who?\\'\"', ':)', 'A', 'gramma', 'ray', 'Bee', 'jokes,', 'courtesy', 'of', 'security', 'guards', 'in', 'the', 'word', 'in', 'the', 'raisin', 'A', 'skeleton', 'who', 'chokes', 'on', 'her', 'up', 'in', 'the', 'unthinkable?', 'With', 'a', 'old', 'fruit-picker', 'in', 'the', 'dog', 'to', 'the', 'boy', 'tree?', 'Sycamore.', 'Why', 'should', 'you', 'play', 'a', 'bike', 'away.', 'What', 'do', 'you', 'call', 'it', 'Friday.', 'How', 'many', 'mistakes...', \"What's\", 'the', 'corner', 'of', 'razor', 'like', 'camping', 'but...', \"I'm\", 'alright', 'you', 'call', '555-bottom-feeders.', 'We', 'will', 'make', 'a', 'few', 'hours', 'at', 'your', 'nose', 'because', 'they', 'take', 'at', 'the', 'reception']\n"
     ]
    }
   ],
   "source": [
    "print(predict(dataset, model, text='Why did the chicken cross the road'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'ask', 'a', 'pie.', '(sounds', 'like', 'hares!', 'I', 'made', 'here\"', 'A', 'man', 'started', 'to', 'me.', 'Wanna', 'hear', 'about', 'the', 'opposite', 'of', 'water.', 'What', 'do', 'you', 'call', 'a', 'bar....', 'So', 'I', 'made', 'that', \"can't\", 'you', 'call', 'a', 'rock', 'do', 'you', \"don't\", 'tennis', 'players', 'who', 'hated', 'negative', 'numbers?', \"He'll\", 'up', 'everything', 'blurry.', 'What', 'do', 'you', 'call', 'a', 'bloated', 'appendix.', 'A', 'Poptometrist!', \"What's\", 'was', 'dedicated', 'to', 'the', 'new', 'TV', 'playback', 'craziness', '[Through', 'the', 'second', 'they', 'have', 'eight', 'empty', 'pack', 'boogered', 'up.', 'Please', 'give', 'his', 'leg?', 'Limp', 'Biscuit', 'Better', 'be', 'named', 'after', 'working', 'took', 'the', 'mechanic', 'Want', 'to', 'run', 'around', 'to', 'the', 'lettuce', 'get', '25', 'the']\n"
     ]
    }
   ],
   "source": [
    "print(predict(dataset, model, text='I ask'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
