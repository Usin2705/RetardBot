{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn # the neural network package that contains functions for creating the neural network layers\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim # a package that allows use to use an optimizer in order to update the parameters during training\n",
    "from torch.utils.data import DataLoader # allows use to process the data in batches\n",
    "from torch.nn.utils.rnn import pad_sequence # a function that zero-pads the sentences so they can have equal size in a batch\n",
    "\n",
    "import pickle\n",
    "torch.manual_seed(0) # set a random seed for reproducibility\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "is_preprocessing_LSTM = False #This toggle preprocessing for LSTM model (not preprocessing for text)\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "#device = torch.device('cpu')\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Sentence boundaries\n",
    "\n",
    "When dealing with language, it is good to know when a sentence starts and when it ends. That will help the model at the beginning of the prediction, when we don't have any previous words as context. For that purpose, we are going to pad each sentence with a start-of-sentence symbol _\"&lt;s>\"_ and an end-of-sentence symbol _\"&lt;/s>\"_. \n",
    "\n",
    "Since you already did a similar thing in the n-grams exercise, this function is already implemented for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sentence_boundaries(data):\n",
    "    \"\"\"\n",
    "    Takes the data, where each line is a sentence, appends <s> token at the beginning and </s> at the end of each sentence\n",
    "    Example input: I live in Helsinki\n",
    "    Example output: <s> I live in Helsinki </s>\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    data : list\n",
    "            a list of sentences\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    res : list\n",
    "            a list of sentences, where each sentence has <s> at the beginning and </s> at the end\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    for sent in data:\n",
    "        sent = '<s> ' + sent.rstrip() + ' </s>'\n",
    "        res.append(sent)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index dictionaries <a class=\"anchor\" id=\"task_1_1\"></a> \n",
    "Neural networks can't process words as raw strings. Due to that, we need to represent the words with numbers. The first step in doing that is creating two dictionaries: word2idx and idx2word.\n",
    "\n",
    "The word2idx dictionary contains unique words as keys and unique indices for each of the words as values. <br>\n",
    "The idx2word dictionary contains unique indices as keys and unique words for each of those indices as values. It is essentially a reversed word2dx, where the keys are the values and the values are the keys.\n",
    "\n",
    "Example sentences: [\"I look forward\", \"You look forward\"] <br>\n",
    "word2idx = {\"I\": 1, \"look\": 2, \"forward\": 3, \"You\": 4} <br>\n",
    "idx2word = {1: \"I\", 2: \"look\", 3: \"forward\", 4: \"You\"} <br>\n",
    "\n",
    "Write a function that creates two dictionaries: word2idx and idx2work. The dictionaries should contain all the unique words in the data. <b>The indices should start from 1 and not from 0<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_indices(data):\n",
    "    \"\"\"\n",
    "    This function creates two dictionaries: word2idx and idx2word, containing each unique word in the dataset\n",
    "    and its corresponding index.\n",
    "    Remember that the starting index should be 1 and not 0\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    data - list\n",
    "            a list of sentences, where each sentence starts with <s>\n",
    "            and ends with </s> token\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    word2idx - dictionary\n",
    "                a dictionary, where the keys are the words and the values are the indices\n",
    "                \n",
    "    idx2word - dictionary\n",
    "                a dictionary, where the keys are the indices and the values are the words\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "    word2idx = dict()\n",
    "    idx2word = dict()\n",
    "    \n",
    "    data_list = ''\n",
    "    for sentence in data:\n",
    "        data_list = data_list + ' ' + sentence\n",
    "\n",
    "    data_list = data_list[1:]\n",
    "    data_split = data_list.split(' ')\n",
    "    data_unique = []\n",
    "    for word in data_split:\n",
    "        if word not in data_unique:\n",
    "            data_unique.append(word)\n",
    "            if word not in word2idx.keys():\n",
    "                 word2idx[word] = data_unique.index(word)+1\n",
    "    \n",
    "    for key, value in word2idx.items():\n",
    "        idx2word[value] = key\n",
    "    \n",
    "    return word2idx, idx2word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index data <a class=\"anchor\" id=\"task_1_2\"></a>\n",
    "After we have created the word2idx and idx2word dictionaries, it is time to index the data. In other words, we need to replace each word in the data with its corresponding index.\n",
    "\n",
    "Write a function that reads each sentence from the data and replaces each word in the sentence with its index from the word2idx dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_data(data, word2idx):\n",
    "    \"\"\"\n",
    "    This function replaces each word in the data with its corresponding index\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    data - list\n",
    "            a list of sentences, where each sentence starts with <s>\n",
    "            and ends with </s> token\n",
    "    \n",
    "    word2idx - dict\n",
    "            a dictionary where the keys are the unique words in the data\n",
    "            and the values are the unique indices corresponding to the words%\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    data_indexed - list\n",
    "                a list of sentences, where each word in the sentence is replaced with its index\n",
    "    \"\"\"\n",
    "    \n",
    "    data_indexed = []\n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "    \n",
    "    for sentence in data:\n",
    "        sentence_index = []\n",
    "        for word in sentence.split(' '):\n",
    "            sentence_index.append(word2idx[word])\n",
    "        data_indexed.append(sentence_index)\n",
    "    \n",
    "\n",
    "    return data_indexed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert sentences to tensors\n",
    "\n",
    "This function converts each indexed sentence to a LongTensor data type. This is required in order to process it later using Pytorch.\n",
    "\n",
    "You don't have to modify this function. It is already implemented for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tensor(data_indexed):\n",
    "    \"\"\"\n",
    "    This function converts the indexed sentences to LongTensors\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    data_indexed - list\n",
    "            a list of sentences, where each word in the sentence\n",
    "            is replaced by its index\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tensor_array - list\n",
    "                a list of sentences, where each sentence\n",
    "                is a LongTensor\n",
    "    \"\"\"\n",
    "    \n",
    "    tensor_array = []\n",
    "    for sent in data_indexed:\n",
    "        tensor_array.append(torch.LongTensor(sent))    \n",
    "        \n",
    "    return tensor_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine features and labels in a tuple\n",
    "\n",
    "This function combines each indexed sentence and its corresponding labels to a tuple. This will be beneficial for us when we zero-pad the data later, in order to make the batches have equal-length samples.\n",
    "\n",
    "You don't have to modify this function. It is already implemented for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_data(input_data, labels_data):\n",
    "    \"\"\"\n",
    "    This function converts the input features and the labels into tuples\n",
    "    where each tuple corresponds to one sentence in the format (features, labels)\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    input_data - list\n",
    "            a list of tensors containing the training features\n",
    "    \n",
    "    labels_data - list\n",
    "            a list of tensors containing the training labels\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    res - list\n",
    "            a list of tuples, where each tuple corresponds to one sentece pair\n",
    "            in the format (features, labels)\n",
    "    \"\"\"\n",
    "    \n",
    "    res = []\n",
    "    \n",
    "    for i in range(len(input_data)):\n",
    "        res.append((input_data[i], labels_data[i]))\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove extra data\n",
    "\n",
    "Since we will be processing the data in equal batches during training, we need to make sure that each batch has equal number of sentences. In case the last batch contains less sentences than the batch size, that batch will be discarded.\n",
    "\n",
    "This function discards the extra data that doesn't fit in a batch.\n",
    "\n",
    "You don't have to modify this function. It is already implemented for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extra(data, batch_size):\n",
    "    \"\"\"\n",
    "    This function removes the extra data that does not fit in a batch   \n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    data - list\n",
    "            a list of tuples, where each tuple corresponds to a\n",
    "            sentence in a format (features, labels)\n",
    "            \n",
    "    batch_size - integer\n",
    "                    the size of the batch\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    data - list\n",
    "            a list of tuples, where each tuple corresponds to a\n",
    "            sentence in a format (features, labels)\n",
    "    \"\"\"\n",
    "    \n",
    "    extra = len(data) % batch_size\n",
    "    if extra != 0:\n",
    "        data = data[:-extra][:]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-pad the data\n",
    "\n",
    "In order to process the data in batches, we need to make sure that the sentences in each batch have equal lengths. Since we are working with sentences, each sentence in a batch can have different number of words. In this case, we need to  make the length of each sentence the same as the length of the longest sentence in that batch. We do that by adding zeros at the end of each sentence, until the sentence has equal length as the longest one in the batch.\n",
    "\n",
    "This function implements the zero-padding.\n",
    "\n",
    "You don't have to modify this function. It is already implemented for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(list_of_samples):\n",
    "    \"\"\"\n",
    "    This function zero-pads the training data in order to process the sentences\n",
    "    in a batch during training\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    list_of_samples - list\n",
    "                        a list of tuples, where each tuple corresponds to a\n",
    "                        sentence in a format (features, labels)\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pad_input_data - tensor\n",
    "                        a tensor of input features equal to the batch size,\n",
    "                        where features are zero-padded to have equal lengths\n",
    "                        \n",
    "    input_data_lengths - list\n",
    "                        a list where each element is the length of the \n",
    "                        corresponding sentence\n",
    "    \n",
    "    pad_labels_data - tensor\n",
    "                        a tensor of labels equal to the batch size,\n",
    "                        where labels are zero-padded to have equal lengths\n",
    "            \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    list_of_samples.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    input_data, labels_data = zip(*list_of_samples)\n",
    "\n",
    "    input_data_lengths = [len(seq) for seq in input_data]\n",
    "    \n",
    "    padding_value = 0\n",
    "\n",
    "    # pad input\n",
    "    pad_input_data = pad_sequence(input_data, padding_value=padding_value)\n",
    "    \n",
    "    # pad labels\n",
    "    pad_labels_data = pad_sequence(labels_data, padding_value=padding_value)\n",
    "\n",
    "    return pad_input_data, input_data_lengths, pad_labels_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare features and labels <a class=\"anchor\" id=\"task_1_3\"></a> \n",
    "During training, the model takes an input word and outputs a prediction. We will need to compare this prediction to 'true label'. True label is just the next word in the text, but we will need to organize the data, so that every word in the text is considered as this 'true label'.\n",
    "\n",
    "In the label sentence, every word is moved a step in time, and for the input sentence the last word is missing. \n",
    "\n",
    "Example sentence: oops i did it again <br>\n",
    "INPUT: oops i did it <br>\n",
    "LABEL: i did it again\n",
    "\n",
    "Note: the first word in the sentence is start-of-sentence symbol and the last one is end-of-sentence symbol.\n",
    "\n",
    "Write a function that takes as input the indexed data and returns two arrays: the input array where the last word from each sentence is missing, and the label array, where every word is moved a step in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_training(data_indexed):\n",
    "    \"\"\"\n",
    "    This function creates the input features and their corresponding labels\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    data_indexed - list\n",
    "            a list of sentences, where each word in the sentence\n",
    "            is replaced by its index\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    input_data - list\n",
    "            a list of indexed sentences, where the last element of each sentence is removed\n",
    "            \n",
    "    labels_data - list\n",
    "            a list of indexed sentences, where the first element of each sentence is removed\n",
    "    \"\"\"\n",
    "    \n",
    "    input_data = []\n",
    "    labels_data = []\n",
    "\n",
    "     # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "    for data in data_indexed:    \n",
    "        input_data.append(data[:-1])\n",
    "        labels_data.append(data[1:])\n",
    "    \n",
    "    return input_data, labels_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data <a class=\"anchor\" id=\"task_1_4\"></a>\n",
    "At this point, we have all the necessary functions to prepare the data for training. What is left to do is to run them one by one and get the data in the desired format.\n",
    "\n",
    "Write a function that takes the data and prepares it for training. You need to do the following steps:\n",
    "\n",
    "    1. Add sentence boundaries\n",
    "    2. Create index dictionaries (word2idx and idx2word)\n",
    "    3. Index the data in a way that each word is replaced by its index\n",
    "    4. Convert the indexed data to a list of tensors, where each tensor is a sentence\n",
    "    5. Split each sentence to input and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    \"\"\"\n",
    "    This function runs the whole preprocessing pipeline and returns the prepared\n",
    "    input features and labels, along with the word2idx and idx2word dictionaries\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    data - list\n",
    "            a list of sentences that need to be prepared for training\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    input_data - list\n",
    "            a list of tensors, where each tensor is an indexed sentence used as input feature\n",
    "            \n",
    "    labels_data - list\n",
    "            a list of tensors, where each tensor is an indexed sentence used as a true label\n",
    "    \n",
    "    word2idx - dictionary\n",
    "                a dictionary, where the keys are the words and the values are the indices\n",
    "                \n",
    "    idx2word - dictionary\n",
    "                a dictionary, where the keys are the indices and the values are the words\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "    #1. Add sentence boundaries    \n",
    "    res = add_sentence_boundaries(data)\n",
    "    \n",
    "    #2. Create index dictionaries (word2idx and idx2word)\n",
    "    word2idx, idx2word = create_indices(res)    \n",
    "    \n",
    "    #3. Index the data in a way that each word is replaced by its index\n",
    "    indexed_data = index_data(res, word2idx)\n",
    "    \n",
    "    #4. Convert the indexed data to a list of tensors, where each tensor is a sentence\n",
    "    tensor_array = convert_to_tensor(indexed_data)    \n",
    "    \n",
    "    #5. Split each sentence to input and labels\n",
    "    input_data, labels_data = prepare_for_training(tensor_array)\n",
    "    \n",
    "    return input_data, labels_data, word2idx, idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We know you love Chewy.', \"We know you're here.\", \"We know you know the Chewy-RyanCohen-GameStop connection, but it wasn't real enough for you yet.\", \"Well, I don't have to tell you, because you're not stupid, but I will anyway:  it's gotten really real enough for you now.\", 'The Chewy executive triumvirate joining the GameStop board of directors is your signal, friend.', 'You may start pumping GME to your boomer audience.', 'Now.', \"I don't believe reddit has been too kind to you in the past, but worry not, follow through with this and you'll have lots of friends here and we'll have your back forever.\", 'Well, definitely not forever, but at least for a while.', 'What better time to start than today?', 'With love, brother.', 'P.S.', \"- don't be afraid to use the rocket ðŸš€, it feels good.\"]\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "# Load result:\n",
    "with open(\"data_sample.txt\", \"rb\") as fp:   # Unpickling\n",
    "    sentences = pickle.load(fp)\n",
    "\n",
    "print(sentences[22:35])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_preprocessing_LSTM:\n",
    "    train_input, train_labels, word2idx, idx2word = preprocess_data(sentences) # run the preprocessing pipeline\n",
    "    train_data = combine_data(train_input, train_labels)\n",
    "    train_data = remove_extra(train_data, batch_size)\n",
    "    torch.save(train_input, \"LSTM_data/train_input.pt\")\n",
    "    torch.save(train_labels, \"LSTM_data/train_labels.pt\")\n",
    "    torch.save(word2idx, \"LSTM_data/word2idx.pt\")\n",
    "    torch.save(idx2word, \"LSTM_data/idx2word.pt\")\n",
    "    torch.save(train_data, \"LSTM_data/train_data.pt\")\n",
    "else:\n",
    "    train_input = torch.load('LSTM_data/train_input.pt')\n",
    "    train_labels = torch.load('LSTM_data/train_labels.pt')\n",
    "    word2idx = torch.load('LSTM_data/word2idx.pt')\n",
    "    idx2word = torch.load('LSTM_data/idx2word.pt')\n",
    "    train_data = combine_data(train_input, train_labels)\n",
    "    train_data = remove_extra(train_data, batch_size)\n",
    "    #train_data = torch.load('LSTM_data/train_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_batch_train = DataLoader(dataset=train_data,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    collate_fn=collate,\n",
    "                    pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, word2idx, embed_dim, context_dim, num_layers):\n",
    "        \"\"\"\n",
    "        This function initializes the layers of the model\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        word2idx - dictionary\n",
    "                    a dictionary where the keys are the unique words in the data\n",
    "                    and the values are the unique indices corresponding to the words\n",
    "        \n",
    "        embed_dim - integer\n",
    "                        the size of the word embeddings\n",
    "\n",
    "        context_dim - integer\n",
    "                        the dimension of the hidden size\n",
    "                        \n",
    "        num_layers - integer\n",
    "                        the number of layers in the GRU cell\n",
    "        \"\"\"\n",
    "        super(RNN, self).__init__()\n",
    "        self.word2idx = word2idx\n",
    "        self.embed_dim = embed_dim\n",
    "        self.context_dim = context_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # here we initialise weighs of a model\n",
    "        self.word_embed = nn.Embedding(len(self.word2idx)+1, self.embed_dim) # embedding layer\n",
    "        self.gru = nn.GRU(self.embed_dim, self.context_dim, num_layers=self.num_layers) # GRU cell        \n",
    "        self.dropout = nn.Dropout(0.1) # Dropout        \n",
    "        self.out = nn.Linear(self.context_dim, len(self.word2idx)+1) # output layer\n",
    "\n",
    "    \n",
    "    def forward(self, word, hidden):\n",
    "        \"\"\"\n",
    "        This function implements the forward pass of the model\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        word - tensor\n",
    "                a tensor containing indices of the words in a batch\n",
    "                \n",
    "        hidden - tensor\n",
    "                    the previous hidden state of the GRU model\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        output - tensor\n",
    "                    a tensor of logits from the linear transformation\n",
    "        \n",
    "        hidden - tensor\n",
    "                    the current hidden state of the GRU model\n",
    "        \"\"\" \n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        #raise NotImplementedError()\n",
    "        #1. Replace the indexed word with its embedding vector. \n",
    "        #In other words, pass it through the embedding layer\n",
    "        embeds = self.word_embed(word)\n",
    "        \n",
    "        batch_size = word.shape[0]\n",
    "        #print(batch_size)\n",
    "        #2. Reshape the embedding vector to a shape of (1, batch_size)\n",
    "        embeds = embeds.reshape(1, batch_size, self.embed_dim)\n",
    "        \n",
    "        #3. Pass the embedding through the GRU cell to get the output \n",
    "        #and the hidden tensors. The GRU function takes as input the \n",
    "        #work embedding and the previous hidden state.\n",
    "        output, hidden = self.gru(embeds, hidden)\n",
    "        \n",
    "        #4. Addpy a dropout to the output of the GRU\n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        #5. Apply the linear transformation to the output of the dropout layer.\n",
    "        output = self.out(output)\n",
    "        \n",
    "        #6. Reshape the output to have a shape (batch_size, vocab_length+1)\n",
    "        output = output.reshape(batch_size, len(self.word2idx) + 1)\n",
    "        \n",
    "        #7. Return the output of the linear transformation and the hidden tensor\n",
    "        hidden = hidden.to(device)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 150 # the number of epochs to train\n",
    "embed_dim = 300 # the size of the embedding\n",
    "hidden_size = 450 # the size of the hidden state\n",
    "num_layers = 1 # the number of layers in the GRU cell\n",
    "rnn_model = RNN(word2idx, embed_dim, hidden_size, num_layers).to(device) # initialize the RNN model\n",
    "loss_function = nn.CrossEntropyLoss(ignore_index=0) # define the loss function\n",
    "rnn_optimizer = optim.Adam(rnn_model.parameters(), lr=0.001) # define the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(pairs_batch_train, rnn_model, hidden_size, num_layers, loss_function, rnn_optimizer, n_epochs):\n",
    "    \"\"\"\n",
    "    This function implements the training of the model\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    pairs_batch_train - object\n",
    "                            a DataLoader object that contains the batched data\n",
    "\n",
    "    rnn_model - object\n",
    "                an RNN object that contains the initialized model\n",
    "                \n",
    "    hidden_size - integer\n",
    "                    the size of the hidden layer (the context size)\n",
    "    \n",
    "    num_layers - integer\n",
    "                        the number of layers in the GRU cell\n",
    "\n",
    "    loss_function - object\n",
    "                        the CrossEntropy loss function\n",
    "\n",
    "    rnn_optimizer - object\n",
    "                        an Adam object of the optimizer class\n",
    "\n",
    "    n_epochs - integer\n",
    "                the number of epochs to train\n",
    "    \"\"\" \n",
    "\n",
    "    for epoch in tqdm(range(n_epochs)): # iterate over the epochs\n",
    "        epoch_loss = 0\n",
    "        rnn_model.train() # put the model in training mode\n",
    "        \n",
    "        for iteration, batch in enumerate(pairs_batch_train): # at each step take a batch of sentences\n",
    "            sent_loss = 0\n",
    "            rnn_optimizer.zero_grad() # clear gradients\n",
    "            \n",
    "            train_input, train_input_lengths, train_labels = batch # extract the data from the batch\n",
    "            train_input = train_input.to(device)\n",
    "            #train_input_lengths = train_input_lengths.to(device) #this is a list\n",
    "            train_labels = train_labels.to(device)\n",
    "            \n",
    "            hidden = torch.zeros((num_layers, train_input.size(1), hidden_size)) # initialize the hidden state\n",
    "            \n",
    "            hidden = hidden.to(device)\n",
    "            \n",
    "            for i in range(train_input.size(0)): # iterate over the word in the sentence\n",
    "                output, hidden = rnn_model(train_input[i], hidden) # forward pass               \n",
    "                    \n",
    "                labels = torch.LongTensor(train_labels.size(1)) # define a random tensor with batch_size as number of elements\n",
    "                labels = labels.to(device)\n",
    "                labels[:] = train_labels[i][:] # put the correct label values in the tensor\n",
    "                \n",
    "                sent_loss += loss_function(output, labels) # compute the loss, compare the predictions and the labels\n",
    "\n",
    "            sent_loss.backward() # compute the backward pass\n",
    "            rnn_optimizer.step() # update the parameters\n",
    "\n",
    "            epoch_loss += sent_loss\n",
    "            \n",
    "        print('Epoch: {}   Loss: {}'.format(epoch+1, epoch_loss / len(pairs_batch_train))) # print the loss at each epoch\n",
    "        filename = \"RNN_00005\" + str(epoch+1) + '.pt'\n",
    "        torch.save(rnn_model.state_dict(), filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|â–Œ                                                                             | 1/150 [10:17<25:33:40, 617.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1   Loss: 280.58489990234375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|â–ˆ                                                                             | 2/150 [20:46<25:39:11, 624.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2   Loss: 225.30166625976562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|â–ˆâ–Œ                                                                            | 3/150 [31:05<25:23:38, 621.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3   Loss: 168.77413940429688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|â–ˆâ–ˆ                                                                            | 4/150 [41:54<25:39:35, 632.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4   Loss: 124.60802459716797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|â–ˆâ–ˆâ–Œ                                                                           | 5/150 [53:13<26:09:07, 649.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5   Loss: 99.58843231201172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|â–ˆâ–ˆâ–ˆ                                                                         | 6/150 [1:04:14<26:07:58, 653.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6   Loss: 85.2634048461914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|â–ˆâ–ˆâ–ˆâ–Œ                                                                        | 7/150 [1:14:24<25:23:12, 639.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7   Loss: 75.74676513671875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|â–ˆâ–ˆâ–ˆâ–ˆ                                                                        | 8/150 [1:24:44<24:58:24, 633.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8   Loss: 68.05842590332031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                       | 9/150 [1:36:00<25:19:10, 646.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9   Loss: 61.71651077270508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                      | 10/150 [1:47:05<25:21:57, 652.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10   Loss: 58.01473617553711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                     | 11/150 [1:57:19<24:43:31, 640.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11   Loss: 54.75751876831055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                     | 12/150 [2:07:22<24:06:45, 629.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12   Loss: 52.01997375488281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                    | 13/150 [2:17:44<23:51:44, 627.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13   Loss: 49.96446228027344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                    | 14/150 [2:28:20<23:46:55, 629.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14   Loss: 48.16546630859375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                   | 15/150 [2:38:15<23:13:13, 619.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15   Loss: 47.073646545410156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                   | 16/150 [2:48:20<22:53:12, 614.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16   Loss: 46.673465728759766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                  | 17/150 [2:58:16<22:30:49, 609.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17   Loss: 45.77009201049805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                  | 18/150 [3:08:12<22:11:24, 605.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18   Loss: 45.391292572021484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                 | 19/150 [3:18:01<21:51:08, 600.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19   Loss: 45.047569274902344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                 | 20/150 [3:27:49<21:33:00, 596.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20   Loss: 44.887874603271484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                | 21/150 [3:37:38<21:18:07, 594.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21   Loss: 45.310333251953125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                | 22/150 [3:47:26<21:03:39, 592.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22   Loss: 45.464622497558594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                               | 23/150 [3:57:15<20:51:35, 591.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23   Loss: 46.571075439453125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                               | 24/150 [4:07:06<20:41:48, 591.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24   Loss: 47.21337127685547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                              | 25/150 [4:16:56<20:31:15, 591.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25   Loss: 47.92580032348633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                              | 26/150 [4:26:45<20:19:47, 590.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26   Loss: 48.28879165649414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                             | 27/150 [4:36:33<20:08:48, 589.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27   Loss: 49.132362365722656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                             | 28/150 [4:46:23<19:58:51, 589.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28   Loss: 50.625755310058594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                            | 29/150 [4:56:11<19:48:34, 589.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29   Loss: 55.365055084228516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                            | 30/150 [5:06:01<19:38:33, 589.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30   Loss: 51.389801025390625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                           | 31/150 [5:15:48<19:27:33, 588.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31   Loss: 52.08329772949219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                           | 32/150 [5:25:38<19:18:24, 589.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32   Loss: 57.5995979309082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                          | 33/150 [5:35:27<19:08:55, 589.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33   Loss: 52.019981384277344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                          | 34/150 [5:45:13<18:57:25, 588.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34   Loss: 51.36544418334961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                         | 35/150 [5:55:01<18:46:55, 587.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35   Loss: 53.97654724121094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                         | 36/150 [6:04:49<18:37:06, 587.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36   Loss: 60.09857177734375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                        | 37/150 [6:14:38<18:28:18, 588.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37   Loss: 73.1925277709961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                        | 38/150 [6:24:27<18:18:33, 588.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38   Loss: 71.34232330322266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                       | 39/150 [6:34:16<18:08:53, 588.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39   Loss: 70.45771026611328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                       | 40/150 [6:44:03<17:58:09, 588.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40   Loss: 71.49202728271484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                      | 41/150 [6:53:50<17:48:02, 587.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41   Loss: 70.97381591796875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                      | 42/150 [7:03:38<17:38:14, 587.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 42   Loss: 74.9870376586914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                     | 43/150 [7:13:27<17:29:03, 588.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 43   Loss: 71.26129913330078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                     | 44/150 [7:23:17<17:20:07, 588.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 44   Loss: 66.70741271972656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                    | 45/150 [7:33:03<17:09:02, 588.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 45   Loss: 65.69152069091797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                    | 46/150 [7:42:54<17:00:24, 588.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 46   Loss: 68.58171844482422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                   | 47/150 [7:52:41<16:50:13, 588.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 47   Loss: 77.31651306152344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                   | 48/150 [8:02:30<16:40:27, 588.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 48   Loss: 80.95665740966797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                  | 49/150 [8:12:18<16:30:37, 588.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 49   Loss: 95.46806335449219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                  | 50/150 [8:22:08<16:21:13, 588.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50   Loss: 86.12346649169922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                 | 51/150 [8:31:57<16:11:46, 588.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 51   Loss: 87.65180206298828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                 | 52/150 [8:41:46<16:02:04, 589.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 52   Loss: 87.52787017822266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                | 53/150 [8:51:35<15:52:02, 588.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 53   Loss: 86.48388671875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                | 54/150 [9:01:25<15:42:53, 589.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 54   Loss: 86.64820861816406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                               | 55/150 [9:11:16<15:33:29, 589.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 55   Loss: 95.76700592041016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                               | 56/150 [9:21:04<15:23:20, 589.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 56   Loss: 97.14685821533203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                              | 57/150 [9:30:54<15:13:41, 589.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 57   Loss: 99.8570785522461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                              | 58/150 [9:40:39<15:01:58, 588.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 58   Loss: 99.42066192626953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                             | 59/150 [9:50:28<14:52:23, 588.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 59   Loss: 100.92277526855469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                            | 60/150 [10:00:14<14:41:34, 587.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 60   Loss: 97.43387603759766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                            | 61/150 [10:10:08<14:34:12, 589.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 61   Loss: 96.98050689697266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                           | 62/150 [10:19:58<14:24:49, 589.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 62   Loss: 95.04287719726562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                           | 63/150 [10:29:46<14:14:27, 589.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 63   Loss: 92.36700439453125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                          | 64/150 [10:39:37<14:05:22, 589.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 64   Loss: 89.00550842285156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                          | 65/150 [10:49:26<13:55:07, 589.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 65   Loss: 87.59112548828125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                         | 66/150 [10:59:14<13:44:30, 588.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 66   Loss: 96.00997924804688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                         | 67/150 [11:09:05<13:35:35, 589.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 67   Loss: 99.48390197753906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                        | 68/150 [11:18:55<13:25:53, 589.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 68   Loss: 92.53302764892578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                        | 69/150 [11:28:42<13:15:12, 589.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 69   Loss: 90.38249969482422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                       | 70/150 [11:38:32<13:05:39, 589.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 70   Loss: 93.33223724365234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                       | 71/150 [11:48:19<12:54:47, 588.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 71   Loss: 91.8568115234375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                      | 72/150 [11:58:06<12:44:38, 588.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 72   Loss: 88.2503890991211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                      | 73/150 [12:07:57<12:35:53, 589.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 73   Loss: 88.21111297607422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                     | 74/150 [12:17:46<12:26:12, 589.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 74   Loss: 89.57752990722656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                     | 75/150 [12:27:37<12:17:02, 589.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 75   Loss: 87.11522674560547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                    | 76/150 [12:37:28<12:07:33, 589.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 76   Loss: 84.998291015625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                    | 77/150 [12:47:17<11:57:25, 589.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 77   Loss: 90.22279357910156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                   | 78/150 [12:57:07<11:47:53, 589.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 78   Loss: 89.26412963867188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                   | 79/150 [13:06:58<11:38:22, 590.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 79   Loss: 94.3436508178711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                  | 80/150 [13:16:51<11:29:22, 590.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 80   Loss: 90.99979400634766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                  | 81/150 [13:26:48<11:21:46, 592.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 81   Loss: 94.06163787841797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                 | 82/150 [13:36:59<11:18:06, 598.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 82   Loss: 98.6934814453125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                 | 83/150 [13:47:09<11:11:47, 601.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 83   Loss: 96.24359893798828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                 | 83/150 [13:54:12<11:13:23, 603.04s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-643aec1ac6fe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_rnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpairs_batch_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrnn_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrnn_optimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-17-7c8d62b1f4ab>\u001b[0m in \u001b[0;36mtrain_rnn\u001b[1;34m(pairs_batch_train, rnn_model, hidden_size, num_layers, loss_function, rnn_optimizer, n_epochs)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_input\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# iterate over the word in the sentence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m                 \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnn_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_input\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# forward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m                 \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# define a random tensor with batch_size as number of elements\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\aalto\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-814fc9b9acdc>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, word, hidden)\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[1;31m#and the hidden tensors. The GRU function takes as input the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[1;31m#work embedding and the previous hidden state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;31m#4. Addpy a dropout to the output of the GRU\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\aalto\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\aalto\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    820\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m             result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[1;32m--> 822\u001b[1;33m                              self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[0;32m    823\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    824\u001b[0m             result = _VF.gru(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_rnn(pairs_batch_train, rnn_model,hidden_size, num_layers, loss_function, rnn_optimizer, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rnn(rnn_model, hidden_size, num_layers, word2idx, idx2word, context, max_len):\n",
    "    \"\"\"\n",
    "    This function predicts the next word, based on the history of the previous words.\n",
    "    We start with the 'context' and then feed the prediction as the next input.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    rnn_model - object\n",
    "                an RNN object that contains the trained model\n",
    "                \n",
    "    hidden_size - integer\n",
    "                    the size of the hidden layer (the context size)\n",
    "                    \n",
    "    num_layers - integer\n",
    "                    the number of layers in the GRU cell\n",
    "                \n",
    "    word2idx - dictionary\n",
    "                    a dictionary where the keys are the unique words in the data\n",
    "                    and the values are the unique indices corresponding to the words\n",
    "                    \n",
    "    idx2word - dictionary\n",
    "                a dictionary, where the keys are the indices and the values are the words\n",
    "                    \n",
    "    context - string\n",
    "                the context sentence\n",
    "    \n",
    "    max_len - integer\n",
    "                integer value representing up to how many words to generate\n",
    "                            \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    predictions - string\n",
    "                    a string containing the generated sentence\n",
    "    \"\"\"\n",
    "    \n",
    "    # index the context\n",
    "    context_indexed = []\n",
    "    for word in context.split():\n",
    "        word_indexed = torch.LongTensor(1)\n",
    "        word_indexed[:] = word2idx[word]\n",
    "        context_indexed.append(word_indexed)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        predictions = []\n",
    "        # first build the hidden state from the context\n",
    "        hidden = torch.zeros((num_layers, 1, hidden_size), device=device)\n",
    "        for word in context_indexed:\n",
    "            predictions.append(idx2word[word.item()])\n",
    "            word = word.to(device)\n",
    "            output, hidden = rnn_model(word, hidden)\n",
    "            \n",
    "        next_input = context_indexed[-1]\n",
    "        while((len(predictions) < max_len) and (predictions[-1] != '</s>')):\n",
    "            \n",
    "            # YOUR CODE HERE\n",
    "            #raise NotImplementedError()\n",
    "            #1. Run the forward pass to get the output. Don't forget to include the `hidden` state\n",
    "            #print(\"INPUT\", next_input)\n",
    "            next_input = next_input.to(device)\n",
    "            out, hidden = rnn_model.forward(next_input, hidden)\n",
    "            \n",
    "            #2. Run the output through a softmax to convert it to a probability distribution (`F.softmax`)\n",
    "            out = F.softmax(out)\n",
    "            \n",
    "            #3. Get the word with the highest probability using the `topk(1)` function\n",
    "            value, index = out.topk(1)\n",
    "            \n",
    "            #4. Convert the index of the predicted word to the actual word using the idx2word dictionary\n",
    "            word = idx2word[index.item()]\n",
    "            \n",
    "            #5. Append the predicted word to the `predictions` array\n",
    "            predictions.append(word)\n",
    "            next_input = index\n",
    "            \n",
    "    predictions = ' '.join(predictions)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> to the moon at the game downside group might be profitable for the retail else sales in limiting the market is still from the future soon. </s>\n",
      "\n",
      "\n",
      "<s> GME Fuzzy numbers calls and says try to buy corporate bond that bond sectors is enough to decision that he needs help. </s>\n",
      "\n",
      "\n",
      "<s> my wife does not prevent that well, not. </s>\n",
      "\n",
      "\n",
      "<s> Elon Musk Has Made Millionaires train Chinese train and says it says to bear market open. </s>\n",
      "\n",
      "\n",
      "<s> the best stock is extremely unlikely, is not huge and closed a huge blow for the market. </s>\n",
      "\n",
      "\n",
      "<s> I think the market will pump the fed from the future wave as well as well as invested as the as the of the public gain as indicators of control on the market. </s>\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usin\\Anaconda3\\envs\\aalto\\lib\\site-packages\\ipykernel_launcher.py:64: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "contexts = ['<s> to the moon', '<s> GME', '<s> my wife', '<s> Elon Musk', '<s> the best stock is', '<s> I think the market will']\n",
    "max_len = 50\n",
    "\n",
    "for context in contexts:\n",
    "    predictions = predict_rnn(rnn_model, hidden_size, num_layers, word2idx, idx2word, context, max_len)\n",
    "    print(predictions)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(rnn_model.state_dict(), \"rnn_epoch_83_lr0001.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
