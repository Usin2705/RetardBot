{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "step_03_RNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBFYmMybhw2P"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn # the neural network package that contains functions for creating the neural network layers\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim # a package that allows use to use an optimizer in order to update the parameters during training\n",
        "from torch.utils.data import DataLoader # allows use to process the data in batches\n",
        "from torch.nn.utils.rnn import pad_sequence # a function that zero-pads the sentences so they can have equal size in a batch\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import pickle\n",
        "torch.manual_seed(0) # set a random seed for reproducibility\n",
        "from tqdm import tqdm\n",
        "\n",
        "MODEL_CONSTANT = ['GRU', 'LSTM', 'BI-LSTM']"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZRD5dayUjEt"
      },
      "source": [
        "###Set important constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMwGdw4vh86_"
      },
      "source": [
        "is_google_colab = True\n",
        "batch_size = 8\n",
        "is_preprocessing = False #This toggle preprocessing for RNN model (FALSE = not preprocessing for text, but instead load from processed data saved from previous run)\n",
        "model_type = MODEL_CONSTANT[2] # change this variable to correct model type\n",
        "load_model = False # this is for saving/loading the model from the checkpoint when colab crashes\n",
        "\n",
        "device = torch.device('cuda:0')\n",
        "#device = torch.device('cpu')\n",
        "\n",
        "# Below are the dataset file we will perform our training on:\n",
        "data_file = \"data_sample_8x.txt\"\n",
        "\n",
        "# Below are the folder contain the preprocessed data for training\n",
        "#prep_file_name = 'RNN_data'\n",
        "#prep_file_name = 'RNN_data_full'\n",
        "prep_file_name = 'RNN_data_8x'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDJa2Ishhw2U",
        "outputId": "933834d3-5d2c-4149-caec-6585194f9548"
      },
      "source": [
        "# this is to connect this notebook to the contents of your Google Drive\n",
        "# this is Ryoko's Google Drive filepath\n",
        "# please specify your own, or we can (probably) share a folder for it\n",
        "if is_google_colab:\n",
        "    # files uploaded to Google Drive will not be deleted by inactivity,\n",
        "    # but it does require an authorization code every time you use it\n",
        "    from google.colab import drive\n",
        "    filepath = r'/content/drive/My Drive/RetardBot/'\n",
        "    drive.mount(r'/content/drive')\n",
        "else:\n",
        "    filepath = ''\n",
        "print(\"Running model :\", model_type)\n",
        "\n",
        "load_file = model_type + '_temp_quarter' #file name if I'm going to load the model\n",
        "tmp_name = model_type + '_temp_quarter' #temporary file I want to create checkpoint in"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Running model : BI-LSTM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlB_UBk1hw2W"
      },
      "source": [
        "###  Sentence boundaries\n",
        "\n",
        "When dealing with language, it is good to know when a sentence starts and when it ends. That will help the model at the beginning of the prediction, when we don't have any previous words as context. For that purpose, we are going to pad each sentence with a start-of-sentence symbol _\"&lt;s>\"_ and an end-of-sentence symbol _\"&lt;/s>\"_. \n",
        "\n",
        "Since you already did a similar thing in the n-grams exercise, this function is already implemented for you."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvKkH0Plhw2X"
      },
      "source": [
        "def add_sentence_boundaries(data):\n",
        "    \"\"\"\n",
        "    Takes the data, where each line is a sentence, appends <s> token at the beginning and </s> at the end of each sentence\n",
        "    Example input: I live in Helsinki\n",
        "    Example output: <s> I live in Helsinki </s>\n",
        "    \n",
        "    Arguments\n",
        "    ---------\n",
        "    data : list\n",
        "            a list of sentences\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    res : list\n",
        "            a list of sentences, where each sentence has <s> at the beginning and </s> at the end\n",
        "    \"\"\"\n",
        "    #res = []\n",
        "    #for sent in data:\n",
        "    #    sent = '<s> ' + sent.rstrip() + ' </s>'\n",
        "    #    res.append(sent)\n",
        "\n",
        "    res = ['<s> ' + sent.rstrip() + ' </s>' for sent in data]\n",
        "    \n",
        "    return res"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9PSn77Jhw2Z"
      },
      "source": [
        "### Index dictionaries <a class=\"anchor\" id=\"task_1_1\"></a> \n",
        "Neural networks can't process words as raw strings. Due to that, we need to represent the words with numbers. The first step in doing that is creating two dictionaries: word2idx and idx2word.\n",
        "\n",
        "The word2idx dictionary contains unique words as keys and unique indices for each of the words as values. <br>\n",
        "The idx2word dictionary contains unique indices as keys and unique words for each of those indices as values. It is essentially a reversed word2dx, where the keys are the values and the values are the keys.\n",
        "\n",
        "Example sentences: [\"I look forward\", \"You look forward\"] <br>\n",
        "word2idx = {\"I\": 1, \"look\": 2, \"forward\": 3, \"You\": 4} <br>\n",
        "idx2word = {1: \"I\", 2: \"look\", 3: \"forward\", 4: \"You\"} <br>\n",
        "\n",
        "Write a function that creates two dictionaries: word2idx and idx2work. The dictionaries should contain all the unique words in the data. <b>The indices should start from 1 and not from 0<b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEut8UGmhw2Z"
      },
      "source": [
        "def create_indices(data):\n",
        "    \"\"\"\n",
        "    This function creates two dictionaries: word2idx and idx2word, containing each unique word in the dataset\n",
        "    and its corresponding index.\n",
        "    Remember that the starting index should be 1 and not 0\n",
        "    \n",
        "    Arguments\n",
        "    ---------\n",
        "    data - list\n",
        "            a list of sentences, where each sentence starts with <s>\n",
        "            and ends with </s> token\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    word2idx - dictionary\n",
        "                a dictionary, where the keys are the words and the values are the indices\n",
        "                \n",
        "    idx2word - dictionary\n",
        "                a dictionary, where the keys are the indices and the values are the words\n",
        "    \"\"\"\n",
        "    \n",
        "    # YOUR CODE HERE\n",
        "    #raise NotImplementedError()\n",
        "    # There's several method to to this, using Counter would make our model run faster\n",
        "    # in preprocessing, otherwise it would take forever to run with full data\n",
        "    # sklearn also offer similar function, just watch out for stopwords\n",
        "    # sklearn.feature_extraction.text.CountVectorizer(min_df=1, stop_words=None)\n",
        "    # word_tokenizer = vectorizer.build_tokenizer()\n",
        "    # word2idx = vectorizer.vocabulary_\n",
        "    as_one = ' '.join(data)\n",
        "    words = as_one.split()\n",
        "    counts = Counter(words)    \n",
        "    word2idx = {word: ii for ii, word in enumerate(counts, 1)}\n",
        "    word2idx[''] = len(word2idx)+1 # We need this one in case our sentences have double space\n",
        "    idx2word = {value:key for key,value in word2idx.items()}\n",
        "    \n",
        "    return word2idx, idx2word"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yk9LJKLhw2b"
      },
      "source": [
        "### Index data <a class=\"anchor\" id=\"task_1_2\"></a>\n",
        "After we have created the word2idx and idx2word dictionaries, it is time to index the data. In other words, we need to replace each word in the data with its corresponding index.\n",
        "\n",
        "Write a function that reads each sentence from the data and replaces each word in the sentence with its index from the word2idx dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5pVYNw0hw2b"
      },
      "source": [
        "def index_data(data, word2idx):\n",
        "    \"\"\"\n",
        "    This function replaces each word in the data with its corresponding index\n",
        "    \n",
        "    Arguments\n",
        "    ---------\n",
        "    data - list\n",
        "            a list of sentences, where each sentence starts with <s>\n",
        "            and ends with </s> token\n",
        "    \n",
        "    word2idx - dict\n",
        "            a dictionary where the keys are the unique words in the data\n",
        "            and the values are the unique indices corresponding to the words%\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    data_indexed - list\n",
        "                a list of sentences, where each word in the sentence is replaced with its index\n",
        "    \"\"\"\n",
        "    \n",
        "    data_indexed = []\n",
        "    # YOUR CODE HERE\n",
        "    #raise NotImplementedError()\n",
        "    \n",
        "    for sentence in data:\n",
        "        sentence_index = [word2idx[word] for word in sentence.split(' ')]\n",
        "        # Further improvement can be done here to remove the top loop\n",
        "        # For now it's fast enough to ignore\n",
        "        data_indexed.append(sentence_index)\n",
        "    \n",
        "\n",
        "    return data_indexed"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTt-hGFrhw2d"
      },
      "source": [
        "### Convert sentences to tensors\n",
        "\n",
        "This function converts each indexed sentence to a LongTensor data type. This is required in order to process it later using Pytorch.\n",
        "\n",
        "You don't have to modify this function. It is already implemented for you."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iw5AifOHhw2g"
      },
      "source": [
        "def convert_to_tensor(data_indexed):\n",
        "    \"\"\"\n",
        "    This function converts the indexed sentences to LongTensors\n",
        "    \n",
        "    Arguments\n",
        "    ---------\n",
        "    data_indexed - list\n",
        "            a list of sentences, where each word in the sentence\n",
        "            is replaced by its index\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    tensor_array - list\n",
        "                a list of sentences, where each sentence\n",
        "                is a LongTensor\n",
        "    \"\"\"\n",
        "    tensor_array = [torch.LongTensor(sent) for sent in data_indexed]\n",
        "        \n",
        "    return tensor_array"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbhRIOvfhw2h"
      },
      "source": [
        "### Combine features and labels in a tuple\n",
        "\n",
        "This function combines each indexed sentence and its corresponding labels to a tuple. This will be beneficial for us when we zero-pad the data later, in order to make the batches have equal-length samples.\n",
        "\n",
        "You don't have to modify this function. It is already implemented for you."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_AbIgnkhw2j"
      },
      "source": [
        "def combine_data(input_data, labels_data):\n",
        "    \"\"\"\n",
        "    This function converts the input features and the labels into tuples\n",
        "    where each tuple corresponds to one sentence in the format (features, labels)\n",
        "    \n",
        "    Arguments\n",
        "    ---------\n",
        "    input_data - list\n",
        "            a list of tensors containing the training features\n",
        "    \n",
        "    labels_data - list\n",
        "            a list of tensors containing the training labels\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    res - list\n",
        "            a list of tuples, where each tuple corresponds to one sentece pair\n",
        "            in the format (features, labels)\n",
        "    \"\"\"\n",
        "    \n",
        "    res = [(input_data[i], labels_data[i]) for i in range(len(input_data))]\n",
        "\n",
        "    return res"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybApSolDhw2k"
      },
      "source": [
        "### Remove extra data\n",
        "\n",
        "Since we will be processing the data in equal batches during training, we need to make sure that each batch has equal number of sentences. In case the last batch contains less sentences than the batch size, that batch will be discarded.\n",
        "\n",
        "This function discards the extra data that doesn't fit in a batch.\n",
        "\n",
        "You don't have to modify this function. It is already implemented for you."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTaECt-rhw2k"
      },
      "source": [
        "def remove_extra(data, batch_size):\n",
        "    \"\"\"\n",
        "    This function removes the extra data that does not fit in a batch   \n",
        "    \n",
        "    Arguments\n",
        "    ---------\n",
        "    data - list\n",
        "            a list of tuples, where each tuple corresponds to a\n",
        "            sentence in a format (features, labels)\n",
        "            \n",
        "    batch_size - integer\n",
        "                    the size of the batch\n",
        "    \n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    data - list\n",
        "            a list of tuples, where each tuple corresponds to a\n",
        "            sentence in a format (features, labels)\n",
        "    \"\"\"\n",
        "    \n",
        "    extra = len(data) % batch_size\n",
        "    if extra != 0:\n",
        "        data = data[:-extra][:]\n",
        "\n",
        "    return data"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7O9Zphvqhw2l"
      },
      "source": [
        "### Zero-pad the data\n",
        "\n",
        "In order to process the data in batches, we need to make sure that the sentences in each batch have equal lengths. Since we are working with sentences, each sentence in a batch can have different number of words. In this case, we need to  make the length of each sentence the same as the length of the longest sentence in that batch. We do that by adding zeros at the end of each sentence, until the sentence has equal length as the longest one in the batch.\n",
        "\n",
        "This function implements the zero-padding.\n",
        "\n",
        "You don't have to modify this function. It is already implemented for you."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zWi4aAZhw2l"
      },
      "source": [
        "def collate(list_of_samples):\n",
        "    \"\"\"\n",
        "    This function zero-pads the training data in order to process the sentences\n",
        "    in a batch during training\n",
        "    \n",
        "    Arguments\n",
        "    ---------\n",
        "    list_of_samples - list\n",
        "                        a list of tuples, where each tuple corresponds to a\n",
        "                        sentence in a format (features, labels)\n",
        "    \n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    pad_input_data - tensor\n",
        "                        a tensor of input features equal to the batch size,\n",
        "                        where features are zero-padded to have equal lengths\n",
        "                        \n",
        "    input_data_lengths - list\n",
        "                        a list where each element is the length of the \n",
        "                        corresponding sentence\n",
        "    \n",
        "    pad_labels_data - tensor\n",
        "                        a tensor of labels equal to the batch size,\n",
        "                        where labels are zero-padded to have equal lengths\n",
        "            \n",
        "    \"\"\"\n",
        "    \n",
        "    \n",
        "    list_of_samples.sort(key=lambda x: len(x[0]), reverse=True)\n",
        "    input_data, labels_data = zip(*list_of_samples)\n",
        "\n",
        "    input_data_lengths = [len(seq) for seq in input_data]\n",
        "    \n",
        "    padding_value = 0\n",
        "\n",
        "    # pad input\n",
        "    pad_input_data = pad_sequence(input_data, padding_value=padding_value)\n",
        "    \n",
        "    # pad labels\n",
        "    pad_labels_data = pad_sequence(labels_data, padding_value=padding_value)\n",
        "\n",
        "    return pad_input_data, input_data_lengths, pad_labels_data"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNYX0EJQhw2l"
      },
      "source": [
        "### Prepare features and labels <a class=\"anchor\" id=\"task_1_3\"></a> \n",
        "During training, the model takes an input word and outputs a prediction. We will need to compare this prediction to 'true label'. True label is just the next word in the text, but we will need to organize the data, so that every word in the text is considered as this 'true label'.\n",
        "\n",
        "In the label sentence, every word is moved a step in time, and for the input sentence the last word is missing. \n",
        "\n",
        "Example sentence: oops i did it again <br>\n",
        "INPUT: oops i did it <br>\n",
        "LABEL: i did it again\n",
        "\n",
        "Note: the first word in the sentence is start-of-sentence symbol and the last one is end-of-sentence symbol.\n",
        "\n",
        "Write a function that takes as input the indexed data and returns two arrays: the input array where the last word from each sentence is missing, and the label array, where every word is moved a step in time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zoIZXwbhw2m"
      },
      "source": [
        "def prepare_for_training(data_indexed):\n",
        "    \"\"\"\n",
        "    This function creates the input features and their corresponding labels\n",
        "    \n",
        "    Arguments\n",
        "    ---------\n",
        "    data_indexed - list\n",
        "            a list of sentences, where each word in the sentence\n",
        "            is replaced by its index\n",
        "    \n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    input_data - list\n",
        "            a list of indexed sentences, where the last element of each sentence is removed\n",
        "            \n",
        "    labels_data - list\n",
        "            a list of indexed sentences, where the first element of each sentence is removed\n",
        "    \"\"\"\n",
        "    \n",
        "    #input_data = []\n",
        "    #labels_data = []\n",
        "\n",
        "     # YOUR CODE HERE\n",
        "    #raise NotImplementedError()\n",
        "\n",
        "    input_data  = [data[:-1] for data in data_indexed]\n",
        "    labels_data = [data[1:] for data in data_indexed]\n",
        "\n",
        "    return input_data, labels_data"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygFyidgvhw2m"
      },
      "source": [
        "### Preprocess data <a class=\"anchor\" id=\"task_1_4\"></a>\n",
        "At this point, we have all the necessary functions to prepare the data for training. What is left to do is to run them one by one and get the data in the desired format.\n",
        "\n",
        "Write a function that takes the data and prepares it for training. You need to do the following steps:\n",
        "\n",
        "    1. Add sentence boundaries\n",
        "    2. Create index dictionaries (word2idx and idx2word)\n",
        "    3. Index the data in a way that each word is replaced by its index\n",
        "    4. Convert the indexed data to a list of tensors, where each tensor is a sentence\n",
        "    5. Split each sentence to input and labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmF4xoMZhw2m"
      },
      "source": [
        "def preprocess_data(data):\n",
        "    \"\"\"\n",
        "    This function runs the whole preprocessing pipeline and returns the prepared\n",
        "    input features and labels, along with the word2idx and idx2word dictionaries\n",
        "    \n",
        "    Arguments\n",
        "    ---------\n",
        "    data - list\n",
        "            a list of sentences that need to be prepared for training\n",
        "    \n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    input_data - list\n",
        "            a list of tensors, where each tensor is an indexed sentence used as input feature\n",
        "            \n",
        "    labels_data - list\n",
        "            a list of tensors, where each tensor is an indexed sentence used as a true label\n",
        "    \n",
        "    word2idx - dictionary\n",
        "                a dictionary, where the keys are the words and the values are the indices\n",
        "                \n",
        "    idx2word - dictionary\n",
        "                a dictionary, where the keys are the indices and the values are the words\n",
        "    \"\"\"\n",
        "    \n",
        "    # YOUR CODE HERE\n",
        "    #raise NotImplementedError()\n",
        "    #1. Add sentence boundaries    \n",
        "    print(\"Add sentence boundaries\")\n",
        "    res = add_sentence_boundaries(data)\n",
        "        \n",
        "    #2. Create index dictionaries (word2idx and idx2word)\n",
        "    print(\"Create index dictionaries (word2idx and idx2word)\")\n",
        "    word2idx, idx2word = create_indices(res)    \n",
        "    \n",
        "    #3. Index the data in a way that each word is replaced by its index\n",
        "    print(\"Index the data in a way that each word is replaced by its index\")\n",
        "    indexed_data = index_data(res, word2idx)\n",
        "    \n",
        "    print(\"Convert the indexed data to a list of tensors\")\n",
        "    #4. Convert the indexed data to a list of tensors, where each tensor is a sentence\n",
        "    tensor_array = convert_to_tensor(indexed_data)   \n",
        "\n",
        "    print(\"Split each sentence to input and labels\")\n",
        "    #5. Split each sentence to input and labels\n",
        "    input_data, labels_data = prepare_for_training(tensor_array)\n",
        "\n",
        "    print(\"Done preprocessing\")\n",
        "    return input_data, labels_data, word2idx, idx2word"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXZswejHhw2n",
        "outputId": "62949a9b-9b07-4a5d-9e22-b67459bb2507"
      },
      "source": [
        "# Load data\n",
        "# Load result:\n",
        "with open(filepath + data_file, \"rb\") as fp:   # Unpickling\n",
        "#with open(filepath + \"clean_data.txt\", \"rb\") as fp:   # Unpickling\n",
        "    sentences = pickle.load(fp)\n",
        "\n",
        "print(sentences[22:35])\n",
        "print(len(sentences))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['climb to the tallest building in your city and turn on the retard signal', 'i predicted every green and red day correctly this week and still managed to lose money, that takes talent', 'sales will be sky high because everyone buying a new iphone every day.', 'lol you can lose more than that.', 'yeah sure... theata gang is so fucking chad.', 'she realized the \"\"\"real economy\"\"\" is a total farce and the only safe industry is one based on selling fizzy sugar water to americans.', \"like i said before, half the world's population has a tesla in their pocket\", 'aoc, no shirt, push-up bra', 'wait...is this negative gains?', '* a growing pool of easily radicalized, aggressive and unproductive/demoralized men.', 'this ainâ€™t normal lmao', 'puts on boomers', 'your flair made me laugh more than this post did']\n",
            "160000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7XkO9y5hm3D"
      },
      "source": [
        "### Preprocessing data\n",
        "\n",
        "Preprocessing data and save those to files for future run (very important if we run big model in Google Colab, as it take DAYS and multiple run to get good result).\n",
        "\n",
        "Or load the data if we already run it before\n",
        "\n",
        "**Remember to create the folder to store the data (torch.save) won't auto create new folder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzn16N4qhw2n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05e0bf61-e4c8-4348-c32d-1fe6efbe2f3e"
      },
      "source": [
        "if is_preprocessing:\n",
        "    train_input, train_labels, word2idx, idx2word = preprocess_data(sentences) # run the preprocessing pipeline\n",
        "    print(\"Saving...\")\n",
        "    torch.save(train_input, filepath + prep_file_name + '/train_input.pt')\n",
        "    torch.save(train_labels, filepath + prep_file_name + '/train_labels.pt')\n",
        "    torch.save(word2idx, filepath + prep_file_name + '/word2idx.pt')\n",
        "    torch.save(idx2word, filepath + prep_file_name + '/idx2word.pt')\n",
        "    train_data = combine_data(train_input, train_labels)\n",
        "    train_data = remove_extra(train_data, batch_size)\n",
        "    torch.save(train_data, filepath + prep_file_name + '/train_data.pt')    \n",
        "else:\n",
        "    print(\"Loading...\")\n",
        "    #train_input = torch.load(filepath + prep_file_name + '/train_input.pt')\n",
        "    #train_labels = torch.load(filepath + prep_file_name + '/train_labels.pt')    \n",
        "    #train_data = combine_data(train_input, train_labels)\n",
        "    #train_data = remove_extra(train_data, batch_size)\n",
        "    # Fastest load is only load 3 variable below:\n",
        "    word2idx = torch.load(filepath + prep_file_name + '/word2idx.pt')\n",
        "    idx2word = torch.load(filepath + prep_file_name + '/idx2word.pt')\n",
        "    train_data = torch.load(filepath + prep_file_name + '/train_data.pt') "
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HO_iajnUhw2o"
      },
      "source": [
        "pairs_batch_train = DataLoader(dataset=train_data,\n",
        "                    batch_size=batch_size,\n",
        "                    shuffle=True,\n",
        "                    collate_fn=collate,\n",
        "                    pin_memory=True)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzFt3xV-hw2o"
      },
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, word2idx, embed_dim, context_dim, num_layers):\n",
        "        \"\"\"\n",
        "        This function initializes the layers of the model\n",
        "        \n",
        "        Arguments\n",
        "        ---------\n",
        "        word2idx - dictionary\n",
        "                    a dictionary where the keys are the unique words in the data\n",
        "                    and the values are the unique indices corresponding to the words\n",
        "        \n",
        "        embed_dim - integer\n",
        "                        the size of the word embeddings\n",
        "\n",
        "        context_dim - integer\n",
        "                        the dimension of the hidden size\n",
        "                        \n",
        "        num_layers - integer\n",
        "                        the number of layers in the GRU cell\n",
        "        \"\"\"\n",
        "        super(RNN, self).__init__()\n",
        "        self.word2idx = word2idx\n",
        "        self.embed_dim = embed_dim\n",
        "        self.context_dim = context_dim\n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        # here we initialise weighs of a model\n",
        "        self.word_embed = nn.Embedding(len(self.word2idx)+1, self.embed_dim) # embedding layer\n",
        "        \n",
        "        #GRU\n",
        "        if model_type == MODEL_CONSTANT[0]:            \n",
        "            self.rnn = nn.GRU(self.embed_dim,\n",
        "                              self.context_dim,\n",
        "                              num_layers=self.num_layers)\n",
        "        \n",
        "        #LSTM\n",
        "        elif model_type == MODEL_CONSTANT[1]:            \n",
        "            self.rnn = nn.LSTM(input_size = self.embed_dim, \n",
        "                               hidden_size = self.context_dim,\n",
        "                               num_layers = self.num_layers)\n",
        "        #BI-LSTM\n",
        "        else:\n",
        "            self.rnn = nn.LSTM(input_size = self.embed_dim, \n",
        "                               hidden_size = self.context_dim,\n",
        "                               num_layers = self.num_layers,\n",
        "                               bidirectional=True)\n",
        "        \n",
        "        self.dropout = nn.Dropout(0.1) # Dropout        \n",
        "        \n",
        "        if model_type == MODEL_CONSTANT[2]:\n",
        "            self.out = nn.Linear(self.context_dim*2, len(self.word2idx)+1) # output layer\n",
        "        else:\n",
        "            self.out = nn.Linear(self.context_dim, len(self.word2idx)+1) # output layer\n",
        "\n",
        "    \n",
        "    def forward(self, word, hidden):\n",
        "        \"\"\"\n",
        "        This function implements the forward pass of the model\n",
        "        \n",
        "        Arguments\n",
        "        ---------\n",
        "        word - tensor\n",
        "                a tensor containing indices of the words in a batch\n",
        "                \n",
        "        hidden - tensor\n",
        "                    the previous hidden state of the GRU model\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        output - tensor\n",
        "                    a tensor of logits from the linear transformation\n",
        "        \n",
        "        hidden - tensor\n",
        "                    the current hidden state of the GRU model\n",
        "        \"\"\" \n",
        "        \n",
        "        # YOUR CODE HERE\n",
        "        #raise NotImplementedError()\n",
        "        #1. Replace the indexed word with its embedding vector. \n",
        "        #In other words, pass it through the embedding layer\n",
        "        embeds = self.word_embed(word)\n",
        "        \n",
        "        batch_size = word.shape[0]\n",
        "        #print(batch_size)\n",
        "        #2. Reshape the embedding vector to a shape of (1, batch_size)\n",
        "        embeds = embeds.reshape(1, batch_size, self.embed_dim)\n",
        "        \n",
        "        #3. Pass the embedding through the GRU cell to get the output \n",
        "        #and the hidden tensors. The GRU function takes as input the \n",
        "        #work embedding and the previous hidden state.\n",
        "        output, hidden = self.rnn(embeds, hidden)\n",
        "        \n",
        "        #4. Addpy a dropout to the output of the RNN\n",
        "        output = self.dropout(output)\n",
        "        \n",
        "        #5. Apply the linear transformation to the output of the dropout layer.\n",
        "        output = self.out(output)\n",
        "        \n",
        "        #6. Reshape the output to have a shape (batch_size, vocab_length+1)\n",
        "        output = output.reshape(batch_size, len(self.word2idx) + 1)\n",
        "        \n",
        "        #7. Return the output of the linear transformation and the hidden tensor\n",
        "        #only GRU need to move to device\n",
        "        if model_type == MODEL_CONSTANT[0]:\n",
        "            hidden = hidden.to(device)\n",
        "        return output, hidden\n",
        "    \n",
        "    def init_state(self, sequence_length):\n",
        "        if model_type == MODEL_CONSTANT[1]:\n",
        "            return (torch.zeros(self.num_layers, sequence_length, self.context_dim).to(device),\n",
        "                    torch.zeros(self.num_layers, sequence_length, self.context_dim).to(device))\n",
        "        else:\n",
        "            return (torch.zeros(self.num_layers*2, sequence_length, self.context_dim).to(device),\n",
        "                    torch.zeros(self.num_layers*2, sequence_length, self.context_dim).to(device))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJyz9rE6hw2p"
      },
      "source": [
        "n_epochs = 100 # the number of epochs to train\n",
        "embed_dim = 300 # the size of the embedding\n",
        "hidden_size = 450 # the size of the hidden state\n",
        "num_layers = 1 # the number of layers in the RNN cell\n",
        "rnn_model = RNN(word2idx, embed_dim, hidden_size, num_layers).to(device) # initialize the RNN model\n",
        "loss_function = nn.CrossEntropyLoss(ignore_index=0) # define the loss function\n",
        "rnn_optimizer = optim.Adam(rnn_model.parameters(), lr=0.0005) # define the optimizer"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mMmF9CAhw2p"
      },
      "source": [
        "def train_rnn(pairs_batch_train, rnn_model, hidden_size, num_layers, loss_function, rnn_optimizer, n_epochs):\n",
        "    \"\"\"\n",
        "    This function implements the training of the model\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    pairs_batch_train - object\n",
        "                            a DataLoader object that contains the batched data\n",
        "\n",
        "    rnn_model - object\n",
        "                an RNN object that contains the initialized model\n",
        "                \n",
        "    hidden_size - integer\n",
        "                    the size of the hidden layer (the context size)\n",
        "    \n",
        "    num_layers - integer\n",
        "                        the number of layers in the GRU cell\n",
        "\n",
        "    loss_function - object\n",
        "                        the CrossEntropy loss function\n",
        "\n",
        "    rnn_optimizer - object\n",
        "                        an Adam object of the optimizer class\n",
        "\n",
        "    n_epochs - integer\n",
        "                the number of epochs to train\n",
        "    \"\"\" \n",
        "    loss_list = []\n",
        "    word_loss_list = []    \n",
        "\n",
        "    if load_model == True:\n",
        "        if not torch.cuda.is_available():\n",
        "            checkpoint = torch.load(filepath+load_file,map_location=torch.device('cpu'))\n",
        "        else:\n",
        "            checkpoint = torch.load(filepath+load_file)\n",
        "        rnn_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        rnn_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        last_epoch = checkpoint['epoch']\n",
        "    else:\n",
        "        last_epoch = 0\n",
        "\n",
        "    for epoch in tqdm(range(last_epoch, n_epochs)): # iterate over the epochs\n",
        "        epoch_loss = 0\n",
        "        epoch_word_loss = 0\n",
        "        rnn_model.train() # put the model in training mode\n",
        "        \n",
        "        for iteration, batch in enumerate(pairs_batch_train): # at each step take a batch of sentences\n",
        "            sent_loss = 0\n",
        "            rnn_optimizer.zero_grad() # clear gradients\n",
        "            \n",
        "            train_input, train_input_lengths, train_labels = batch # extract the data from the batch\n",
        "            train_input = train_input.to(device)\n",
        "            #train_input_lengths = train_input_lengths.to(device) #this is a list\n",
        "            train_labels = train_labels.to(device)\n",
        "            \n",
        "            # initialize the hidden state for GRU\n",
        "            if model_type == MODEL_CONSTANT[0]: \n",
        "                hidden = torch.zeros((num_layers, train_input.size(1), hidden_size)) \n",
        "                hidden = hidden.to(device)\n",
        "            else:\n",
        "                hidden = rnn_model.init_state(train_input.size(1))      \n",
        "            \n",
        "            \n",
        "            for i in range(train_input.size(0)): # iterate over the word in the sentence\n",
        "                output, hidden = rnn_model(train_input[i], hidden) # forward pass               \n",
        "                    \n",
        "                labels = torch.LongTensor(train_labels.size(1)) # define a random tensor with batch_size as number of elements\n",
        "                labels = labels.to(device)\n",
        "                labels[:] = train_labels[i][:] # put the correct label values in the tensor\n",
        "                \n",
        "                sent_loss += loss_function(output, labels) # compute the loss, compare the predictions and the labels\n",
        "\n",
        "            word_loss = sent_loss.item()/train_labels.size(1)\n",
        "            #if (iteration%500) == 0:\n",
        "            #  print(\"sentence_loss, pair_batch_train, train_inputsize\",\n",
        "            #        sent_loss.item(), len(pairs_batch_train), train_input.shape)\n",
        "            sent_loss.backward() # compute the backward pass\n",
        "            rnn_optimizer.step() # update the parameters\n",
        "\n",
        "            epoch_loss += word_loss        \n",
        "\n",
        "        # print the loss at each epoch\n",
        "        print('Epoch: {}   Loss: {}'.format(epoch+1, \n",
        "                                            epoch_loss / len(pairs_batch_train)))\n",
        "        \n",
        "        # Save model every 1 epoch\n",
        "        if(epoch+1)%1 == 0:\n",
        "            filename = filepath + 'models/' + model_type + '_00005-full-' + str(epoch+1) + '.pt'\n",
        "            torch.save(rnn_model.state_dict(), filename)\n",
        "        \n",
        "        loss_list.append(epoch_loss/ len(pairs_batch_train))\n",
        "        #print(loss_list)\n",
        "        # this is needed for work in Colab because once the time limit is up,\n",
        "        # it will automatically delete all files that are not saved in Google Drive      \n",
        "        print(\"saving model and loss data\")\n",
        "        list_df = {'loss':loss_list}\n",
        "        df = pd.DataFrame(list_df)\n",
        "        df.to_csv(filepath + 'models/' + model_type +  '_00005_loss_half_'+str(epoch+1)+'.csv')\n",
        "        torch.save({\n",
        "              'epoch': epoch+1,\n",
        "              'model_state_dict': rnn_model.state_dict(),\n",
        "              'optimizer_state_dict': rnn_optimizer.state_dict(),\n",
        "              }, filepath + tmp_name)\n",
        "    \n",
        "    return loss_list"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uMyV8txmTwB"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBf1jsKOhw2q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1691f371-b359-438b-a1ca-04b7eda38237"
      },
      "source": [
        "lost_list = train_rnn(pairs_batch_train, rnn_model,hidden_size, num_layers, loss_function, rnn_optimizer, n_epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/100 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "do59QnHphw2q"
      },
      "source": [
        "def predict_rnn(rnn_model, hidden_size, num_layers, word2idx, idx2word, context, max_len):\n",
        "    \"\"\"\n",
        "    This function predicts the next word, based on the history of the previous words.\n",
        "    We start with the 'context' and then feed the prediction as the next input.\n",
        "    \n",
        "    Arguments\n",
        "    ---------\n",
        "    rnn_model - object\n",
        "                an RNN object that contains the trained model\n",
        "                \n",
        "    hidden_size - integer\n",
        "                    the size of the hidden layer (the context size)\n",
        "                    \n",
        "    num_layers - integer\n",
        "                    the number of layers in the GRU cell\n",
        "                \n",
        "    word2idx - dictionary\n",
        "                    a dictionary where the keys are the unique words in the data\n",
        "                    and the values are the unique indices corresponding to the words\n",
        "                    \n",
        "    idx2word - dictionary\n",
        "                a dictionary, where the keys are the indices and the values are the words\n",
        "                    \n",
        "    context - string\n",
        "                the context sentence\n",
        "    \n",
        "    max_len - integer\n",
        "                integer value representing up to how many words to generate\n",
        "                            \n",
        "    Returns\n",
        "    -------\n",
        "    \n",
        "    predictions - string\n",
        "                    a string containing the generated sentence\n",
        "    \"\"\"\n",
        "    \n",
        "    # index the context\n",
        "    context_indexed = []\n",
        "    for word in context.split():\n",
        "        word_indexed = torch.LongTensor(1)\n",
        "        word_indexed[:] = word2idx[word]\n",
        "        context_indexed.append(word_indexed)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        predictions = []\n",
        "        # first build the hidden state from the context\n",
        "        if model_type == MODEL_CONSTANT[0]:\n",
        "            hidden = torch.zeros((num_layers, 1, hidden_size), device=device)\n",
        "        else:\n",
        "            hidden = rnn_model.init_state(1)   \n",
        "            \n",
        "        for word in context_indexed:\n",
        "            predictions.append(idx2word[word.item()])\n",
        "            word = word.to(device)\n",
        "            output, hidden = rnn_model(word, hidden)\n",
        "            \n",
        "        next_input = context_indexed[-1]\n",
        "        while((len(predictions) < max_len) and (predictions[-1] != '</s>')):\n",
        "            \n",
        "            # YOUR CODE HERE\n",
        "            #raise NotImplementedError()\n",
        "            #1. Run the forward pass to get the output. Don't forget to include the `hidden` state\n",
        "            #print(\"INPUT\", next_input)\n",
        "            next_input = next_input.to(device)\n",
        "            out, hidden = rnn_model.forward(next_input, hidden)\n",
        "            \n",
        "            #2. Run the output through a softmax to convert it to a probability distribution (`F.softmax`)\n",
        "            out = F.softmax(out)\n",
        "            \n",
        "            #3. Get the word with the highest probability using the `topk(1)` function\n",
        "            value, index = out.topk(1)\n",
        "            \n",
        "            #4. Convert the index of the predicted word to the actual word using the idx2word dictionary\n",
        "            word = idx2word[index.item()]\n",
        "            \n",
        "            #5. Append the predicted word to the `predictions` array\n",
        "            predictions.append(word)\n",
        "            next_input = index\n",
        "            \n",
        "    predictions = ' '.join(predictions)\n",
        "    \n",
        "    return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlV8-57Ik6CG"
      },
      "source": [
        "#checkpoint = torch.load(filepath+load_file)\n",
        "#rnn_model.load_state_dict(checkpoint['model_state_dict'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "iJY5YhuShw2q"
      },
      "source": [
        "contexts = ['<s> can someone', '<s> bought the dip', '<s> wait', '<s> so risky']\n",
        "\n",
        "max_len = 50\n",
        "\n",
        "for context in contexts:\n",
        "    predictions = predict_rnn(rnn_model, hidden_size, num_layers, word2idx, idx2word, context, max_len)\n",
        "    print(predictions)\n",
        "    print('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIt2brLChw2r"
      },
      "source": [
        "#torch.save(rnn_model.state_dict(), \".pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5c086XJ3hw2r"
      },
      "source": [
        "#rnn_model.load_state_dict(torch.load(filepath + \"models/LSTM_00005-6.pt\"))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}