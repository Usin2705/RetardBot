{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "step_03_RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBFYmMybhw2P"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn # the neural network package that contains functions for creating the neural network layers\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim # a package that allows use to use an optimizer in order to update the parameters during training\n",
        "from torch.utils.data import DataLoader # allows use to process the data in batches\n",
        "from torch.nn.utils.rnn import pad_sequence # a function that zero-pads the sentences so they can have equal size in a batch\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import pickle\n",
        "torch.manual_seed(0) # set a random seed for reproducibility\n",
        "from tqdm import tqdm\n",
        "\n",
        "MODEL_CONSTANT = ['GRU', 'LSTM', 'BI-LSTM']"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZRD5dayUjEt"
      },
      "source": [
        "###Set important constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMwGdw4vh86_"
      },
      "source": [
        "is_google_colab = True\n",
        "batch_size = 32\n",
        "is_preprocessing = False #This toggle preprocessing for RNN model (FALSE = not preprocessing for text, but instead load from processed data saved from previous run)\n",
        "model_type = MODEL_CONSTANT[2] # change this variable to correct model type\n",
        "load_model = False # this is for saving/loading the model from the checkpoint when colab crashes\n",
        "\n",
        "device = torch.device('cuda:0')\n",
        "#device = torch.device('cpu')\n",
        "\n",
        "# Below are the dataset file we will perform our training on:\n",
        "data_file = \"data_sample_2x.txt\"\n",
        "prep_file_name = data_file[:-4]\n",
        "\n",
        "\n",
        "learning_rate = 0.0005 # Learning rate\n",
        "n_epochs = 40 # the number of epochs to train\n",
        "embed_dim = 300 # the size of the embedding\n",
        "hidden_size = 450 # the size of the hidden state\n",
        "num_layers = 1 # the number of layers in the RNN cell"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDJa2Ishhw2U",
        "outputId": "2ef884cc-df4a-4bf8-8fe6-e88a1b3f1d8e"
      },
      "source": [
        "# this is to connect this notebook to the contents of your Google Drive\n",
        "# this is Ryoko's Google Drive filepath\n",
        "# please specify your own, or we can (probably) share a folder for it\n",
        "if is_google_colab:\n",
        "    # files uploaded to Google Drive will not be deleted by inactivity,\n",
        "    # but it does require an authorization code every time you use it\n",
        "    from google.colab import drive\n",
        "    filepath = r'/content/drive/My Drive/RetardBot/'\n",
        "    drive.mount(r'/content/drive')\n",
        "else:\n",
        "    filepath = ''\n",
        "print(\"Running model :\", model_type)\n",
        "\n",
        "load_file = 'temp_model_' + model_type + '_' + prep_file_name #file name if I'm going to temporary save/load the model"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Running model : BI-LSTM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlB_UBk1hw2W"
      },
      "source": [
        "###  Sentence boundaries\n",
        "\n",
        "When dealing with language, it is good to know when a sentence starts and when it ends. That will help the model at the beginning of the prediction, when we don't have any previous words as context. For that purpose, we are going to pad each sentence with a start-of-sentence symbol _\"&lt;s>\"_ and an end-of-sentence symbol _\"&lt;/s>\"_. \n",
        "\n",
        "Since you already did a similar thing in the n-grams exercise, this function is already implemented for you."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvKkH0Plhw2X"
      },
      "source": [
        "def add_sentence_boundaries(data):\n",
        "    \"\"\"\n",
        "    Takes the data, where each line is a sentence, appends <s> token at the beginning and </s> at the end of each sentence\n",
        "    Example input: I live in Helsinki\n",
        "    Example output: <s> I live in Helsinki </s>\n",
        "    \n",
        "    Arguments\n",
        "    ---------\n",
        "    data : list\n",
        "            a list of sentences\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    res : list\n",
        "            a list of sentences, where each sentence has <s> at the beginning and </s> at the end\n",
        "    \"\"\"\n",
        "    #res = []\n",
        "    #for sent in data:\n",
        "    #    sent = '<s> ' + sent.rstrip() + ' </s>'\n",
        "    #    res.append(sent)\n",
        "\n",
        "    res = ['<s> ' + sent.rstrip() + ' </s>' for sent in data]\n",
        "    \n",
        "    return res"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9PSn77Jhw2Z"
      },
      "source": [
        "### Index dictionaries <a class=\"anchor\" id=\"task_1_1\"></a> \n",
        "Neural networks can't process words as raw strings. Due to that, we need to represent the words with numbers. The first step in doing that is creating two dictionaries: word2idx and idx2word.\n",
        "\n",
        "The word2idx dictionary contains unique words as keys and unique indices for each of the words as values. <br>\n",
        "The idx2word dictionary contains unique indices as keys and unique words for each of those indices as values. It is essentially a reversed word2dx, where the keys are the values and the values are the keys.\n",
        "\n",
        "Example sentences: [\"I look forward\", \"You look forward\"] <br>\n",
        "word2idx = {\"I\": 1, \"look\": 2, \"forward\": 3, \"You\": 4} <br>\n",
        "idx2word = {1: \"I\", 2: \"look\", 3: \"forward\", 4: \"You\"} <br>\n",
        "\n",
        "Write a function that creates two dictionaries: word2idx and idx2work. The dictionaries should contain all the unique words in the data. <b>The indices should start from 1 and not from 0<b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEut8UGmhw2Z"
      },
      "source": [
        "def create_indices(data):\n",
        "    \"\"\"\n",
        "    This function creates two dictionaries: word2idx and idx2word, containing each unique word in the dataset\n",
        "    and its corresponding index.\n",
        "    Remember that the starting index should be 1 and not 0\n",
        "    \n",
        "    Arguments\n",
        "    ---------\n",
        "    data - list\n",
        "            a list of sentences, where each sentence starts with <s>\n",
        "            and ends with </s> token\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    word2idx - dictionary\n",
        "                a dictionary, where the keys are the words and the values are the indices\n",
        "                \n",
        "    idx2word - dictionary\n",
        "                a dictionary, where the keys are the indices and the values are the words\n",
        "    \"\"\"\n",
        "    \n",
        "    # YOUR CODE HERE\n",
        "    #raise NotImplementedError()\n",
        "      # There's several method to to this, using Counter would make our model run faster\n",
        "    # in preprocessing, otherwise it would take forever to run with full data\n",
        "    # sklearn also offer similar function, just watch out for stopwords\n",
        "    # sklearn.feature_extraction.text.CountVectorizer(min_df=1, stop_words=None)\n",
        "    # word_tokenizer = vectorizer.build_tokenizer()\n",
        "    # vectorizer.fit(data)\n",
        "    # word2idx = vectorizer.vocabulary_\n",
        "\n",
        "    data_list = ' '.join(data).split(' ')    \n",
        "    data_unique = list(dict.fromkeys(data_list))\n",
        "    word2idx = {word:data_unique.index(word)+1 for word in data_unique}\n",
        "    idx2word = {value:key for key,value in word2idx.items()}\n",
        "    \n",
        "    return word2idx, idx2word"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yk9LJKLhw2b"
      },
      "source": [
        "### Index data <a class=\"anchor\" id=\"task_1_2\"></a>\n",
        "After we have created the word2idx and idx2word dictionaries, it is time to index the data. In other words, we need to replace each word in the data with its corresponding index.\n",
        "\n",
        "Write a function that reads each sentence from the data and replaces each word in the sentence with its index from the word2idx dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5pVYNw0hw2b"
      },
      "source": [
        "def index_data(data, word2idx):\n",
        "    \"\"\"\n",
        "    This function replaces each word in the data with its corresponding index\n",
        "    \n",
        "    Arguments\n",
        "    ---------\n",
        "    data - list\n",
        "            a list of sentences, where each sentence starts with <s>\n",
        "            and ends with </s> token\n",
        "    \n",
        "    word2idx - dict\n",
        "            a dictionary where the keys are the unique words in the data\n",
        "            and the values are the unique indices corresponding to the words%\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    data_indexed - list\n",
        "                a list of sentences, where each word in the sentence is replaced with its index\n",
        "    \"\"\"\n",
        "    \n",
        "    data_indexed = []\n",
        "    # YOUR CODE HERE\n",
        "    #raise NotImplementedError()\n",
        "    \n",
        "    for sentence in data:\n",
        "        sentence_index = [word2idx[word] for word in sentence.split(' ')]\n",
        "        # Further improvement can be done here to remove the top loop\n",
        "        # For now it's fast enough to ignore\n",
        "        data_indexed.append(sentence_index)\n",
        "    \n",
        "\n",
        "    return data_indexed"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTt-hGFrhw2d"
      },
      "source": [
        "### Convert sentences to tensors\n",
        "\n",
        "This function converts each indexed sentence to a LongTensor data type. This is required in order to process it later using Pytorch.\n",
        "\n",
        "You don't have to modify this function. It is already implemented for you."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iw5AifOHhw2g"
      },
      "source": [
        "def convert_to_tensor(data_indexed):\n",
        "    \"\"\"\n",
        "    This function converts the indexed sentences to LongTensors\n",
        "    \n",
        "    Arguments\n",
        "    ---------\n",
        "    data_indexed - list\n",
        "            a list of sentences, where each word in the sentence\n",
        "            is replaced by its index\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    tensor_array - list\n",
        "                a list of sentences, where each sentence\n",
        "                is a LongTensor\n",
        "    \"\"\"\n",
        "    tensor_array = [torch.LongTensor(sent) for sent in data_indexed]\n",
        "        \n",
        "    return tensor_array"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbhRIOvfhw2h"
      },
      "source": [
        "### Combine features and labels in a tuple\n",
        "\n",
        "This function combines each indexed sentence and its corresponding labels to a tuple. This will be beneficial for us when we zero-pad the data later, in order to make the batches have equal-length samples.\n",
        "\n",
        "You don't have to modify this function. It is already implemented for you."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_AbIgnkhw2j"
      },
      "source": [
        "def combine_data(input_data, labels_data):\n",
        "    \"\"\"\n",
        "    This function converts the input features and the labels into tuples\n",
        "    where each tuple corresponds to one sentence in the format (features, labels)\n",
        "    \n",
        "    Arguments\n",
        "    ---------\n",
        "    input_data - list\n",
        "            a list of tensors containing the training features\n",
        "    \n",
        "    labels_data - list\n",
        "            a list of tensors containing the training labels\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    res - list\n",
        "            a list of tuples, where each tuple corresponds to one sentece pair\n",
        "            in the format (features, labels)\n",
        "    \"\"\"\n",
        "    \n",
        "    res = [(input_data[i], labels_data[i]) for i in range(len(input_data))]\n",
        "\n",
        "    return res"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybApSolDhw2k"
      },
      "source": [
        "### Remove extra data\n",
        "\n",
        "Since we will be processing the data in equal batches during training, we need to make sure that each batch has equal number of sentences. In case the last batch contains less sentences than the batch size, that batch will be discarded.\n",
        "\n",
        "This function discards the extra data that doesn't fit in a batch.\n",
        "\n",
        "You don't have to modify this function. It is already implemented for you."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTaECt-rhw2k"
      },
      "source": [
        "def remove_extra(data, batch_size):\n",
        "    \"\"\"\n",
        "    This function removes the extra data that does not fit in a batch   \n",
        "    \n",
        "    Arguments\n",
        "    ---------\n",
        "    data - list\n",
        "            a list of tuples, where each tuple corresponds to a\n",
        "            sentence in a format (features, labels)\n",
        "            \n",
        "    batch_size - integer\n",
        "                    the size of the batch\n",
        "    \n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    data - list\n",
        "            a list of tuples, where each tuple corresponds to a\n",
        "            sentence in a format (features, labels)\n",
        "    \"\"\"\n",
        "    \n",
        "    extra = len(data) % batch_size\n",
        "    if extra != 0:\n",
        "        data = data[:-extra][:]\n",
        "\n",
        "    return data"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7O9Zphvqhw2l"
      },
      "source": [
        "### Zero-pad the data\n",
        "\n",
        "In order to process the data in batches, we need to make sure that the sentences in each batch have equal lengths. Since we are working with sentences, each sentence in a batch can have different number of words. In this case, we need to  make the length of each sentence the same as the length of the longest sentence in that batch. We do that by adding zeros at the end of each sentence, until the sentence has equal length as the longest one in the batch.\n",
        "\n",
        "This function implements the zero-padding.\n",
        "\n",
        "You don't have to modify this function. It is already implemented for you."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zWi4aAZhw2l"
      },
      "source": [
        "def collate(list_of_samples):\n",
        "    \"\"\"\n",
        "    This function zero-pads the training data in order to process the sentences\n",
        "    in a batch during training\n",
        "    \n",
        "    Arguments\n",
        "    ---------\n",
        "    list_of_samples - list\n",
        "                        a list of tuples, where each tuple corresponds to a\n",
        "                        sentence in a format (features, labels)\n",
        "    \n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    pad_input_data - tensor\n",
        "                        a tensor of input features equal to the batch size,\n",
        "                        where features are zero-padded to have equal lengths\n",
        "                        \n",
        "    input_data_lengths - list\n",
        "                        a list where each element is the length of the \n",
        "                        corresponding sentence\n",
        "    \n",
        "    pad_labels_data - tensor\n",
        "                        a tensor of labels equal to the batch size,\n",
        "                        where labels are zero-padded to have equal lengths\n",
        "            \n",
        "    \"\"\"\n",
        "    \n",
        "    \n",
        "    list_of_samples.sort(key=lambda x: len(x[0]), reverse=True)\n",
        "    input_data, labels_data = zip(*list_of_samples)\n",
        "\n",
        "    input_data_lengths = [len(seq) for seq in input_data]\n",
        "    \n",
        "    padding_value = 0\n",
        "\n",
        "    # pad input\n",
        "    pad_input_data = pad_sequence(input_data, padding_value=padding_value)\n",
        "    \n",
        "    # pad labels\n",
        "    pad_labels_data = pad_sequence(labels_data, padding_value=padding_value)\n",
        "\n",
        "    return pad_input_data, input_data_lengths, pad_labels_data"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNYX0EJQhw2l"
      },
      "source": [
        "### Prepare features and labels <a class=\"anchor\" id=\"task_1_3\"></a> \n",
        "During training, the model takes an input word and outputs a prediction. We will need to compare this prediction to 'true label'. True label is just the next word in the text, but we will need to organize the data, so that every word in the text is considered as this 'true label'.\n",
        "\n",
        "In the label sentence, every word is moved a step in time, and for the input sentence the last word is missing. \n",
        "\n",
        "Example sentence: oops i did it again <br>\n",
        "INPUT: oops i did it <br>\n",
        "LABEL: i did it again\n",
        "\n",
        "Note: the first word in the sentence is start-of-sentence symbol and the last one is end-of-sentence symbol.\n",
        "\n",
        "Write a function that takes as input the indexed data and returns two arrays: the input array where the last word from each sentence is missing, and the label array, where every word is moved a step in time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zoIZXwbhw2m"
      },
      "source": [
        "def prepare_for_training(data_indexed):\n",
        "    \"\"\"\n",
        "    This function creates the input features and their corresponding labels\n",
        "    \n",
        "    Arguments\n",
        "    ---------\n",
        "    data_indexed - list\n",
        "            a list of sentences, where each word in the sentence\n",
        "            is replaced by its index\n",
        "    \n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    input_data - list\n",
        "            a list of indexed sentences, where the last element of each sentence is removed\n",
        "            \n",
        "    labels_data - list\n",
        "            a list of indexed sentences, where the first element of each sentence is removed\n",
        "    \"\"\"\n",
        "    \n",
        "    #input_data = []\n",
        "    #labels_data = []\n",
        "\n",
        "     # YOUR CODE HERE\n",
        "    #raise NotImplementedError()\n",
        "\n",
        "    input_data  = [data[:-1] for data in data_indexed]\n",
        "    labels_data = [data[1:] for data in data_indexed]\n",
        "\n",
        "    return input_data, labels_data"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygFyidgvhw2m"
      },
      "source": [
        "### Preprocess data <a class=\"anchor\" id=\"task_1_4\"></a>\n",
        "At this point, we have all the necessary functions to prepare the data for training. What is left to do is to run them one by one and get the data in the desired format.\n",
        "\n",
        "Write a function that takes the data and prepares it for training. You need to do the following steps:\n",
        "\n",
        "    1. Add sentence boundaries\n",
        "    2. Create index dictionaries (word2idx and idx2word)\n",
        "    3. Index the data in a way that each word is replaced by its index\n",
        "    4. Convert the indexed data to a list of tensors, where each tensor is a sentence\n",
        "    5. Split each sentence to input and labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmF4xoMZhw2m"
      },
      "source": [
        "def preprocess_data(data):\n",
        "    \"\"\"\n",
        "    This function runs the whole preprocessing pipeline and returns the prepared\n",
        "    input features and labels, along with the word2idx and idx2word dictionaries\n",
        "    \n",
        "    Arguments\n",
        "    ---------\n",
        "    data - list\n",
        "            a list of sentences that need to be prepared for training\n",
        "    \n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    input_data - list\n",
        "            a list of tensors, where each tensor is an indexed sentence used as input feature\n",
        "            \n",
        "    labels_data - list\n",
        "            a list of tensors, where each tensor is an indexed sentence used as a true label\n",
        "    \n",
        "    word2idx - dictionary\n",
        "                a dictionary, where the keys are the words and the values are the indices\n",
        "                \n",
        "    idx2word - dictionary\n",
        "                a dictionary, where the keys are the indices and the values are the words\n",
        "    \"\"\"\n",
        "    \n",
        "    # YOUR CODE HERE\n",
        "    #raise NotImplementedError()\n",
        "    #1. Add sentence boundaries    \n",
        "    print(\"Add sentence boundaries\")    \n",
        "    res = add_sentence_boundaries(data)\n",
        "        \n",
        "    #2. Create index dictionaries (word2idx and idx2word)\n",
        "    print(\"Create index dictionaries (word2idx and idx2word)\")\n",
        "    word2idx, idx2word = create_indices(res)    \n",
        "    \n",
        "    #3. Index the data in a way that each word is replaced by its index\n",
        "    print(\"Index the data in a way that each word is replaced by its index\")\n",
        "    indexed_data = index_data(res, word2idx)\n",
        "    \n",
        "    print(\"Convert the indexed data to a list of tensors\")\n",
        "    #4. Convert the indexed data to a list of tensors, where each tensor is a sentence\n",
        "    tensor_array = convert_to_tensor(indexed_data)   \n",
        "\n",
        "    print(\"Split each sentence to input and labels\")\n",
        "    #5. Split each sentence to input and labels\n",
        "    input_data, labels_data = prepare_for_training(tensor_array)\n",
        "\n",
        "    print(\"Done preprocessing\")\n",
        "    return input_data, labels_data, word2idx, idx2word"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXZswejHhw2n",
        "outputId": "ba63284b-f899-43a9-aba0-ce5007b93dbc"
      },
      "source": [
        "# Load data\n",
        "# Load result:\n",
        "with open(filepath + data_file, \"rb\") as fp:   # Unpickling\n",
        "#with open(filepath + \"clean_data.txt\", \"rb\") as fp:   # Unpickling\n",
        "    sentences = pickle.load(fp)\n",
        "\n",
        "print(sentences[22:35])\n",
        "print(len(sentences))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['you see grand kids, there was a time long long ago, the markets would have two directions, some times there were days where it actually went down, and the color would change from green to red.', 'what will happen is he’ll have to go all cash for 90 days though if he doesn’t pay it.', \"it's been a long and winding journey to here.\", 'i want to buy tqqq at $70/share..', '\" sadly for some of us their immune system is stronger\\'', 'you’re welcome.', 'so i sold everything and bought puts.', 'whole market is down, just load up more pltr positions for cheap 💰', 'matt and i just used 53m for total shares outstanding (i later added an adjustment to 73m in my update to account for all of the options and warrants going in the money.', 'pltr $30 🚀🚀🚀🤑🤑🤑', 'investing in yourself is some of the highest possible dividends.', 'i check my robinhood.', 'lmao there’s really gonna be an only fans ipo']\n",
            "40000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7XkO9y5hm3D"
      },
      "source": [
        "### Preprocessing data\n",
        "\n",
        "Preprocessing data and save those to files for future run (very important if we run big model in Google Colab, as it take DAYS and multiple run to get good result). If we run short model like 40k sentences then we might not need this, but large model will take a long time to create word2idx.\n",
        "\n",
        "Or load the data if we already run it before\n",
        "\n",
        "**Remember to create the folder to store the data (torch.save) won't auto create new folder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzn16N4qhw2n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8415d3d5-a32e-49cf-aadd-52f3959c99cd"
      },
      "source": [
        "if is_preprocessing:\n",
        "    train_input, train_labels, word2idx, idx2word = preprocess_data(sentences) # run the preprocessing pipeline\n",
        "    print(\"Saving...\")\n",
        "    torch.save(train_input, filepath + prep_file_name + '/train_input.pt')\n",
        "    torch.save(train_labels, filepath + prep_file_name + '/train_labels.pt')\n",
        "    torch.save(word2idx, filepath + prep_file_name + '/word2idx.pt')\n",
        "    torch.save(idx2word, filepath + prep_file_name + '/idx2word.pt')\n",
        "    train_data = combine_data(train_input, train_labels)\n",
        "    train_data = remove_extra(train_data, batch_size)\n",
        "    torch.save(train_data, filepath + prep_file_name + '/train_data.pt')    \n",
        "else:\n",
        "    print(\"Loading...\")\n",
        "    #train_input = torch.load(filepath + prep_file_name + '/train_input.pt')\n",
        "    #train_labels = torch.load(filepath + prep_file_name + '/train_labels.pt')    \n",
        "    #train_data = combine_data(train_input, train_labels)\n",
        "    #train_data = remove_extra(train_data, batch_size)\n",
        "    # Fastest load is only load 3 variable below:\n",
        "    word2idx = torch.load(filepath + prep_file_name + '/word2idx.pt')\n",
        "    idx2word = torch.load(filepath + prep_file_name + '/idx2word.pt')\n",
        "    train_data = torch.load(filepath + prep_file_name + '/train_data.pt') "
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HO_iajnUhw2o"
      },
      "source": [
        "pairs_batch_train = DataLoader(dataset=train_data,\n",
        "                    batch_size=batch_size,\n",
        "                    shuffle=True,\n",
        "                    collate_fn=collate,\n",
        "                    pin_memory=True)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0clC32QCThK"
      },
      "source": [
        "### RNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzFt3xV-hw2o"
      },
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, word2idx, embed_dim, context_dim, num_layers):\n",
        "        \"\"\"\n",
        "        This function initializes the layers of the model\n",
        "        \n",
        "        Arguments\n",
        "        ---------\n",
        "        word2idx - dictionary\n",
        "                    a dictionary where the keys are the unique words in the data\n",
        "                    and the values are the unique indices corresponding to the words\n",
        "        \n",
        "        embed_dim - integer\n",
        "                        the size of the word embeddings\n",
        "\n",
        "        context_dim - integer\n",
        "                        the dimension of the hidden size\n",
        "                        \n",
        "        num_layers - integer\n",
        "                        the number of layers in the GRU cell\n",
        "        \"\"\"\n",
        "        super(RNN, self).__init__()\n",
        "        self.word2idx = word2idx\n",
        "        self.embed_dim = embed_dim\n",
        "        self.context_dim = context_dim\n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        # here we initialise weighs of a model\n",
        "        self.word_embed = nn.Embedding(len(self.word2idx)+1, self.embed_dim) # embedding layer\n",
        "        \n",
        "        #GRU\n",
        "        if model_type == MODEL_CONSTANT[0]:            \n",
        "            self.rnn = nn.GRU(self.embed_dim,\n",
        "                              self.context_dim,\n",
        "                              num_layers=self.num_layers)\n",
        "        \n",
        "        #LSTM\n",
        "        elif model_type == MODEL_CONSTANT[1]:            \n",
        "            self.rnn = nn.LSTM(input_size = self.embed_dim, \n",
        "                               hidden_size = self.context_dim,\n",
        "                               num_layers = self.num_layers)\n",
        "        #BI-LSTM\n",
        "        else:\n",
        "            self.rnn = nn.LSTM(input_size = self.embed_dim, \n",
        "                               hidden_size = self.context_dim,\n",
        "                               num_layers = self.num_layers,\n",
        "                               bidirectional=True)\n",
        "        \n",
        "        self.dropout = nn.Dropout(0.1) # Dropout        \n",
        "        \n",
        "        if model_type == MODEL_CONSTANT[2]:\n",
        "            self.out = nn.Linear(self.context_dim*2, len(self.word2idx)+1) # output layer\n",
        "        else:\n",
        "            self.out = nn.Linear(self.context_dim, len(self.word2idx)+1) # output layer\n",
        "\n",
        "    \n",
        "    def forward(self, word, hidden):\n",
        "        \"\"\"\n",
        "        This function implements the forward pass of the model\n",
        "        \n",
        "        Arguments\n",
        "        ---------\n",
        "        word - tensor\n",
        "                a tensor containing indices of the words in a batch\n",
        "                \n",
        "        hidden - tensor\n",
        "                    the previous hidden state of the GRU model\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        output - tensor\n",
        "                    a tensor of logits from the linear transformation\n",
        "        \n",
        "        hidden - tensor\n",
        "                    the current hidden state of the GRU model\n",
        "        \"\"\" \n",
        "        \n",
        "        # YOUR CODE HERE\n",
        "        #raise NotImplementedError()\n",
        "        #1. Replace the indexed word with its embedding vector. \n",
        "        #In other words, pass it through the embedding layer\n",
        "        embeds = self.word_embed(word)\n",
        "        \n",
        "        batch_size = word.shape[0]\n",
        "        #print(batch_size)\n",
        "        #2. Reshape the embedding vector to a shape of (1, batch_size)\n",
        "        embeds = embeds.reshape(1, batch_size, self.embed_dim)\n",
        "        \n",
        "        #3. Pass the embedding through the GRU cell to get the output \n",
        "        #and the hidden tensors. The GRU function takes as input the \n",
        "        #work embedding and the previous hidden state.\n",
        "        output, hidden = self.rnn(embeds, hidden)\n",
        "        \n",
        "        #4. Addpy a dropout to the output of the RNN\n",
        "        output = self.dropout(output)\n",
        "        \n",
        "        #5. Apply the linear transformation to the output of the dropout layer.\n",
        "        output = self.out(output)\n",
        "        \n",
        "        #6. Reshape the output to have a shape (batch_size, vocab_length+1)\n",
        "        output = output.reshape(batch_size, len(self.word2idx) + 1)\n",
        "        \n",
        "        #7. Return the output of the linear transformation and the hidden tensor\n",
        "        #only GRU need to move to device\n",
        "        if model_type == MODEL_CONSTANT[0]:\n",
        "            hidden = hidden.to(device)\n",
        "        return output, hidden\n",
        "    \n",
        "    def init_state(self, sequence_length):\n",
        "        if model_type == MODEL_CONSTANT[1]:\n",
        "            return (torch.zeros(self.num_layers, sequence_length, self.context_dim).to(device),\n",
        "                    torch.zeros(self.num_layers, sequence_length, self.context_dim).to(device))\n",
        "        else:\n",
        "            return (torch.zeros(self.num_layers*2, sequence_length, self.context_dim).to(device),\n",
        "                    torch.zeros(self.num_layers*2, sequence_length, self.context_dim).to(device))"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJyz9rE6hw2p"
      },
      "source": [
        "rnn_model = RNN(word2idx, embed_dim, hidden_size, num_layers).to(device) # initialize the RNN model\n",
        "loss_function = nn.CrossEntropyLoss(ignore_index=0) # define the loss function\n",
        "rnn_optimizer = optim.Adam(rnn_model.parameters(), lr=learning_rate) # define the optimizer"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYqX6_BwcLER"
      },
      "source": [
        "### Training RNN network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mMmF9CAhw2p"
      },
      "source": [
        "def train_rnn(pairs_batch_train, rnn_model, hidden_size, num_layers, loss_function, rnn_optimizer, n_epochs):\n",
        "    \"\"\"\n",
        "    This function implements the training of the model\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    pairs_batch_train - object\n",
        "                            a DataLoader object that contains the batched data\n",
        "\n",
        "    rnn_model - object\n",
        "                an RNN object that contains the initialized model\n",
        "                \n",
        "    hidden_size - integer\n",
        "                    the size of the hidden layer (the context size)\n",
        "    \n",
        "    num_layers - integer\n",
        "                        the number of layers in the GRU cell\n",
        "\n",
        "    loss_function - object\n",
        "                        the CrossEntropy loss function\n",
        "\n",
        "    rnn_optimizer - object\n",
        "                        an Adam object of the optimizer class\n",
        "\n",
        "    n_epochs - integer\n",
        "                the number of epochs to train\n",
        "    \"\"\" \n",
        "    loss_list = []\n",
        "    word_loss_list = []    \n",
        "\n",
        "    if load_model == True:\n",
        "        if not torch.cuda.is_available():\n",
        "            checkpoint = torch.load(filepath+load_file,map_location=torch.device('cpu'))\n",
        "        else:\n",
        "            checkpoint = torch.load(filepath+load_file)\n",
        "        rnn_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        rnn_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        last_epoch = checkpoint['epoch']\n",
        "    else:\n",
        "        last_epoch = 0\n",
        "\n",
        "    for epoch in tqdm(range(last_epoch, n_epochs)): # iterate over the epochs\n",
        "        epoch_loss = 0\n",
        "        epoch_word_loss = 0\n",
        "        rnn_model.train() # put the model in training mode\n",
        "        \n",
        "        for iteration, batch in enumerate(pairs_batch_train): # at each step take a batch of sentences\n",
        "            sent_loss = 0\n",
        "            rnn_optimizer.zero_grad() # clear gradients\n",
        "            \n",
        "            train_input, train_input_lengths, train_labels = batch # extract the data from the batch\n",
        "            train_input = train_input.to(device)\n",
        "            #train_input_lengths = train_input_lengths.to(device) #this is a list\n",
        "            train_labels = train_labels.to(device)\n",
        "            \n",
        "            # initialize the hidden state for GRU\n",
        "            if model_type == MODEL_CONSTANT[0]: \n",
        "                hidden = torch.zeros((num_layers, train_input.size(1), hidden_size)) \n",
        "                hidden = hidden.to(device)\n",
        "            else:\n",
        "                hidden = rnn_model.init_state(train_input.size(1))      \n",
        "            \n",
        "            \n",
        "            for i in range(train_input.size(0)): # iterate over the word in the sentence\n",
        "                output, hidden = rnn_model(train_input[i], hidden) # forward pass               \n",
        "                    \n",
        "                labels = torch.LongTensor(train_labels.size(1)) # define a random tensor with batch_size as number of elements\n",
        "                labels = labels.to(device)\n",
        "                labels[:] = train_labels[i][:] # put the correct label values in the tensor\n",
        "                \n",
        "                sent_loss += loss_function(output, labels) # compute the loss, compare the predictions and the labels\n",
        "\n",
        "            print(train_labels.size(1))\n",
        "            word_loss = sent_loss.item()/train_labels.size(1)\n",
        "            #if (iteration%500) == 0:\n",
        "            #  print(\"sentence_loss, pair_batch_train, train_inputsize\",\n",
        "            #        sent_loss.item(), len(pairs_batch_train), train_input.shape)\n",
        "            sent_loss.backward() # compute the backward pass\n",
        "            rnn_optimizer.step() # update the parameters\n",
        "\n",
        "            epoch_loss += word_loss        \n",
        "\n",
        "        # print the loss at each epoch\n",
        "        print('Epoch: {}   Loss: {}'.format(epoch+1, \n",
        "                                            epoch_loss / len(pairs_batch_train)))\n",
        "        \n",
        "        # Save model every 4 epoch\n",
        "        if(epoch+1)%4 == 0:\n",
        "            filename = filepath + 'models/' + model_type + '_' + prep_file_name + '_' + str(learning_rate) + '_' + str(epoch+1) + '.pt'\n",
        "            torch.save(rnn_model.state_dict(), filename)\n",
        "        \n",
        "        loss_list.append(epoch_loss/ len(pairs_batch_train))\n",
        "        #print(loss_list)\n",
        "        # this is needed for work in Colab because once the time limit is up,\n",
        "        # it will automatically delete all files that are not saved in Google Drive      \n",
        "        print(\"saving model and loss data\")\n",
        "        list_df = {'loss':loss_list}\n",
        "        df = pd.DataFrame(list_df)\n",
        "        df.to_csv(filepath + 'models/' + model_type + '_' + prep_file_name + '_' + str(learning_rate) + '_' + str(epoch+1) + '.csv')\n",
        "        torch.save({\n",
        "              'epoch': epoch+1,\n",
        "              'model_state_dict': rnn_model.state_dict(),\n",
        "              'optimizer_state_dict': rnn_optimizer.state_dict(),\n",
        "              }, filepath + load_file)\n",
        "    \n",
        "    return loss_list"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uMyV8txmTwB"
      },
      "source": [
        "#### Training step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBf1jsKOhw2q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5df6cad-285b-4f08-8ba1-78e00ce7e419"
      },
      "source": [
        "lost_list = train_rnn(pairs_batch_train, rnn_model,hidden_size, num_layers, loss_function, rnn_optimizer, n_epochs)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/13 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 28   Loss: 0.6502838764667511\n",
            "saving model and loss data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  8%|▊         | 1/13 [07:02<1:24:34, 422.86s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 29   Loss: 0.5731421462297439\n",
            "saving model and loss data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 15%|█▌        | 2/13 [14:05<1:17:30, 422.79s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 30   Loss: 0.5369274916172028\n",
            "saving model and loss data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 23%|██▎       | 3/13 [21:07<1:10:24, 422.49s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 31   Loss: 0.5289100328683853\n",
            "saving model and loss data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 31%|███       | 4/13 [28:08<1:03:20, 422.24s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 32   Loss: 0.5196785936832428\n",
            "saving model and loss data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 38%|███▊      | 5/13 [35:09<56:14, 421.84s/it]  "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 33   Loss: 0.5101091520309449\n",
            "saving model and loss data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 46%|████▌     | 6/13 [42:10<49:10, 421.48s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 34   Loss: 0.5165486315965653\n",
            "saving model and loss data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 54%|█████▍    | 7/13 [49:12<42:10, 421.79s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 35   Loss: 0.5193125486135483\n",
            "saving model and loss data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 62%|██████▏   | 8/13 [56:12<35:05, 421.14s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 36   Loss: 0.5098922646284103\n",
            "saving model and loss data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 69%|██████▉   | 9/13 [1:03:14<28:05, 421.27s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 37   Loss: 0.5045015212535858\n",
            "saving model and loss data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 77%|███████▋  | 10/13 [1:10:14<21:03, 421.05s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 38   Loss: 0.49602408640384676\n",
            "saving model and loss data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 85%|████████▍ | 11/13 [1:17:18<14:03, 421.99s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 39   Loss: 0.49522546541690826\n",
            "saving model and loss data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 92%|█████████▏| 12/13 [1:24:19<07:01, 421.57s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 40   Loss: 0.4922507870674133\n",
            "saving model and loss data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 13/13 [1:31:22<00:00, 421.71s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "do59QnHphw2q"
      },
      "source": [
        "def predict_rnn(rnn_model, hidden_size, num_layers, word2idx, idx2word, context, max_len):\n",
        "    \"\"\"\n",
        "    This function predicts the next word, based on the history of the previous words.\n",
        "    We start with the 'context' and then feed the prediction as the next input.\n",
        "    \n",
        "    Arguments\n",
        "    ---------\n",
        "    rnn_model - object\n",
        "                an RNN object that contains the trained model\n",
        "                \n",
        "    hidden_size - integer\n",
        "                    the size of the hidden layer (the context size)\n",
        "                    \n",
        "    num_layers - integer\n",
        "                    the number of layers in the GRU cell\n",
        "                \n",
        "    word2idx - dictionary\n",
        "                    a dictionary where the keys are the unique words in the data\n",
        "                    and the values are the unique indices corresponding to the words\n",
        "                    \n",
        "    idx2word - dictionary\n",
        "                a dictionary, where the keys are the indices and the values are the words\n",
        "                    \n",
        "    context - string\n",
        "                the context sentence\n",
        "    \n",
        "    max_len - integer\n",
        "                integer value representing up to how many words to generate\n",
        "                            \n",
        "    Returns\n",
        "    -------\n",
        "    \n",
        "    predictions - string\n",
        "                    a string containing the generated sentence\n",
        "    \"\"\"\n",
        "    \n",
        "    # index the context\n",
        "    context_indexed = []\n",
        "    for word in context.split():\n",
        "        word_indexed = torch.LongTensor(1)\n",
        "        word_indexed[:] = word2idx[word]\n",
        "        context_indexed.append(word_indexed)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        predictions = []\n",
        "        # first build the hidden state from the context\n",
        "        if model_type == MODEL_CONSTANT[0]:\n",
        "            hidden = torch.zeros((num_layers, 1, hidden_size), device=device)\n",
        "        else:\n",
        "            hidden = rnn_model.init_state(1)   \n",
        "            \n",
        "        for word in context_indexed:\n",
        "            predictions.append(idx2word[word.item()])\n",
        "            word = word.to(device)\n",
        "            output, hidden = rnn_model(word, hidden)\n",
        "            \n",
        "        next_input = context_indexed[-1]\n",
        "        while((len(predictions) < max_len) and (predictions[-1] != '</s>')):\n",
        "            \n",
        "            # YOUR CODE HERE\n",
        "            #raise NotImplementedError()\n",
        "            #1. Run the forward pass to get the output. Don't forget to include the `hidden` state\n",
        "            #print(\"INPUT\", next_input)\n",
        "            next_input = next_input.to(device)\n",
        "            out, hidden = rnn_model.forward(next_input, hidden)\n",
        "            \n",
        "            #2. Run the output through a softmax to convert it to a probability distribution (`F.softmax`)\n",
        "            out = F.softmax(out)\n",
        "            \n",
        "            #3. Get the word with the highest probability using the `topk(1)` function\n",
        "            value, index = out.topk(1)\n",
        "            \n",
        "            #4. Convert the index of the predicted word to the actual word using the idx2word dictionary\n",
        "            word = idx2word[index.item()]\n",
        "            \n",
        "            #5. Append the predicted word to the `predictions` array\n",
        "            predictions.append(word)\n",
        "            next_input = index\n",
        "            \n",
        "    predictions = ' '.join(predictions)\n",
        "    \n",
        "    return predictions"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlV8-57Ik6CG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ac8c408-3847-424d-9635-1a99fd1c7d66"
      },
      "source": [
        "checkpoint = torch.load(filepath+load_file)\n",
        "rnn_model.load_state_dict(checkpoint['model_state_dict'])"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "iJY5YhuShw2q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cd6f3f2-c268-49a5-defd-911b7829ad3a"
      },
      "source": [
        "contexts = ['<s> can someone', '<s> bought the dip', '<s> wait', '<s> this is the', '<s> to the']\n",
        "\n",
        "max_len = 50\n",
        "\n",
        "for context in contexts:\n",
        "    for i in range(5):\n",
        "      predictions = predict_rnn(rnn_model, hidden_size, num_layers, word2idx, idx2word, context, max_len)\n",
        "      print(predictions)\n",
        "    print('\\n')"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:68: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "<s> can someone please explain to me why not just the worst of the american people hate so to look at the scoreboard and make its - great way to keep yoloing he's from options trading it is a certain way, but there is no need for it </s>\n",
            "<s> can someone with profit is the smart money in this sub put options trading for a 10 percent in the last quarter </s>\n",
            "<s> can someone with please do my life can keep between the single stock over his name on the day. </s>\n",
            "<s> can someone say is a wsb moron it’s out. </s>\n",
            "<s> can someone please have their house prices they don’t use it to buy more shares at a strike price </s>\n",
            "\n",
            "\n",
            "<s> bought the dip on dkng </s>\n",
            "<s> bought the dip at $59.30, $60c 8/7 mean edit: like that. </s>\n",
            "<s> bought the dip at $59.30, $60c 8/7 yolo whole account edit: down 20% so far, still won’t sell, because i believe in it, not just cause i’m restricted from day trading 🤡 </s>\n",
            "<s> bought the dip for the 25th </s>\n",
            "<s> bought the dip yesterday </s>\n",
            "\n",
            "\n",
            "<s> wait a second... everybody gets rich from tesla calls. </s>\n",
            "<s> wait i have a good idea to talk about my puts not a fucking retard </s>\n",
            "<s> wait for the ride to bump up/down, and then hop back on. </s>\n",
            "<s> wait trump had a major bug to be made for me of the day for five before the market wants to correct. </s>\n",
            "<s> wait for new to buy a dip to buying nio </s>\n",
            "\n",
            "\n",
            "<s> this is the same place where “death to america” chants frequently break out in their literal fucking government, and during their prayer ceremonies. </s>\n",
            "<s> this is the only place on reddit </s>\n",
            "<s> this is the most boring day. </s>\n",
            "<s> this is the same place twice where i have done reddit to 1 paper trade for my trades to be as stonks only in a time i live for something super high amount and who was about things to take profits from gains to everyone else’s but. </s>\n",
            "<s> this is the last stand. </s>\n",
            "\n",
            "\n",
            "<s> to the moon and back. </s>\n",
            "<s> to the moon and did some not consider the same thing. </s>\n",
            "<s> to the moon we go 😤 </s>\n",
            "<s> to the moon we go 😤 </s>\n",
            "<s> to the moon and back. </s>\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIt2brLChw2r"
      },
      "source": [
        ""
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5c086XJ3hw2r"
      },
      "source": [
        "#rnn_model.load_state_dict(torch.load(filepath + \"models/LSTM_00005-6.pt\"))"
      ],
      "execution_count": 53,
      "outputs": []
    }
  ]
}