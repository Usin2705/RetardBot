{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Sentence boundaries\n",
    "\n",
    "When dealing with language, it is good to know when a sentence starts and when it ends. That will help the model at the beginning of the prediction, when we don't have any previous words as context. For that purpose, we are going to pad each sentence with a start-of-sentence symbol _\"&lt;s>\"_ and an end-of-sentence symbol _\"&lt;/s>\"_. \n",
    "\n",
    "Since you already did a similar thing in the n-grams exercise, this function is already implemented for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sentence_boundaries(data):\n",
    "    \"\"\"\n",
    "    Takes the data, where each line is a sentence, appends <s> token at the beginning and </s> at the end of each sentence\n",
    "    Example input: I live in Helsinki\n",
    "    Example output: <s> I live in Helsinki </s>\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    data : list\n",
    "            a list of sentences\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    res : list\n",
    "            a list of sentences, where each sentence has <s> at the beginning and </s> at the end\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    for sent in data:\n",
    "        sent = '<s> ' + sent.rstrip() + ' </s>'\n",
    "        res.append(sent)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index dictionaries <a class=\"anchor\" id=\"task_1_1\"></a> \n",
    "Neural networks can't process words as raw strings. Due to that, we need to represent the words with numbers. The first step in doing that is creating two dictionaries: word2idx and idx2word.\n",
    "\n",
    "The word2idx dictionary contains unique words as keys and unique indices for each of the words as values. <br>\n",
    "The idx2word dictionary contains unique indices as keys and unique words for each of those indices as values. It is essentially a reversed word2dx, where the keys are the values and the values are the keys.\n",
    "\n",
    "Example sentences: [\"I look forward\", \"You look forward\"] <br>\n",
    "word2idx = {\"I\": 1, \"look\": 2, \"forward\": 3, \"You\": 4} <br>\n",
    "idx2word = {1: \"I\", 2: \"look\", 3: \"forward\", 4: \"You\"} <br>\n",
    "\n",
    "Write a function that creates two dictionaries: word2idx and idx2work. The dictionaries should contain all the unique words in the data. <b>The indices should start from 1 and not from 0<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_indices(data):\n",
    "    \"\"\"\n",
    "    This function creates two dictionaries: word2idx and idx2word, containing each unique word in the dataset\n",
    "    and its corresponding index.\n",
    "    Remember that the starting index should be 1 and not 0\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    data - list\n",
    "            a list of sentences, where each sentence starts with <s>\n",
    "            and ends with </s> token\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    word2idx - dictionary\n",
    "                a dictionary, where the keys are the words and the values are the indices\n",
    "                \n",
    "    idx2word - dictionary\n",
    "                a dictionary, where the keys are the indices and the values are the words\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "    word2idx = dict()\n",
    "    idx2word = dict()\n",
    "    \n",
    "    data_list = ''\n",
    "    for sentence in data:\n",
    "        data_list = data_list + ' ' + sentence\n",
    "\n",
    "    data_list = data_list[1:]\n",
    "    data_split = data_list.split(' ')\n",
    "    data_unique = []\n",
    "    for word in data_split:\n",
    "        if word not in data_unique:\n",
    "            data_unique.append(word)\n",
    "            if word not in word2idx.keys():\n",
    "                 word2idx[word] = data_unique.index(word)+1\n",
    "    \n",
    "    for key, value in word2idx.items():\n",
    "        idx2word[value] = key\n",
    "    \n",
    "    return word2idx, idx2word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index data <a class=\"anchor\" id=\"task_1_2\"></a>\n",
    "After we have created the word2idx and idx2word dictionaries, it is time to index the data. In other words, we need to replace each word in the data with its corresponding index.\n",
    "\n",
    "Write a function that reads each sentence from the data and replaces each word in the sentence with its index from the word2idx dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_data(data, word2idx):\n",
    "    \"\"\"\n",
    "    This function replaces each word in the data with its corresponding index\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    data - list\n",
    "            a list of sentences, where each sentence starts with <s>\n",
    "            and ends with </s> token\n",
    "    \n",
    "    word2idx - dict\n",
    "            a dictionary where the keys are the unique words in the data\n",
    "            and the values are the unique indices corresponding to the words%\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    data_indexed - list\n",
    "                a list of sentences, where each word in the sentence is replaced with its index\n",
    "    \"\"\"\n",
    "    \n",
    "    data_indexed = []\n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "    \n",
    "    for sentence in data:\n",
    "        sentence_index = []\n",
    "        for word in sentence.split(' '):\n",
    "            sentence_index.append(word2idx[word])\n",
    "        data_indexed.append(sentence_index)\n",
    "    \n",
    "\n",
    "    return data_indexed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert sentences to tensors\n",
    "\n",
    "This function converts each indexed sentence to a LongTensor data type. This is required in order to process it later using Pytorch.\n",
    "\n",
    "You don't have to modify this function. It is already implemented for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tensor(data_indexed):\n",
    "    \"\"\"\n",
    "    This function converts the indexed sentences to LongTensors\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    data_indexed - list\n",
    "            a list of sentences, where each word in the sentence\n",
    "            is replaced by its index\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tensor_array - list\n",
    "                a list of sentences, where each sentence\n",
    "                is a LongTensor\n",
    "    \"\"\"\n",
    "    \n",
    "    tensor_array = []\n",
    "    for sent in data_indexed:\n",
    "        tensor_array.append(torch.LongTensor(sent))    \n",
    "        \n",
    "    return tensor_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine features and labels in a tuple\n",
    "\n",
    "This function combines each indexed sentence and its corresponding labels to a tuple. This will be beneficial for us when we zero-pad the data later, in order to make the batches have equal-length samples.\n",
    "\n",
    "You don't have to modify this function. It is already implemented for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_data(input_data, labels_data):\n",
    "    \"\"\"\n",
    "    This function converts the input features and the labels into tuples\n",
    "    where each tuple corresponds to one sentence in the format (features, labels)\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    input_data - list\n",
    "            a list of tensors containing the training features\n",
    "    \n",
    "    labels_data - list\n",
    "            a list of tensors containing the training labels\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    res - list\n",
    "            a list of tuples, where each tuple corresponds to one sentece pair\n",
    "            in the format (features, labels)\n",
    "    \"\"\"\n",
    "    \n",
    "    res = []\n",
    "    \n",
    "    for i in range(len(input_data)):\n",
    "        res.append((input_data[i], labels_data[i]))\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove extra data\n",
    "\n",
    "Since we will be processing the data in equal batches during training, we need to make sure that each batch has equal number of sentences. In case the last batch contains less sentences than the batch size, that batch will be discarded.\n",
    "\n",
    "This function discards the extra data that doesn't fit in a batch.\n",
    "\n",
    "You don't have to modify this function. It is already implemented for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extra(data, batch_size):\n",
    "    \"\"\"\n",
    "    This function removes the extra data that does not fit in a batch   \n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    data - list\n",
    "            a list of tuples, where each tuple corresponds to a\n",
    "            sentence in a format (features, labels)\n",
    "            \n",
    "    batch_size - integer\n",
    "                    the size of the batch\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    data - list\n",
    "            a list of tuples, where each tuple corresponds to a\n",
    "            sentence in a format (features, labels)\n",
    "    \"\"\"\n",
    "    \n",
    "    extra = len(data) % batch_size\n",
    "    if extra != 0:\n",
    "        data = data[:-extra][:]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-pad the data\n",
    "\n",
    "In order to process the data in batches, we need to make sure that the sentences in each batch have equal lengths. Since we are working with sentences, each sentence in a batch can have different number of words. In this case, we need to  make the length of each sentence the same as the length of the longest sentence in that batch. We do that by adding zeros at the end of each sentence, until the sentence has equal length as the longest one in the batch.\n",
    "\n",
    "This function implements the zero-padding.\n",
    "\n",
    "You don't have to modify this function. It is already implemented for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(list_of_samples):\n",
    "    \"\"\"\n",
    "    This function zero-pads the training data in order to process the sentences\n",
    "    in a batch during training\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    list_of_samples - list\n",
    "                        a list of tuples, where each tuple corresponds to a\n",
    "                        sentence in a format (features, labels)\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pad_input_data - tensor\n",
    "                        a tensor of input features equal to the batch size,\n",
    "                        where features are zero-padded to have equal lengths\n",
    "                        \n",
    "    input_data_lengths - list\n",
    "                        a list where each element is the length of the \n",
    "                        corresponding sentence\n",
    "    \n",
    "    pad_labels_data - tensor\n",
    "                        a tensor of labels equal to the batch size,\n",
    "                        where labels are zero-padded to have equal lengths\n",
    "            \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    list_of_samples.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    input_data, labels_data = zip(*list_of_samples)\n",
    "\n",
    "    input_data_lengths = [len(seq) for seq in input_data]\n",
    "    \n",
    "    padding_value = 0\n",
    "\n",
    "    # pad input\n",
    "    pad_input_data = pad_sequence(input_data, padding_value=padding_value)\n",
    "    \n",
    "    # pad labels\n",
    "    pad_labels_data = pad_sequence(labels_data, padding_value=padding_value)\n",
    "\n",
    "    return pad_input_data, input_data_lengths, pad_labels_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare features and labels <a class=\"anchor\" id=\"task_1_3\"></a> \n",
    "During training, the model takes an input word and outputs a prediction. We will need to compare this prediction to 'true label'. True label is just the next word in the text, but we will need to organize the data, so that every word in the text is considered as this 'true label'.\n",
    "\n",
    "In the label sentence, every word is moved a step in time, and for the input sentence the last word is missing. \n",
    "\n",
    "Example sentence: oops i did it again <br>\n",
    "INPUT: oops i did it <br>\n",
    "LABEL: i did it again\n",
    "\n",
    "Note: the first word in the sentence is start-of-sentence symbol and the last one is end-of-sentence symbol.\n",
    "\n",
    "Write a function that takes as input the indexed data and returns two arrays: the input array where the last word from each sentence is missing, and the label array, where every word is moved a step in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_training(data_indexed):\n",
    "    \"\"\"\n",
    "    This function creates the input features and their corresponding labels\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    data_indexed - list\n",
    "            a list of sentences, where each word in the sentence\n",
    "            is replaced by its index\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    input_data - list\n",
    "            a list of indexed sentences, where the last element of each sentence is removed\n",
    "            \n",
    "    labels_data - list\n",
    "            a list of indexed sentences, where the first element of each sentence is removed\n",
    "    \"\"\"\n",
    "    \n",
    "    input_data = []\n",
    "    labels_data = []\n",
    "\n",
    "     # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "    for data in data_indexed:    \n",
    "        input_data.append(data[:-1])\n",
    "        labels_data.append(data[1:])\n",
    "    \n",
    "    return input_data, labels_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data <a class=\"anchor\" id=\"task_1_4\"></a>\n",
    "At this point, we have all the necessary functions to prepare the data for training. What is left to do is to run them one by one and get the data in the desired format.\n",
    "\n",
    "Write a function that takes the data and prepares it for training. You need to do the following steps:\n",
    "\n",
    "    1. Add sentence boundaries\n",
    "    2. Create index dictionaries (word2idx and idx2word)\n",
    "    3. Index the data in a way that each word is replaced by its index\n",
    "    4. Convert the indexed data to a list of tensors, where each tensor is a sentence\n",
    "    5. Split each sentence to input and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    \"\"\"\n",
    "    This function runs the whole preprocessing pipeline and returns the prepared\n",
    "    input features and labels, along with the word2idx and idx2word dictionaries\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    data - list\n",
    "            a list of sentences that need to be prepared for training\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    input_data - list\n",
    "            a list of tensors, where each tensor is an indexed sentence used as input feature\n",
    "            \n",
    "    labels_data - list\n",
    "            a list of tensors, where each tensor is an indexed sentence used as a true label\n",
    "    \n",
    "    word2idx - dictionary\n",
    "                a dictionary, where the keys are the words and the values are the indices\n",
    "                \n",
    "    idx2word - dictionary\n",
    "                a dictionary, where the keys are the indices and the values are the words\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "    #1. Add sentence boundaries    \n",
    "    res = add_sentence_boundaries(data)\n",
    "    \n",
    "    #2. Create index dictionaries (word2idx and idx2word)\n",
    "    word2idx, idx2word = create_indices(res)    \n",
    "    \n",
    "    #3. Index the data in a way that each word is replaced by its index\n",
    "    indexed_data = index_data(res, word2idx)\n",
    "    \n",
    "    #4. Convert the indexed data to a list of tensors, where each tensor is a sentence\n",
    "    tensor_array = convert_to_tensor(indexed_data)    \n",
    "    \n",
    "    #5. Split each sentence to input and labels\n",
    "    input_data, labels_data = prepare_for_training(tensor_array)\n",
    "    \n",
    "    return input_data, labels_data, word2idx, idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We know you love Chewy.', \"We know you're here.\", \"We know you know the Chewy-RyanCohen-GameStop connection, but it wasn't real enough for you yet.\", \"Well, I don't have to tell you, because you're not stupid, but I will anyway:  it's gotten really real enough for you now.\", 'The Chewy executive triumvirate joining the GameStop board of directors is your signal, friend.', 'You may start pumping GME to your boomer audience.', 'Now.', \"I don't believe reddit has been too kind to you in the past, but worry not, follow through with this and you'll have lots of friends here and we'll have your back forever.\", 'Well, definitely not forever, but at least for a while.', 'What better time to start than today?', 'With love, brother.', 'P.S.', \"- don't be afraid to use the rocket ðŸš€, it feels good.\"]\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "# Load result:\n",
    "with open(\"data.txt\", \"rb\") as fp:   # Unpickling\n",
    "    sentences = pickle.load(fp)\n",
    "\n",
    "print(sentences[22:35])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, train_labels, word2idx, idx2word = preprocess_data(sentences) # run the preprocessing pipeline\n",
    "train_data = combine_data(train_input, train_labels)\n",
    "train_data = remove_extra(train_data, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pairs_batch_train = DataLoader(dataset=train_data,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    collate_fn=collate,\n",
    "                    pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
